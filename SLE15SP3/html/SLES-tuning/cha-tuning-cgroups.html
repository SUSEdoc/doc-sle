<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLES 15 SP3 | System Analysis and Tuning Guide | Kernel control groups</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Kernel control groups | SLES 15 SP3"/>
<meta name="description" content="Kernel Control Groups (cgroups) are a kernel feature for assigning and limiting hardware and system resources for processes. Processes can also be or…"/>
<meta name="product-name" content="SUSE Linux Enterprise Server"/>
<meta name="product-number" content="15 SP3"/>
<meta name="book-title" content="System Analysis and Tuning Guide"/>
<meta name="chapter-title" content="Chapter 10. Kernel control groups"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="fs@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise Server 15 SP3"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="System Analysis and Tuning Guide"/>
<meta property="og:description" content="Analyze and tune SLES systems"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="System Analysis and Tuning Guide"/>
<meta name="twitter:description" content="Analyze and tune SLES systems"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    
    "inLanguage": "en",
    

    "headline": "Kernel control groups",
  
    "description": "Kernel Control Groups (cgroups) are a kernel feature for assigning and limiting hardware and system resources for processes. Processes can also be or…",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2024-03-12T00:00+02:00",
      
    "datePublished": "2021-06-22T00:00+02:00",
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="cha-tuning-resources.html" title="Chapter 9. General system resource management"/><link rel="next" href="cha-tuning-numactl.html" title="Chapter 11. Automatic Non-Uniform Memory Access (NUMA) balancing"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">System Analysis and Tuning Guide</a><span> / </span><a class="crumb" href="part-tuning-resources.html">Resource management</a><span> / </span><a class="crumb" href="cha-tuning-cgroups.html">Kernel control groups</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">System Analysis and Tuning Guide</div><ol><li><a href="preface-tuning.html" class=" "><span class="title-number"> </span><span class="title-name">Preface</span></a></li><li><a href="part-tuning-basics.html" class="has-children "><span class="title-number">I </span><span class="title-name">Basics</span></a><ol><li><a href="cha-tuning-basics.html" class=" "><span class="title-number">1 </span><span class="title-name">General notes on system tuning</span></a></li></ol></li><li><a href="part-tuning-monitoring.html" class="has-children "><span class="title-number">II </span><span class="title-name">System monitoring</span></a><ol><li><a href="cha-util.html" class=" "><span class="title-number">2 </span><span class="title-name">System monitoring utilities</span></a></li><li><a href="cha-tuning-syslog.html" class=" "><span class="title-number">3 </span><span class="title-name">System log files</span></a></li></ol></li><li><a href="part-tuning-kerneltrace.html" class="has-children "><span class="title-number">III </span><span class="title-name">Kernel monitoring</span></a><ol><li><a href="cha-tuning-systemtap.html" class=" "><span class="title-number">4 </span><span class="title-name">SystemTap—filtering and analyzing system data</span></a></li><li><a href="cha-tuning-kprobes.html" class=" "><span class="title-number">5 </span><span class="title-name">Kernel probes</span></a></li><li><a href="cha-perf.html" class=" "><span class="title-number">6 </span><span class="title-name">Hardware-based performance monitoring with Perf</span></a></li><li><a href="cha-tuning-oprofile.html" class=" "><span class="title-number">7 </span><span class="title-name">OProfile—system-wide profiler</span></a></li><li><a href="cha-tuning-dynamic-debug.html" class=" "><span class="title-number">8 </span><span class="title-name">Dynamic debug—kernel debugging messages</span></a></li></ol></li><li class="active"><a href="part-tuning-resources.html" class="has-children you-are-here"><span class="title-number">IV </span><span class="title-name">Resource management</span></a><ol><li><a href="cha-tuning-resources.html" class=" "><span class="title-number">9 </span><span class="title-name">General system resource management</span></a></li><li><a href="cha-tuning-cgroups.html" class=" you-are-here"><span class="title-number">10 </span><span class="title-name">Kernel control groups</span></a></li><li><a href="cha-tuning-numactl.html" class=" "><span class="title-number">11 </span><span class="title-name">Automatic Non-Uniform Memory Access (NUMA) balancing</span></a></li><li><a href="cha-tuning-power.html" class=" "><span class="title-number">12 </span><span class="title-name">Power management</span></a></li><li><a href="cha-tuning-tuned.html" class=" "><span class="title-number">13 </span><span class="title-name">Adaptive and dynamic tuning using TuneD</span></a></li></ol></li><li><a href="part-tuning-kernel.html" class="has-children "><span class="title-number">V </span><span class="title-name">Kernel tuning</span></a><ol><li><a href="cha-tuning-io.html" class=" "><span class="title-number">14 </span><span class="title-name">Tuning I/O performance</span></a></li><li><a href="cha-tuning-taskscheduler.html" class=" "><span class="title-number">15 </span><span class="title-name">Tuning the task scheduler</span></a></li><li><a href="cha-tuning-memory.html" class=" "><span class="title-number">16 </span><span class="title-name">Tuning the memory management subsystem</span></a></li><li><a href="cha-tuning-network.html" class=" "><span class="title-number">17 </span><span class="title-name">Tuning the network</span></a></li><li><a href="cha-tuning-sapconf.html" class=" "><span class="title-number">18 </span><span class="title-name">Tuning SUSE Linux Enterprise for SAP</span></a></li></ol></li><li><a href="part-tuning-dumps.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Handling system dumps</span></a><ol><li><a href="cha-tuning-tracing.html" class=" "><span class="title-number">19 </span><span class="title-name">Tracing tools</span></a></li><li><a href="cha-tuning-kexec.html" class=" "><span class="title-number">20 </span><span class="title-name">Kexec and Kdump</span></a></li><li><a href="cha-tuning-systemd-coredump.html" class=" "><span class="title-number">21 </span><span class="title-name">Using <code class="systemitem">systemd-coredump</code> to debug application crashes</span></a></li></ol></li><li><a href="part-tuning-ptp.html" class="has-children "><span class="title-number">VII </span><span class="title-name">Synchronized clocks with Precision Time Protocol</span></a><ol><li><a href="cha-tuning-ptp.html" class=" "><span class="title-number">22 </span><span class="title-name">Precision Time Protocol</span></a></li></ol></li><li><a href="bk07apa.html" class=" "><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-tuning-cgroups" data-id-title="Kernel control groups"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">15 SP3</span></span></span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">Kernel control groups</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Kernel Control Groups (<span class="quote">“<span class="quote">cgroups</span>”</span>) are a kernel feature for
    assigning and limiting hardware and system resources for processes.
    Processes can also be organized in a hierarchical tree structure.
   </p></div></div></div></div><section class="sect1" id="sec-tuning-cgroups-overview" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.1 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#sec-tuning-cgroups-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Every process is assigned exactly one administrative cgroup. cgroups are
   ordered in a hierarchical tree structure. You can set resource limitations,
   such as CPU, memory, disk I/O, or network bandwidth usage, for single
   processes or for whole branches of the hierarchy tree.
  </p><p>
   On <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, <code class="systemitem">systemd</code> uses cgroups to organize all processes in
   groups, which <code class="systemitem">systemd</code> calls slices. <code class="systemitem">systemd</code> also provides an interface
   for setting cgroup properties.
  </p><p>
   The command <code class="command">systemd-cgls</code> displays the hierarchy tree.
  </p><p>
   There are two versions of cgroup APIs provided by the kernel. These differ
   in the cgroup attributes they provide, and in the organization of controller
   hierarchies. <code class="systemitem">systemd</code> attempts to abstract the differences away. By default
   <code class="systemitem">systemd</code> runs in the <span class="emphasis"><em>hybrid</em></span> mode, which means
   controllers are used through the v1 API. cgroup v2 is only used for
   systemd's own tracking. There is also the <span class="emphasis"><em>unified</em></span> mode
   when the controllers are used though v2 API. You may set only one mode.
  </p><p>
   To enable the unified control group hierarchy, append
   <code class="option">systemd.unified_cgroup_hierarchy=1</code> as a kernel command line
   parameter to the GRUB 2 boot loader. (Refer to <span class="intraxref">Book “Administration Guide”, Chapter 14 “The boot loader GRUB 2”</span>
   for more details about configuring GRUB 2.)
  </p></section><section class="sect1" id="sec-tuning-cgroups-accounting" data-id-title="Resource accounting"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.2 </span><span class="title-name">Resource accounting</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#sec-tuning-cgroups-accounting">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Organizing processes into different cgroups can be used to obtain per-cgroup
   resource consumption data.
  </p><p>
   The accounting has relatively small but non-zero overhead, whose impact
   depends on the workload. Activating accounting for one unit will also
   implicitly activate it for all units in the same slice, and for all its
   parent slices, and the units contained in them.
  </p><p>
   The accounting can be set on a per-unit basis with directives such as
   <code class="literal">MemoryAccounting=</code> or globally for all units in
   <code class="filename">/etc/systemd/system.conf</code> with the directive
   <code class="literal">DefaultMemoryAccounting=</code>. Refer to <code class="command">man
   systemd.resource-control</code> for the exhaustive list of possible
   directives.
  </p></section><section class="sect1" id="sec-tuning-cgroups-usage" data-id-title="Setting resource limits"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.3 </span><span class="title-name">Setting resource limits</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#sec-tuning-cgroups-usage">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.9.6.3.5.2" data-id-title="Implicit resource consumption" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Implicit resource consumption</div><p>
    Be aware that resource consumption implicitly depends on the environment
    where your workload executes (for example, size of data structures in
    libraries/kernel, forking behavior of utilities, computational efficiency).
    Hence it is recommended to (re)calibrate your limits should the environment
    change.
   </p></div><p>
   Limitations to cgroups can be set with the <code class="command">systemctl
   set-property</code> command. The syntax is:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl set-property [--runtime] <em class="replaceable">NAME</em> <em class="replaceable">PROPERTY1</em>=<em class="replaceable">VALUE</em> [<em class="replaceable">PROPERTY2</em>=<em class="replaceable">VALUE</em>]</code></pre></div><p>
   The configured value is applied immediately. Optionally, use the
   <code class="option">--runtime</code> option, so that the new values do not persist
   after reboot.
  </p><p>
   Replace <em class="replaceable">NAME</em> with a <code class="systemitem">systemd</code> service, scope, or
   slice name.
  </p><p>
   For a complete list of properties and more details, see <code class="command">man
   systemd.resource-control</code>.
  </p></section><section class="sect1" id="sec-tuning-cgroups-tasksmax" data-id-title="Preventing fork bombs with TasksMax"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.4 </span><span class="title-name">Preventing fork bombs with <code class="literal">TasksMax</code></span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#sec-tuning-cgroups-tasksmax">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="systemitem">systemd</code> supports configuring task count limits both for each individual
   leaf unit, or aggregated on slices. Upstream <code class="systemitem">systemd</code> ships with defaults
   that limit the number of tasks in each unit (15% of the kernel global limit,
   run <code class="command">/usr/sbin/sysctl kernel.pid_max</code> to see the total
   limit). Each user's slice is limited to 33% of the kernel limit. However,
   this is different for SLE.
  </p><section class="sect2" id="sec-tasksmax-defaults" data-id-title="Finding the current default TasksMax values"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.4.1 </span><span class="title-name">Finding the current default <code class="literal">TasksMax</code> values</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#sec-tasksmax-defaults">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    It became apparent, in practice, that there is not a single default that
    applies to all use cases. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> ships with two custom
    configurations that override the upstream defaults for system units and for
    user slices, and sets them both to <code class="literal">infinity</code>.
    <code class="filename">/usr/lib/systemd/system.conf.d/__25-defaults-SLE.conf </code>
    contains these lines:
   </p><div class="verbatim-wrap"><pre class="screen">[Manager]
DefaultTasksMax=infinity</pre></div><p>
    <code class="filename">/usr/lib/systemd/system/user-.slice.d/25-defaults-SLE.conf
    </code> contains these lines:
   </p><div class="verbatim-wrap"><pre class="screen">[Slice]
TasksMax=infinity</pre></div><p>
    Use <code class="command">systemctl</code> to verify the DefaultTasksMax value:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">systemctl show --property DefaultTasksMax</code>
DefaultTasksMax=infinity</pre></div><p>
    <code class="literal">infinity</code> means having no limit. It is not a requirement
    to change the default, but setting some limits may help to prevent system
    crashes from runaway processes.
   </p></section><section class="sect2" id="sec-edit-taskmax-default" data-id-title="Overriding the DefaultTasksMax value"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.4.2 </span><span class="title-name">Overriding the <code class="literal">DefaultTasksMax</code> value</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#sec-edit-taskmax-default">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Change the global <code class="literal">DefaultTasksMax</code> value by creating a
    new override file,
    <code class="filename">/etc/systemd/system.conf.d/90-system-tasksmax.conf</code>,
    and write the following lines to set a new default limit of 256 tasks per
    system unit:
   </p><div class="verbatim-wrap"><pre class="screen">[Manager]
DefaultTasksMax=256</pre></div><p>
    Load the new setting, then verify that it changed:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">systemctl daemon-reload</code>
<code class="prompt user">&gt; </code><code class="command">systemctl show --property DefaultTasksMax</code>
DefaultTasksMax=256</pre></div><p>
    Adjust this default value to suit your needs. You can set different limits
    on individual services as needed. This example is for MariaDB. First check
    the current active value:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">systemctl status mariadb.service</code>
  ● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset&gt;
   Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 11845 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 256)
   CGroup: /system.slice/mariadb.service
           └─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql</pre></div><p>
    The Tasks line shows that MariaDB currently has 30 tasks running, and has
    an upper limit of the default 256, which is inadequate for a database. The
    following example demonstrates how to raise MariaDB's limit to 8192.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">systemctl set-property mariadb.service TasksMax=8192</code>
<code class="prompt user">&gt; </code><code class="command">systemctl status mariadb.service</code> 
● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab&gt;
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─50-TasksMax.conf
   Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
  Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta&gt;
  Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta&gt;
 Main PID: 3452 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 8192)
   CGroup: /system.slice/mariadb.service
           └─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql</pre></div><p>
    <code class="command">systemctl set-property</code> applies the new limit and creates
    a drop-in file for persistence,
    <code class="filename">/etc/systemd/system/mariadb.service.d/50-TasksMax.conf</code>,
    that contains only the changes you want to apply to the existing unit file.
    The value does not have to be 8192, but should be whatever limit is
    appropriate for your workloads.
   </p></section><section class="sect2" id="id-1.9.6.3.6.5" data-id-title="Default TasksMax limit on users"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.4.3 </span><span class="title-name">Default <code class="literal">TasksMax</code> limit on users</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#id-1.9.6.3.6.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The default limit on users should be fairly high, because user sessions
    need more resources. Set your own default for any user by creating a new
    file, for example
    <code class="filename">/etc/systemd/system/user-.slice.d/40-user-taskmask.conf</code>.
    The following example sets a default of 16284:
   </p><div class="verbatim-wrap"><pre class="screen">[Slice]
TasksMax=16284</pre></div><div id="id-1.9.6.3.6.5.4" data-id-title="Numeric prefixes reference" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Numeric prefixes reference</div><p>
     See
     <a class="link" href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in" target="_blank">https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in</a>
     to learn what numeric prefixes are expected for drop-in files.
    </p></div><p>
    Then reload systemd to load the new value, and verify the change:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">systemctl daemon-reload</code>
<code class="prompt user">&gt; </code><code class="command">systemctl show --property TasksMax user-1000.slice</code>
TasksMax=16284</pre></div><p>
    How do you know what values to use? This varies according to your
    workloads, system resources, and other resource configurations. When your
    <code class="literal">TasksMax</code> value is too low, you will see error messages
    such as <span class="emphasis"><em>Failed to fork (Resources temporarily
    unavailable)</em></span>, <span class="emphasis"><em>Can't create thread to handle new
    connection</em></span>, and <span class="emphasis"><em>Error: Function call 'fork' failed
    with error code 11, 'Resource temporarily unavailable'</em></span>.
   </p><p>
    For more information on configuring system resources in systemd, see
    <code class="literal">systemd.resource-control (5)</code>.
   </p></section></section><section class="sect1" id="id-1.9.6.3.7" data-id-title="Controlling I/O with proportional weight policy"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.5 </span><span class="title-name">Controlling I/O with proportional weight policy</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#id-1.9.6.3.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section introduces using the Linux kernel's block I/O controller to
   prioritize I/O operations. The cgroup blkio subsystem controls and monitors
   access to I/O on block devices. State objects that contain the subsystem
   parameters for a cgroup are represented as pseudo-files within the cgroup
   virtual file system, also called a pseudo-file system.
  </p><p>
   The examples in this section show how writing values to some of these
   pseudo-files limits access or bandwidth, and reading values from some of
   these pseudo-files provides information on I/O operations. Examples are
   provided for both cgroup-v1 and cgroup-v2.
  </p><p>
   You need a test directory containing two files for testing performance and
   changed settings. A quick way to create test files fully populated with text
   is using the <code class="command">yes</code> command. The following example commands
   create a test directory, and then populate it with two 537 MB text files:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">host1:~ # </code>mkdir /io-cgroup
<code class="prompt root">host1:~ # </code>cd /io-cgroup
<code class="prompt root">host1:~ # </code>yes this is a test file | head -c 537MB &gt; file1.txt
<code class="prompt root">host1:~ # </code>yes this is a test file | head -c 537MB &gt; file2.txt</pre></div><p>
   To run the examples open three command shells. Two shells are for reader
   processes, and one shell is for running the steps that control I/O. In the
   examples, each command prompt is labeled to indicate if it represents one of
   the reader processes, or I/O.
  </p><section class="sect2" id="id-1.9.6.3.7.7" data-id-title="Using cgroup-v1"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.5.1 </span><span class="title-name">Using cgroup-v1</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#id-1.9.6.3.7.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following proportional weight policy files can be used to grant a
    reader process a higher priority for I/O operations than other reader
    processes accessing the same disk.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="filename">blkio.bfq.weight</code> (available in kernels starting with
      version 5.0 with blk-mq and when using the BFQ I/O scheduler)
     </p></li></ul></div><p>
    To test this, run a single reader process (in the examples, reading from an
    SSD) without controlling its I/O, using <code class="filename">file2.txt</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[io-controller] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[io-controller] host1:/io-cgroup # </code>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s</pre></div><p>
    Now run a background process reading from the same disk:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[reader1] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[reader1] host1:/io-cgroup # </code>echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
<code class="prompt root">[reader2] host1:/io-cgroup # </code>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s</pre></div><p>
    Each process gets half of the throughput for I/O operations. Next, set up
    two control groups—one for each process—verify that BFQ is
    used, and set a different weight for reader2:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[io-controller] host1:/io-cgroup # </code>cd /sys/fs/cgroup/blkio/
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>mkdir reader1
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>mkdir reader2
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>echo 5220 &gt; reader1/cgroup.procs
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>echo 5251 &gt; reader2/cgroup.procs
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>cat reader1/blkio.bfq.weight
100
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>echo 200 &gt; reader2/blkio.bfq.weight
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>cat reader2/blkio.bfq.weight
200</pre></div><p>
    With these settings and reader1 in the background, reader2 should have
    higher throughput than previously:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[reader1] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[reader1] host1:/io-cgroup # </code>echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
<code class="prompt root">[reader2] host1:/io-cgroup # </code>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s</pre></div><p>
    The higher proportional weight resulted in higher throughput for reader2.
    Now double its weight again:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>cat reader1/blkio.bfq.weight
100
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>echo 400 &gt; reader2/blkio.bfq.weight
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </code>cat reader2/blkio.bfq.weight
400</pre></div><p>
    This results in another increase in throughput for reader2:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[reader1] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[reader1] host1:/io-cgroup # </code>echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
<code class="prompt root">[reader2] host1:/io-cgroup # </code>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s</pre></div></section><section class="sect2" id="id-1.9.6.3.7.8" data-id-title="Using cgroup-v2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.5.2 </span><span class="title-name">Using cgroup-v2</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#id-1.9.6.3.7.8">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    First set up your test environment as shown at the beginning of this
    chapter.
   </p><p>
    Then make sure that the Block IO controller is not active, as that is for
    cgroup-v1. To do this, boot with kernel parameter
    <code class="option">cgroup_no_v1=blkio</code>. Verify that this parameter was used,
    and that the IO controller (cgroup-v2) is available:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[io-controller] host1:/io-cgroup # </code>cat /proc/cmdline
BOOT_IMAGE=... cgroup_no_v1=blkio ...
<code class="prompt root">[io-controller] host1:/io-cgroup # </code>cat /sys/fs/cgroup/unified/cgroup.controllers
io</pre></div><p>
    Next, enable the IO controller:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[io-controller] host1:/io-cgroup # </code>cd /sys/fs/cgroup/unified/
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>echo '+io' &gt; cgroup.subtree_control
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>cat cgroup.subtree_control
io</pre></div><p>
    Now run all the test steps, similarly to the steps for cgroup-v1. Note that
    some of the directories are different. Run a single reader process (in the
    examples, reading from an SSD) without controlling its I/O, using
    file2.txt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>cd -
<code class="prompt root">[io-controller] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[io-controller] host1:/io-cgroup # </code>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5633
[...]</pre></div><p>
    Run a background process reading from the same disk and note your
    throughput values:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[reader1] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[reader1] host1:/io-cgroup # </code>echo $$; dd if=file1.txt of=/dev/null bs=4k
5633
[...]
<code class="prompt root">[reader2] host1:/io-cgroup # </code>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5703
[...]</pre></div><p>
    Each process gets half of the throughput for I/O operations. Set up two
    control groups—one for each process—verify that BFQ is the
    active scheduler, and set a different weight for reader2:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[io-controller] host1:/io-cgroup # </code>cd -
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>mkdir reader1
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>mkdir reader2
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>echo 5633 &gt; reader1/cgroup.procs
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>echo 5703 &gt; reader2/cgroup.procs
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>cat reader1/io.bfq.weight
default 100
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>echo 200 &gt; reader2/io.bfq.weight
<code class="prompt root">[io-controller] host1:/sys/fs/cgroup/unified # </code>cat reader2/io.bfq.weight
default 200</pre></div><p>
    Test your throughput with the new settings. reader2 should show an increase
    in throughput.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[reader1] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[reader1] host1:/io-cgroup # </code>echo $$; dd if=file1 of=/dev/null bs=4k
5633
[...]
<code class="prompt root">[reader2] host1:/io-cgroup # </code>echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5703
[...]</pre></div><p>
    Try doubling the weight again for reader2, and testing the new setting:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">[reader2] host1:/io-cgroup # </code>echo 400 &gt; reader1/blkio.bfq.weight
<code class="prompt root">[reader2] host1:/io-cgroup # </code>cat reader2/blkio.bfq.weight
400
<code class="prompt root">[reader1] host1:/io-cgroup # </code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<code class="prompt root">[reader1] host1:/io-cgroup # </code>echo $$; dd if=file1.txt of=/dev/null bs=4k
[...]
<code class="prompt root">[reader2] host1:/io-cgroup # </code>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
[...]</pre></div></section></section><section class="sect1" id="id-1.9.6.3.8" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.6 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="cha-tuning-cgroups.html#id-1.9.6.3.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE15SP3/xml/tuning_cgroups.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Kernel documentation (package <code class="systemitem">kernel-source</code>):
     files in
     <code class="filename">/usr/src/linux/Documentation/admin-guide/cgroup-v1</code>
     and file
     <code class="filename">/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst</code>.
    </p></li><li class="listitem"><p>
     <a class="link" href="https://lwn.net/Articles/604609/" target="_blank">https://lwn.net/Articles/604609/</a>—Brown, Neil:
     Control Groups Series (2014, 7 parts).
    </p></li><li class="listitem"><p>
     <a class="link" href="https://lwn.net/Articles/243795/" target="_blank">https://lwn.net/Articles/243795/</a>—Corbet,
     Jonathan: Controlling memory use in containers (2007).
    </p></li><li class="listitem"><p>
     <a class="link" href="https://lwn.net/Articles/236038/" target="_blank">https://lwn.net/Articles/236038/</a>—Corbet,
     Jonathan: Process containers (2007).
    </p></li></ul></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-tuning-resources.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 9 </span>General system resource management</span></a> </div><div><a class="pagination-link next" href="cha-tuning-numactl.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 11 </span>Automatic Non-Uniform Memory Access (NUMA) balancing</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-tuning-cgroups.html#sec-tuning-cgroups-overview"><span class="title-number">10.1 </span><span class="title-name">Overview</span></a></span></li><li><span class="sect1"><a href="cha-tuning-cgroups.html#sec-tuning-cgroups-accounting"><span class="title-number">10.2 </span><span class="title-name">Resource accounting</span></a></span></li><li><span class="sect1"><a href="cha-tuning-cgroups.html#sec-tuning-cgroups-usage"><span class="title-number">10.3 </span><span class="title-name">Setting resource limits</span></a></span></li><li><span class="sect1"><a href="cha-tuning-cgroups.html#sec-tuning-cgroups-tasksmax"><span class="title-number">10.4 </span><span class="title-name">Preventing fork bombs with <code class="literal">TasksMax</code></span></a></span></li><li><span class="sect1"><a href="cha-tuning-cgroups.html#id-1.9.6.3.7"><span class="title-number">10.5 </span><span class="title-name">Controlling I/O with proportional weight policy</span></a></span></li><li><span class="sect1"><a href="cha-tuning-cgroups.html#id-1.9.6.3.8"><span class="title-number">10.6 </span><span class="title-name">More information</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>