<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><title>SLES 15 SP4 | Virtualization Best Practices</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Virtualization Best Practices | SLES 15 SP4"/>
<meta name="description" content="How to choose the optimal virtualization technology based on workload"/>
<meta name="product-name" content="SUSE Linux Enterprise Server"/>
<meta name="product-number" content="15 SP4"/>
<meta name="book-title" content="Virtualization Best Practices"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="taroth@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise Server 15 SP4"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Virtualization Best Practices"/>
<meta property="og:description" content="Make the best virtualization choice for your setup"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Virtualization Best Practices"/>
<meta name="twitter:description" content="Make the best virtualization choice for your setup"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    
    "inLanguage": "en",
    

    "headline": "Virtualization Best Practices",
  
    "description": "How to choose the optimal virtualization technology based on workload",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2024-03-04T00:00+02:00",
      
    "datePublished": "2022-06-21T00:00+02:00",
      

    "about": [
      {
        "@type": "Thing",
        "name": "Virtualization"
      }
    ],
  
    "mentions": [
      
      { "@type": "SoftwareApplication",
        "name": "SUSE Linux Enterprise Server",
        "softwareVersion": "15 SP4",
        "applicationCategory": "Operating System",
        "operatingSystem": "Linux"
      }
      
    ],
    
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#article-virtualization-best-practices">Virtualization Best Practices</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="article" id="article-virtualization-best-practices" data-id-title="Virtualization Best Practices"><div class="titlepage"><div><div class="big-version-info"><span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">15 SP4</span></span></span></div><div><h1 class="title">Virtualization Best Practices</h1></div><div class="date"><span class="imprint-label">Publication Date: </span>
    October 02, 2024

   </div><div><div class="titlepage-revhistory"><a aria-label="Revision History" hreflang="en" href="rh-article-virtualization-best-practices.html" target="_blank">Revision History: Virtualization Best Practices</a></div></div></div></div><section class="sect1" id="sec-vt-best-scenario" data-id-title="Virtualization scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Virtualization scenarios</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-scenario">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Virtualization offers a lot of capabilities to your environment. It can
      be used in multiple scenarios. To get more details about it, refer to the
      <span class="intraxref">Book “Virtualization Guide”</span> and in particular, to the following
      sections:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <span class="intraxref">Book “Virtualization Guide”, Chapter 2 “Virtualization scenarios”, Section 2.1 “Server consolidation”</span>
        </p></li><li class="listitem"><p>
          <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization technology”, Section 1.2 “Virtualization benefits”</span>
        </p></li></ul></div><p>
      This best practice guide will provide advice for making the right choice
      in your environment. It will recommend or discourage the usage of options
      depending on your workload. Fixing configuration issues and performing
      tuning tasks will increase the performance of VM Guest's near to bare
      metal.
    </p></section><section class="sect1" id="sec-vt-best-intro" data-id-title="Before you apply modifications"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Before you apply modifications</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-intro-backup" data-id-title="Back up first"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Back up first</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-backup">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Changing the configuration of the VM Guest or the VM Host Server can lead to
        data loss or an unstable state. It is really important that you do
        backups of files, data, images, etc. before making any changes. Without
        backups you cannot restore the original state after a data loss or a
        misconfiguration. Do not perform tests or experiments on production
        systems.
      </p></section><section class="sect2" id="sec-vt-best-intro-testing" data-id-title="Test your workloads"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Test your workloads</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-testing">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The efficiency of a virtualization environment depends on many factors.
       This guide helps to make good choices when configuring virtualization
       in a production environment. Nothing is
        <span class="emphasis"><em>carved in stone</em></span>. Hardware, workloads, resource
        capacity, etc. should all be considered when planning, testing, and
        deploying your virtualization infrastructure. Testing your virtualized
        workloads is vital to a successful virtualization implementation.
      </p></section></section><section class="sect1" id="sec-vt-best-reco" data-id-title="Recommendations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Recommendations</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-reco">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-intro-libvirt" data-id-title="Prefer the libvirt framework"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Prefer the <code class="systemitem">libvirt</code> framework</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-libvirt">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        SUSE strongly recommends using the <code class="systemitem">libvirt</code> framework to configure,
        manage, and operate VM Host Servers and VM Guest. It offers a single
        interface (GUI and shell) for all supported virtualization technologies
        and therefore is easier to use than the hypervisor-specific tools.
      </p><p>
        We do not recommend using libvirt and hypervisor-specific tools at the
        same time, because changes done with the hypervisor-specific tools may
        not be recognized by the libvirt tool set. See
        <span class="intraxref">Book “Virtualization Guide”, Chapter 8 “<code class="systemitem">libvirt</code> daemons”</span> for more information on libvirt.
      </p></section><section class="sect2" id="sec-vt-best-intro-qemu" data-id-title="qemu-system-i386 compared to qemu-system-x86_64"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">qemu-system-i386 compared to qemu-system-x86_64</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-qemu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Similar to real 64-bit PC hardware,
        <code class="command">qemu-system-x86_64</code> supports VM Guests running a
        32-bit or a 64-bit operating system. Because
        <code class="command">qemu-system-x86_64</code> usually also provides better
        performance for 32-bit guests, SUSE generally recommends using
        <code class="command">qemu-system-x86_64</code> for both 32-bit and 64-bit
        VM Guests on KVM. Scenarios where <code class="command">qemu-system-i386</code>
        is known to perform better are not supported by SUSE.
      </p><p>
        Xen also uses binaries from the qemu package but prefers
        <code class="command">qemu-system-i386</code>, which can be used for both 32-bit
        and 64-bit Xen VM Guests. To maintain compatibility with the upstream
        Xen Community, SUSE encourages using
        <code class="command">qemu-system-i386</code> for Xen VM Guests.
      </p></section></section><section class="sect1" id="sec-vt-best-hostlevel" data-id-title="VM Host Server configuration and resource allocation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">VM Host Server configuration and resource allocation</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-hostlevel">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Allocation of resources for VM Guests is a crucial point when
      administrating virtual machines. When assigning resources to VM Guests,
      be aware that overcommitting resources may affect the performance of the
      VM Host Server and the VM Guests. If all VM Guests request all their
      resources simultaneously, the host needs to be able to provide all of
      them. If not, the host's performance will be negatively affected and this
      will in turn also have negative effects on the VM Guest's performance.
    </p><section class="sect2" id="sec-vt-best-mem" data-id-title="Memory"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Memory</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Linux manages memory in units called pages. On most systems the default
        page size is 4 KB. Linux and the CPU need to know which pages belong to
        which process. That information is stored in a page table. If a lot of
        processes are running, it takes more time to find where the memory is
        mapped, because of the time required to search the page table. To speed
        up the search, the TLB (Translation Lookaside Buffer) was invented. But
        on a system with a lot of memory, the TLB is not enough. To avoid any
        fallback to normal page table (resulting in a cache miss, which is time
        consuming), huge pages can be used. Using huge pages will reduce TLB
        overhead and TLB misses (pagewalk). A host with 32 GB
        (32*1014*1024 = 33,554,432 KB) of memory and a 4 KB page size
        has a TLB with <span class="emphasis"><em>33,554,432/4 = 8,388,608</em></span> entries.
        Using a 2 MB (2048 KB) page size, the TLB only has
        <span class="emphasis"><em>33554432/2048 = 16384</em></span> entries, considerably
        reducing the TLB misses.
      </p><section class="sect3" id="sec-vt-best-mem-huge-pages" data-id-title="Configuring the VM Host Server and the VM Guest to use huge pages"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.1 </span><span class="title-name">Configuring the VM Host Server and the VM Guest to use huge pages</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-huge-pages">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The AMD64/Intel 64 CPU architecture supports larger pages than 4 KB: huge
          pages. To determine the size of huge pages available on your system
          (could be 2 MB or 1 GB), check the <code class="literal">flags</code>
          line in the output of <code class="filename">/proc/cpuinfo</code> for
          occurrences of <code class="literal">pse</code> and/or
          <code class="literal">pdpe1gb</code>.
        </p><div class="table" id="id-1.13.4.5.3.3.3" data-id-title="Determine the available huge pages size"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 1: </span><span class="title-name">Determine the available huge pages size </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.5.3.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 30%; " class="1"/><col style="width: 70%; " class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    CPU flag
                  </p>
                </th><th style="border-bottom: 1px solid ; ">
                  <p>
                    Huge pages size available
                  </p>
                </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Empty string
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    No huge pages available
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    pse
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    2 MB
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; ">
                  <p>
                    pdpe1gb
                  </p>
                </td><td>
                  <p>
                    1 GB
                  </p>
                </td></tr></tbody></table></div></div><p>
          Using huge pages improves the performance of VM Guests and reduces
          host memory consumption.
        </p><p>
          By default, the system uses THP. To make huge pages available on your
          system, activate it at boot time with <code class="option">hugepages=1</code>,
          and—optionally—add the huge pages size with, for example,
          <code class="option">hugepagesz=2MB</code>.
        </p><div id="id-1.13.4.5.3.3.6" data-id-title="1 GB huge pages" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: 1 GB huge pages</div><p>
            1 GB pages can only be allocated at boot time and cannot be
            freed afterward.
          </p></div><p>
          To allocate and use the huge page table (HugeTlbPage), you need to
          mount <code class="filename">hugetlbfs</code> with correct permissions.
        </p><div id="id-1.13.4.5.3.3.8" data-id-title="Restrictions of huge pages" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Restrictions of huge pages</div><p>
            Even if huge pages provide the best performance, they do come with
            some drawbacks. You lose features such as Memory ballooning (see
            <a class="xref" href="#sec-vt-best-vmguests-virtio-balloon" title="6.1.3. virtio balloon">Section 6.1.3, “virtio balloon”</a>), KSM (see
            <a class="xref" href="#sec-vt-best-perf-ksm" title="4.1.4. KSM and page sharing">Section 4.1.4, “KSM and page sharing”</a>), and huge pages cannot be
            swapped.
          </p></div><div class="procedure" id="id-1.13.4.5.3.3.9" data-id-title="Configuring the use of huge pages"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 2: </span><span class="title-name">Configuring the use of huge pages </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.5.3.3.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Mount <code class="literal">hugetlbfs</code> to
              <code class="filename">/dev/hugepages</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  mount -t hugetlbfs hugetlbfs /dev/hugepages</pre></div></li><li class="step"><p>
              To reserve memory for huge pages, use the
              <code class="command">sysctl</code> command. If your system has a huge page
              size of 2 MB (2048 KB), and you want to reserve
              1 GB (1,048,576 KB) for your VM Guest, you need
              <span class="emphasis"><em>1,048,576/2048=512</em></span> pages in the pool:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sysctl vm.nr_hugepages=<em class="replaceable">512</em></pre></div><p>
              The value is written to
              <code class="filename">/proc/sys/vm/nr_hugepages</code> and represents the
              current number of <span class="emphasis"><em>persistent</em></span> huge pages in
              the kernel's huge page pool. <span class="emphasis"><em>Persistent</em></span> huge
              pages will be returned to the huge page pool when freed by a
              task.
            </p></li><li class="step"><p>
              Add the <code class="literal">memoryBacking</code> element in the VM Guest
              configuration file (by running <code class="command">virsh edit
              <em class="replaceable">CONFIGURATION</em></code>).
            </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</pre></div></li><li class="step"><p>
              Start your VM Guest and check on the host whether it uses
              hugepages:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/meminfo | grep HugePages_
HugePages_Total:<span class="callout" id="co-hp-total">1</span>     512
HugePages_Free:<span class="callout" id="co-hp-free">2</span>       92
HugePages_Rsvd:<span class="callout" id="co-hp-rsvd">3</span>        0
HugePages_Surp:<span class="callout" id="co-hp-surp">4</span>        0</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-total"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Size of the pool of huge pages
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-free"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Number of huge pages in the pool that are not yet allocated
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-rsvd"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Number of huge pages for which a commitment to allocate from
                  the pool has been made, but no allocation has yet been made
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-surp"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Number of huge pages in the pool above the value in
                  <code class="filename">/proc/sys/vm/nr_hugepages</code>. The maximum
                  number of surplus huge pages is controlled by
                  <code class="filename">/proc/sys/vm/nr_overcommit_hugepages</code>
                </p></td></tr></table></div></li></ol></div></div></section><section class="sect3" id="sec-vt-best-mem-thp" data-id-title="Transparent huge pages"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.2 </span><span class="title-name">Transparent huge pages</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-thp">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Transparent huge pages (THP) provide a way to dynamically allocate
          huge pages with the <code class="command">khugepaged</code> kernel thread,
          rather than manually managing their allocation and use. Workloads
          with contiguous memory access patterns can benefit greatly from THP.
          A 1000 fold decrease in page faults can be observed when running
          synthetic workloads with contiguous memory access patterns.
          Conversely, workloads with sparse memory access patterns (like
          databases) may perform poorly with THP. In such cases it may be
          preferable to disable THP by adding the kernel parameter
          <code class="option">transparent_hugepage=never</code>, rebuild your grub2
          configuration, and reboot. Verify if THP is disabled with:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</pre></div><p>
          If disabled, the value <code class="literal">never</code> is shown in square
          brackets like in the example above.
        </p><div id="id-1.13.4.5.3.4.5" data-id-title="Xen" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Xen</div><p>
            THP is not available under Xen.
          </p></div></section><section class="sect3" id="sec-vt-best-mem-xen" data-id-title="Xen-specific memory notes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.3 </span><span class="title-name">Xen-specific memory notes</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-xen">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect4" id="sec-vt-best-mem-xen-dom-0" data-id-title="Managing domain-0 memory"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.1.3.1 </span><span class="title-name">Managing domain-0 memory</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-xen-dom-0">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            In previous versions of SUSE Linux Enterprise Server, the default memory allocation scheme
            of a Xen host was to allocate all host physical memory to Dom0
            and enable auto-ballooning. Memory was automatically ballooned from
            Dom0 when additional domains were started. This behavior has
            always been error prone and disabling it was strongly encouraged.
            Starting in SUSE Linux Enterprise Server 15 SP1, auto-ballooning has been disabled by
            default and Dom0 is given 10% of host physical memory +
            1 GB. For example, on a host with 32 GB of physical
            memory, 4.2 GB of memory is allocated to Dom0.
          </p><p>
            The use of <code class="option">dom0_mem</code> Xen command-line option in
            <code class="filename">/etc/default/grub</code> is still supported and
            encouraged (see <a class="xref" href="#sec-vt-best-kernel-parameter" title="7.5. Change kernel parameters at boot time">Section 7.5, “Change kernel parameters at boot time”</a> for
            more information). You can restore the old behavior by setting
            <code class="option">dom0_mem</code> to the host physical memory size and
            enabling the <code class="option">autoballoon</code> setting in
            <code class="filename">/etc/xen/xl.conf</code>.
          </p></section></section><section class="sect3" id="sec-vt-best-perf-ksm" data-id-title="KSM and page sharing"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.4 </span><span class="title-name">KSM and page sharing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-ksm">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Kernel Samepage Merging is a kernel feature that reduces memory
          consumption on the VM Host Server by sharing blocks of memory that
          VM Guests have in common. The KSM daemon
          <code class="systemitem">ksmd</code> periodically scans
          user memory, looking for pages with identical contents, which can be
          replaced by a single write-protected page. To enable the KSM service,
          first make sure that the package <span class="package">qemu-ksm</span> is
          installed, then run the command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  systemctl enable --now ksm.service</pre></div><p>
          Alternatively, it can also be started by running the command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 1 &gt; /sys/kernel/mm/ksm/run</pre></div><p>
          One advantage of using KSM from a VM Guest's perspective is that all
          guest memory is backed by host anonymous memory. You can share
          <span class="emphasis"><em>pagecache</em></span>, <span class="emphasis"><em>tmpfs</em></span> or any
          kind of memory allocated in the guest.
        </p><p>
          KSM is controlled by <code class="systemitem">sysfs</code>. You can check
          KSM's values in <code class="filename">/sys/kernel/mm/ksm/</code>:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <code class="literal">pages_shared</code>: the number of shared pages that
              are being used (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">pages_sharing</code>: the number of sites sharing the
              pages (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">pages_unshared</code>: the number of pages that are
              unique and repeatedly checked for merging (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">pages_volatile</code>: the number of pages that are
              changing too fast to be considered for merging (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">full_scans</code>: the number of times all mergeable
              areas have been scanned (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">sleep_millisecs</code>: the number of milliseconds
              <code class="systemitem">ksmd</code> should sleep before
              the next scan. A low value will overuse the CPU, consuming CPU
              time that could be used for other tasks. We recommend a value
              greater than <code class="literal">1000</code>.
            </p></li><li class="listitem"><p>
              <code class="literal">pages_to_scan</code>: the number of present pages to
              scan before ksmd goes to sleep. A high value will overuse the
              CPU. We recommend starting with a value of
              <code class="literal">1000</code> and then adjusting as necessary based on
              the KSM results observed while testing your deployment.
            </p></li><li class="listitem"><p>
              <code class="literal">merge_across_nodes</code>: by default, the system
              merges pages across NUMA nodes. Set this option to
              <code class="literal">0</code> to disable this behavior.
            </p></li></ul></div><div id="id-1.13.4.5.3.6.9" data-id-title="Use cases" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Use cases</div><p>
            KSM is a good technique to over-commit host memory when running
            multiple instances of the same application or VM Guest. When
            applications and VM Guest are heterogeneous and do not share any
            common data, it is preferable to disable KSM. To do that, run:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  systemctl disable --now ksm.service</pre></div><p>
            Alternatively, it can also be disabled by running the command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 0 &gt; /sys/kernel/mm/ksm/run</pre></div><p>
            In a mixed heterogeneous and homogeneous environment, KSM can be
            enabled on the host but disabled on a per VM Guest basis. Use
            <code class="command">virsh edit</code> to disable page sharing of a
            VM Guest by adding the following to the guest's XML configuration:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</pre></div></div><div id="id-1.13.4.5.3.6.10" data-id-title="Avoid out-of-memory conditions" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Avoid out-of-memory conditions</div><p>
            KSM can free up some memory on the host system, but the
            administrator should reserve enough swap to avoid out-of-memory
            conditions if that shareable memory decreases. If the amount of
            shareable memory decreases, the use of physical memory is
            increased.
          </p></div><div id="id-1.13.4.5.3.6.11" data-id-title="Memory access latencies" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Memory access latencies</div><p>
            By default, KSM will merge common pages across NUMA nodes. If the
            merged, common page is now located on a distant NUMA node (relative
            to the node running the VM Guest vCPUs), this may degrade
            VM Guest performance. If increased memory access latencies are
            noticed in the VM Guest, disable cross-node merging with the
            <code class="literal">merge_across_nodes</code> sysfs control:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 0 &gt; /sys/kernel/mm/ksm/merge_across_nodes</pre></div></div></section><section class="sect3" id="sec-vt-best-mem-hot" data-id-title="VM Guest: memory hotplug"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.5 </span><span class="title-name">VM Guest: memory hotplug</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-hot">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          To optimize the usage of your host memory, it may be useful to
          hotplug more memory for a running VM Guest when required. To support
          memory hotplugging, you must first configure the
          <code class="literal">&lt;maxMemory&gt;</code> tag in the VM Guest's
          configuration file:
        </p><div class="verbatim-wrap"><pre class="screen">&lt;maxMemory<span class="callout" id="co-mem-hot-max">1</span> slots='16'<span class="callout" id="co-mem-hot-slots">2</span> unit='KiB'&gt;20971520<span class="callout" id="co-mem-hot-size">3</span>&lt;/maxMemory&gt;
  &lt;memory<span class="callout" id="co-mem-hot-mem">4</span> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<span class="callout" id="co-mem-hot-curr">5</span> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-max"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Runtime maximum memory allocation of the guest.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-slots"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Number of slots available for adding memory to the guest
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-size"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Valid units are:
            </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                  "KB" for kilobytes (1,000 bytes)
                </p></li><li class="listitem"><p>
                  "k" or "KiB" for kibibytes (1,024 bytes)
                </p></li><li class="listitem"><p>
                  "MB" for megabytes (1,000,000 bytes)
                </p></li><li class="listitem"><p>
                  "M" or "MiB" for mebibytes (1,048,576 bytes)
                </p></li><li class="listitem"><p>
                  "GB" for gigabytes (1,000,000,000 bytes)
                </p></li><li class="listitem"><p>
                  "G" or "GiB" for gibibytes (1,073,741,824 bytes)
                </p></li><li class="listitem"><p>
                  "TB" for terabytes (1,000,000,000,000 bytes)
                </p></li><li class="listitem"><p>
                  "T" or "TiB" for tebibytes (1,099,511,627,776 bytes)
                </p></li></ul></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-mem"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum allocation of memory for the guest at boot time
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-curr"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Actual allocation of memory for the guest
            </p></td></tr></table></div><p>
          To hotplug memory devices into the slots, create a file
          <code class="filename">mem-dev.xml</code> like the following:
        </p><div class="verbatim-wrap"><pre class="screen">&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'&gt;524287&lt;/size&gt;
  &lt;node&gt;0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</pre></div><p>
          And attach it with the following command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh attach-device vm-name mem-dev.xml</pre></div><p>
          For memory device hotplug, the guest must have at least 1 NUMA cell
          defined (see <a class="xref" href="#sec-vt-best-perf-numa-vmguest-topo" title="4.6.3.1. VM Guest virtual NUMA topology">Section 4.6.3.1, “VM Guest virtual NUMA topology”</a>).
        </p></section></section><section class="sect2" id="sec-vt-best-perf-swap" data-id-title="Swap"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Swap</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-swap">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        <span class="emphasis"><em>Swap</em></span> is usually used by the system to store
        underused physical memory (low usage, or not accessed for a long time).
        To prevent the system running out of memory, setting up a minimum swap
        is highly recommended.
      </p><section class="sect3" id="sec-vt-best-perf-swap-swappiness" data-id-title="swappiness"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.2.1 </span><span class="title-name"><code class="literal">swappiness</code></span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-swap-swappiness">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The <code class="literal">swappiness</code> setting controls your system's swap
          behavior. It defines how memory pages are swapped to disk. A high
          value of <span class="emphasis"><em>swappiness</em></span> results in a system that
          swaps more often. Available values range from <code class="literal">0</code> to
          <code class="literal">200</code>. A value of <code class="literal">200</code> tells the
          system to find inactive pages and put them in swap. A value of
          <code class="option">0</code> disables swapping.
          
        </p><p>
          To do some testing on a live system, change the value of
          <code class="filename">/proc/sys/vm/swappiness</code> on the fly and check the
          memory usage afterward:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 35 &gt; /proc/sys/vm/swappiness</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>free -h
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</pre></div><p>
          To permanently set a swappiness value, add a line in
          <code class="filename">/etc/systcl.conf</code>, for example:
        </p><div class="verbatim-wrap"><pre class="screen">vm.swappiness = 35</pre></div><p>
          You can also control the swap by using the
          <code class="literal">swap_hard_limit</code> element in the XML configuration
          of your VM Guest. Before setting this parameter and using it in a
          production environment, do some testing because the host can
          terminate the domain if the value is too low.
        </p><div class="verbatim-wrap"><pre class="screen">&lt;memtune&gt;<span class="callout" id="co-mem-1">1</span>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<span class="callout" id="co-mem-hard">2</span>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<span class="callout" id="co-mem-soft">3</span>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<span class="callout" id="co-mem-swap">4</span>
&lt;/memtune&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-1"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              This element provides memory tunable parameters for the domain.
              If this is omitted, it defaults to the defaults provided b the
              operating system.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hard"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum memory the guest can use. To avoid any problems on the
              VM Guest it is strongly recommended not to use this parameter.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-soft"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The memory limit to enforce during memory contention.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-swap"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The maximum memory plus swap the VM Guest can use.
            </p></td></tr></table></div></section></section><section class="sect2" id="sec-vt-best-io" data-id-title="I/O"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">I/O</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-io">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect3" id="sec-vt-best-perf-io" data-id-title="I/O scheduler"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.1 </span><span class="title-name">I/O scheduler</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-io">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The I/O scheduler for SUSE Linux Enterprise 15 SP2 and up is Budget Fair Queueing
          (BFQ). The main aim of the BFQ scheduler is to provide a fair
          allocation of the disk I/O bandwidth for all processes that request
          an I/O operation. You can have different I/O schedulers for different
          devices.
        </p><p>
          To get better performance in host and VM Guest, use
          <code class="literal">none</code> in the VM Guest (disable the I/O scheduler)
          and the <code class="literal">mq-deadline</code> scheduler for a virtualization
          host.
        </p><div class="procedure" id="id-1.13.4.5.5.3.4" data-id-title="Checking and changing the I/O scheduler at runtime"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3: </span><span class="title-name">Checking and changing the I/O scheduler at runtime </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.5.5.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To check your current I/O scheduler for your disk (replace
              <em class="replaceable">sdX</em> by the disk you want to check),
              run:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/block/<em class="replaceable">sdX</em>/queue/scheduler
mq-deadline kyber [bfq] none</pre></div><p>
              The value in square brackets is the one currently selected
              (<code class="literal">bfq</code> in the example above).
            </p></li><li class="step"><p>
              You can change the scheduler at runtime by running the following
              command as <code class="systemitem">root</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo mq-deadline &gt; /sys/block/<em class="replaceable">sdX</em>/queue/scheduler</pre></div></li></ol></div></div><p>
          If you need to specify different I/O schedulers for each disk, create
          the file <code class="filename">/usr/lib/tmpfiles.d/IO_ioscheduler.conf</code>
          with content similar to the following example. It defines the
          <code class="literal">mq-deadline</code> scheduler for
          <code class="filename">/dev/sda</code> and the <code class="literal">none</code>
          scheduler for <code class="filename">/dev/sdb</code>. Keep in mind that the
          device name can be different depending on the device type. This
          feature is available on SLE 12 and up.
        </p><div class="verbatim-wrap"><pre class="screen">w /sys/block/sda/queue/scheduler - - - - mq-deadline
w /sys/block/sdb/queue/scheduler - - - - none</pre></div></section><section class="sect3" id="sec-vt-best-io-async" data-id-title="Asynchronous I/O"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.2 </span><span class="title-name">Asynchronous I/O</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-io-async">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Many of the virtual disk back-ends use Linux Asynchronous I/O (aio)
          in their implementation. By default, the maximum number of aio
          contexts is set to 65536, which can be exceeded when running hundreds
          of VM Guests using virtual disks serviced by Linux Asynchronous I/O.
          When running large numbers of VM Guests on a VM Host Server, consider
          increasing /proc/sys/fs/aio-max-nr.
        </p><div class="procedure" id="id-1.13.4.5.5.4.3" data-id-title="Checking and changing aio-max-nr at runtime"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4: </span><span class="title-name">Checking and changing aio-max-nr at runtime </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.5.5.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To check your current aio-max-nr setting run:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/sys/fs/aio-max-nr
65536</pre></div></li><li class="step"><p>
              You can change aio-max-nr at runtime with the following command:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 131072 &gt; /proc/sys/fs/aio-max-nr</pre></div></li></ol></div></div><p>
          To permanently set <code class="option">aio-max-nr</code>, add an entry to a
          custom sysctl file. For example, include the following to
          <code class="filename">/etc/sysctl.d/aio-max-nr.conf</code>:
        </p><div class="verbatim-wrap"><pre class="screen">fs.aio-max-nr = 1048576</pre></div></section><section class="sect3" id="sec-vt-best-io-techniques" data-id-title="I/O Virtualization"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.3 </span><span class="title-name">I/O Virtualization</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-io-techniques">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          SUSE products support various I/O virtualization technologies. The
          following table lists advantages and disadvantages of each
          technology. For more information about I/O in virtualization refer to
          the <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization technology”, Section 1.4 “I/O virtualization”</span>.
        </p><div class="table" id="id-1.13.4.5.5.5.3" data-id-title="I/O Virtualization solutions"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2: </span><span class="title-name">I/O Virtualization solutions </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.5.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 20%; " class="1"/><col style="width: 40%; " class="2"/><col style="width: 40%; " class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Technology
                  </p>
                </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Advantage
                  </p>
                </th><th style="border-bottom: 1px solid ; ">
                  <p>
                    Disadvantage
                  </p>
                </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="4">
                  <p>
                    Device Assignment (pass-through)
                  </p>
                </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Device accessed directly by the guest
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    No sharing among multiple guests
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    High performance
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Live migration is complex
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    PCI device limit is 8 per guest
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Limited number of slots on a server
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
                  <p>
                    Full virtualization (IDE, SATA, SCSI, e1000)
                  </p>
                </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    VM Guest compatibility
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Bad performance
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Easy for live migration
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Emulated operation
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; " rowspan="3">
                  <p>
                    Para-virtualization (virtio-blk, virtio-net, virtio-scsi)
                  </p>
                </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Good performance
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Modified guest (PV drivers)
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Easy for live migration
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  
                </td></tr><tr><td style="border-right: 1px solid ; ">
                  <p>
                    Efficient host communication with VM Guest
                  </p>
                </td><td>
                  
                </td></tr></tbody></table></div></div></section></section><section class="sect2" id="sec-vt-best-fs" data-id-title="Storage and file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">Storage and file system</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-fs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Storage space for VM Guests can either be a block device (for example,
        a partition on a physical disk), or an image file on the file system:
      </p><div class="table" id="id-1.13.4.5.6.3" data-id-title="Block devices compared to disk images"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 3: </span><span class="title-name">Block devices compared to disk images </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.5.6.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 10%; " class="1"/><col style="width: 45%; " class="2"/><col style="width: 45%; " class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <p>
                  Technology
                </p>
              </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <p>
                  Advantages
                </p>
              </th><th style="border-bottom: 1px solid ; ">
                <p>
                  Disadvantages
                </p>
              </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <p>
                  Block devices
                </p>
              </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Better performance
                    </p></li><li class="listitem"><p>
                      Use standard tools for administration/disk modification
                    </p></li><li class="listitem"><p>
                      Accessible from host (pro and con)
                    </p></li></ul></div>
              </td><td style="border-bottom: 1px solid ; ">
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Device management
                    </p></li></ul></div>
              </td></tr><tr><td style="border-right: 1px solid ; ">
                <p>
                  Image files
                </p>
              </td><td style="border-right: 1px solid ; ">
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Easier system management
                    </p></li><li class="listitem"><p>
                      Easily move, clone, expand, back up domains
                    </p></li><li class="listitem"><p>
                      Comprehensive toolkit (guestfs) for image manipulation
                    </p></li><li class="listitem"><p>
                      Reduce overhead through sparse files
                    </p></li><li class="listitem"><p>
                      Fully allocate for best performance
                    </p></li></ul></div>
              </td><td>
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Lower performance than block devices
                    </p></li></ul></div>
              </td></tr></tbody></table></div></div><p>
        For detailed information about image formats and maintaining images
        refer to <a class="xref" href="#sec-vt-best-img" title="5. VM Guest images">Section 5, “VM Guest images”</a>.
      </p><p>
        If your image is stored on an NFS share, you should check some server
        and client parameters to improve access to the VM Guest image.
      </p><section class="sect3" id="sec-vt-best-fs-nfs-rw" data-id-title="NFS read/write (client)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.1 </span><span class="title-name">NFS read/write (client)</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-fs-nfs-rw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Options <code class="option">rsize</code> and <code class="option">wsize</code> specify the
          size of the chunks of data that the client and server pass back and
          forth to each other. You should ensure NFS read/write sizes are
          sufficiently large, especially for large I/O. Change the
          <code class="option">rsize</code> and <code class="option">wsize</code> parameter in your
          <code class="filename">/etc/fstab</code> by increasing the value to 16 KB.
          This will ensure that all operations can be frozen if there is any
          instance of hanging.
        </p><div class="verbatim-wrap"><pre class="screen">nfs_server:/exported/vm_images<span class="callout" id="co-nfs-server">1</span> /mnt/images<span class="callout" id="co-nfs-mnt">2</span> nfs<span class="callout" id="co-nfs-nfs">3</span> rw<span class="callout" id="co-nfs-rw">4</span>,hard<span class="callout" id="co-nfs-hard">5</span>,sync<span class="callout" id="co-nfs-sync">6</span>, rsize=8192<span class="callout" id="co-nfs-rsize">7</span>,wsize=8192<span class="callout" id="co-nfs-wsize">8</span> 0 0</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-server"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              NFS server's host name and export path.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-mnt"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Where to mount the NFS exported share.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-nfs"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              This is an <code class="option">nfs</code> mount point.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-rw"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              This mount point will be accessible in read/write.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-hard"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Determines the recovery behavior of the NFS client after an NFS
              request times out. <code class="option">hard</code> is the best option to
              avoid data corruption.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-sync"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Any system call that writes data to files on that mount point
              causes that data to be flushed to the server before the system
              call returns control to user space.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-rsize"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum number of bytes in each network READ request that the NFS
              client can receive when reading data from a file on an NFS
              server.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-wsize"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum number of bytes per network WRITE request that the NFS
              client can send when writing data to a file on an NFS server.
            </p></td></tr></table></div></section><section class="sect3" id="sec-vt-best-fs-nfs-threads" data-id-title="NFS threads (server)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.2 </span><span class="title-name">NFS threads (server)</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-fs-nfs-threads">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Your NFS server should have enough NFS threads to handle
          multi-threaded workloads. Use the <code class="command">nfsstat</code> tool to
          get RPC statistics on your server:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</pre></div><p>
          If the <code class="literal">retrans</code> is equal to 0, everything is fine.
          Otherwise, the client needs to retransmit, so increase the
          <code class="envar">USE_KERNEL_NFSD_NUMBER</code> variable in
          <code class="filename">/etc/sysconfig/nfs</code>, and adjust accordingly until
          <code class="literal">retrans</code> is equal to <code class="literal">0</code>.
        </p></section></section><section class="sect2" id="sec-vt-best-perf-cpu" data-id-title="CPUs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.5 </span><span class="title-name">CPUs</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Host CPU <span class="quote">“<span class="quote">components</span>”</span> will be <span class="quote">“<span class="quote">translated</span>”</span> to
        virtual CPUs in a VM Guest when being assigned. These components can
        either be:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            <span class="emphasis"><em>CPU processor</em></span>: this describes the main CPU
            unit, which usually has multiple cores and may support
            Hyper-Threading.
          </p></li><li class="listitem"><p>
            <span class="emphasis"><em>CPU core</em></span>: a main CPU unit can provide more
            than one core, and the proximity of cores speeds up the computation
            process and reduces energy costs.
          </p></li><li class="listitem"><p>
            <span class="emphasis"><em>CPU Hyper-Threading</em></span>: this implementation is
            used to improve parallelization of computations, but this is not as
            efficient as a dedicated core.
          </p></li></ul></div><section class="sect3" id="sec-vt-best-perf-cpu-assign" data-id-title="Assigning CPUs"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.5.1 </span><span class="title-name">Assigning CPUs</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-assign">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          CPU overcommit occurs when the cumulative number of virtual CPUs of
          all VM Guests becomes higher than the number of host CPUs. Best
          performance is likely to be achieved when there is no overcommit and
          each virtual CPU matches one hardware processor or core on the
          VM Host Server. In fact, VM Guests running on an overcommitted host will
          experience increased latency and a negative effect on per-VM Guest
          throughput is also likely to be observed. Therefore, you should try
          to avoid overcommitting CPUs.
        </p><p>
          Deciding whether to allow CPU overcommit or not requires good
          a-priori knowledge of workload as a whole. For example, if you know
          that all the VM Guests virtual CPUs will not be loaded more than 50%
          then you can assume that overcommitting the host by a factor of 2
          (which means having 128 virtual CPUs in total, on a host with 64
          CPUs) will work well. On the other hand, if you know that all the
          virtual CPUs of the VM Guests will try to run at 100% for most of
          the time then even having one virtual CPU more than the host has CPUs
          is already a misconfiguration.
        </p><p>
          Overcommitting to a point where the cumulative number of virtual CPUs
          is higher than 8 times the number of physical cores of the VM Host Server
          will most likely lead to a malfunctioning and unstable system and
          should hence be avoided.
        </p><p>
          Unless you know exactly how many virtual CPUs are required for a
          VM Guest, start with one.
          Target a CPU workload of approximately 70% inside your VM (see
          <span class="intraxref">Book “System Analysis and Tuning Guide”, Chapter 2 “System monitoring utilities”, Section 2.3 “Processes”</span> for information on monitoring
          tools). If you allocate more processors than needed in the VM Guest,
          this will negatively affect the performance of host and guest. Cycle
          efficiency will be degraded, as the unused vCPU will still cause
          timer interrupts. In case you primarily run single threaded
          applications on a VM Guest, a single virtual CPU is the best choice.
        </p><p>
          A single VM Guest with more virtual CPUs than the VM Host Server has CPUs
          is always a misconfiguration.
        </p></section><section class="sect3" id="sec-vt-best-perf-cpu-guests" data-id-title="VM Guest CPU configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.5.2 </span><span class="title-name">VM Guest CPU configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          This section describes how to choose and configure a CPU type for a
          VM Guest. You will also learn how to pin virtual CPUs to physical
          CPUs on the host system. For more information about virtual CPU
          configuration and tuning parameters refer to the libvirt
          documentation at
          <a class="link" href="https://libvirt.org/formatdomain.html#elementsCPU" target="_blank">https://libvirt.org/formatdomain.html#elementsCPU</a>.
        </p><section class="sect4" id="sec-vt-best-perf-cpu-guests-model" data-id-title="Virtual CPU models and features"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.1 </span><span class="title-name">Virtual CPU models and features</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests-model">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            The CPU model and topology can be specified individually for each
            VM Guest. Configuration options range from selecting specific CPU
            models to excluding certain CPU features. Predefined CPU models are
            listed in files in the directory
            <code class="filename">/usr/share/libvirt/cpu_map/</code>. A CPU model and
            topology that is similar to the host generally provides the best
            performance. The host system CPU model and topology can be
            displayed by running <code class="command">virsh capabilities</code>.
          </p><p>
            Note that changing the default virtual CPU configuration will
            require a VM Guest shutdown when migrating it to a host with
            different hardware. More information on VM Guest migration is
            available at <span class="intraxref">Book “Virtualization Guide”, Chapter 11 “Basic VM Guest management”, Section 11.7 “Migrating VM Guests”</span>.
          </p><p>
            To specify a particular CPU model for a VM Guest, add a respective
            entry to the VM Guest configuration file. The following example
            configures a Broadwell CPU with the invariant TSC feature:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
  &lt;feature name='invtsc'/&gt;
  &lt;/cpu&gt;</pre></div><p>
            For a virtual CPU that most closely resembles the host physical
            CPU, <code class="literal">&lt;cpu mode='host-passthrough'&gt;</code> can be
            used. Note that a <code class="literal">host-passthrough</code> CPU model may
            not exactly resemble the host physical CPU, since by default KVM
            will mask any non-migratable features. For example invtsc is not
            included in the virtual CPU feature set. Changing the default KVM
            behavior is not directly supported through libvirt, although it
            does allow arbitrary pass-through of KVM command line arguments.
            Continuing with the <code class="literal">invtsc</code> example, you can
            achieve pass-through of the host CPU (including
            <code class="literal">invtsc</code>) with the following command line
            pass-through in the VM Guest configuration file:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
     &lt;qemu:commandline&gt;
     &lt;qemu:arg value='-cpu'/&gt;
     &lt;qemu:arg value='host,migratable=off,+invtsc'/&gt;
     &lt;/qemu:commandline&gt;
     ...
     &lt;/domain&gt;</pre></div><div id="id-1.13.4.5.7.5.3.8" data-id-title="The host-passthrough mode" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: The <code class="literal">host-passthrough</code> mode</div><p>
              Since <code class="literal">host-passthrough</code> exposes the physical
              CPU details to the virtual CPU, migration to dissimilar hardware
              is not possible. See
              <a class="xref" href="#sec-vt-best-perf-cpu-guests-vcpumigration" title="4.5.2.3. Virtual CPU migration considerations">Section 4.5.2.3, “Virtual CPU migration considerations”</a> for
              more information.
            </p></div></section><section class="sect4" id="sec-vt-best-perf-cpu-guests-vcpupin" data-id-title="Virtual CPU pinning"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.2 </span><span class="title-name">Virtual CPU pinning</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests-vcpupin">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            Virtual CPU pinning is used to constrain virtual CPU threads to a
            set of physical CPUs. The <code class="literal">vcpupin</code> element
            specifies the physical host CPUs that a virtual CPU can use. If
            this element is not set and the attribute <code class="literal">cpuset</code>
            of the <code class="literal">vcpu</code> element is not specified, the
            virtual CPU is free to use any of the physical CPUs.
          </p><p>
            CPU intensive workloads can benefit from virtual CPU pinning by
            increasing the physical CPU cache hit ratio. To pin a virtual CPU
            to a specific physical CPU, run the following commands:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh vcpupin <em class="replaceable">DOMAIN_ID</em> --vcpu <em class="replaceable">vCPU_NUMBER</em>
VCPU: CPU Affinity
----------------------------------
0: 0-7
<code class="prompt root"># </code>virsh vcpupin SLE15 --vcpu 0 0 --config</pre></div><p>
            The last command generates the following entry in the XML
            configuration:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</pre></div><div id="id-1.13.4.5.7.5.4.7" data-id-title="Virtual CPU pinning on NUMA nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Virtual CPU pinning on NUMA nodes</div><p>
              To confine a VM Guest's CPUs and its memory to a NUMA node, you
              can use virtual CPU pinning and memory allocation policies on a
              NUMA system. See <a class="xref" href="#sec-vt-best-perf-numa" title="4.6. NUMA tuning">Section 4.6, “NUMA tuning”</a> for more
              information related to NUMA tuning.
            </p></div><div id="id-1.13.4.5.7.5.4.8" data-id-title="Virtual CPU pinning and live migration" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Virtual CPU pinning and live migration</div><p>
              Even though <code class="literal">vcpupin</code> can improve performance,
              it can complicate live migration. See
              <a class="xref" href="#sec-vt-best-perf-cpu-guests-vcpumigration" title="4.5.2.3. Virtual CPU migration considerations">Section 4.5.2.3, “Virtual CPU migration considerations”</a> for
              more information on virtual CPU migration considerations.
            </p></div></section><section class="sect4" id="sec-vt-best-perf-cpu-guests-vcpumigration" data-id-title="Virtual CPU migration considerations"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.3 </span><span class="title-name">Virtual CPU migration considerations</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests-vcpumigration">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            Selecting a virtual CPU model containing all the latest features
            may improve performance of a VM Guest workload, but often at the
            expense of migratability. Unless all hosts in the cluster contain
            the latest CPU features, migration can fail when a destination host
            lacks the new features. If migratability of a virtual CPU is
            preferred over the latest CPU features, a normalized CPU model and
            feature set should be used. The <code class="command">virsh
            cpu-baseline</code> command can help define a normalized virtual
            CPU that can be migrated across all hosts. The following command,
            when run on each host in the migration cluster, illustrates the
            collection of all hosts' capabilities in
            <code class="literal">all-hosts-caps.xml</code>.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh capabilities &gt;&gt; all-hosts-cpu-caps.xml</pre></div><p>
            With the capabilities of each host collected in all-hosts-caps.xml,
            use <code class="command">virsh cpu-baseline</code> to create a virtual CPU
            definition that will be compatible across all hosts.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh cpu-baseline all-hosts-caps.xml</pre></div><p>
            The resulting virtual CPU definition can be used as the
            <code class="literal">cpu</code> element in the VM Guest configuration file.
          </p><p>
            At a logical level, virtual CPU pinning is a form of hardware
            pass-through. CPU pinning couples physical resources to virtual
            resources, which can also be problematic for migration. For
            example, the migration will fail if the requested physical
            resources are not available on the destination host, or if the
            source and destination hosts have different NUMA topologies. For
            more recommendations about Live Migration, see
            <span class="intraxref">Book “Virtualization Guide”, Chapter 11 “Basic VM Guest management”, Section 11.7.1 “Migration requirements”</span>.
          </p></section></section></section><section class="sect2" id="sec-vt-best-perf-numa" data-id-title="NUMA tuning"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.6 </span><span class="title-name">NUMA tuning</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        NUMA is an acronym for Non Uniform Memory Access. A NUMA system has
        multiple physical CPUs, each with local memory attached. Each CPU can
        also access other CPUs' memory, known as <span class="quote">“<span class="quote">remote memory
        access</span>”</span>, but it is much slower than accessing local memory. NUMA
        systems can negatively affect VM Guest performance if not tuned
        properly. Although ultimately tuning is workload dependent, this
        section describes controls that should be considered when deploying
        VM Guests on NUMA hosts. Always consider your host topology when
        configuring and deploying VMs.
      </p><p>
        <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> contains a NUMA auto-balancer that strives to reduce
        remote memory access by placing memory on the same NUMA node as the CPU
        processing it. Standard tools such as <code class="command">cgset</code> and
        virtualization tools such as libvirt provide mechanisms to constrain
        VM Guest resources to physical resources.
      </p><p>
        <code class="command">numactl</code> is used to check for host NUMA capabilities:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3
0:  10  21  21  21
1:  21  10  21  21
2:  21  21  10  21
3:  21  21  21  10</pre></div><p>
        The <code class="command">numactl</code> output shows this is a NUMA system with
        4 nodes or cells, each containing 36 CPUs and approximately 32G memory.
        <code class="command">virsh capabilities</code> can also be used to examine the
        systems NUMA capabilities and CPU topology.
      </p><section class="sect3" id="sec-vt-best-perf-numa-balancing" data-id-title="NUMA balancing"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.1 </span><span class="title-name">NUMA balancing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-balancing">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          On NUMA machines, there is a performance penalty if remote memory is
          accessed by a CPU. Automatic NUMA balancing scans a task's address
          space and unmaps pages. By doing so, it detects whether pages are
          properly placed or whether to migrate the data to a memory node local
          to where the task is running. In defined intervals (configured with
          <code class="literal">numa_balancing_scan_delay_ms</code>), the task scans the
          next scan size number of pages (configured with
          <code class="literal">numa_balancing_scan_size_mb</code>) in its address space.
          When the end of the address space is reached, the scanner restarts
          from the beginning.
        </p><p>
          Higher scan rates cause higher system overhead as page faults must be
          trapped and data needs to be migrated. However, the higher the scan
          rate, the more quickly a task's memory migrates to a local node when
          the workload pattern changes. This minimizes the performance impact
          caused by remote memory accesses. These <code class="command">sysctl</code>
          directives control the thresholds for scan delays and the number of
          pages scanned:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<span class="callout" id="co-numa-balancing">1</span>
kernel.numa_balancing_scan_delay_ms = 1000<span class="callout" id="co-numa-delay">2</span>
kernel.numa_balancing_scan_period_max_ms = 60000<span class="callout" id="co-numa-pmax">3</span>
kernel.numa_balancing_scan_period_min_ms = 1000<span class="callout" id="co-numa-pmin">4</span>
kernel.numa_balancing_scan_size_mb = 256<span class="callout" id="co-numa-size">5</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-balancing"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Enables/disables automatic page fault-based NUMA balancing
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-delay"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Starting scan delay used for a task when it initially forks
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-pmax"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum time in milliseconds to scan a task's virtual memory
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-pmin"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Minimum time in milliseconds to scan a task's virtual memory
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-size"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Size in megabytes' worth of pages to be scanned for a given scan
            </p></td></tr></table></div><p>
          For more information, see <span class="intraxref">Book “System Analysis and Tuning Guide”, Chapter 11 “Automatic Non-Uniform Memory Access (NUMA) balancing”</span>.
        </p><p>
          The main goal of automatic NUMA balancing is either to reschedule
          tasks on the same node's memory (so the CPU follows the memory), or
          to copy the memory's pages to the same node (so the memory follows
          the CPU).
        </p><div id="id-1.13.4.5.8.7.8" data-id-title="Task placement" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Task placement</div><p>
            There are no rules to define the best place to run a task, because
            tasks could share memory with other tasks. For the best
            performance, we recommend to group tasks sharing memory on the same
            node. Check NUMA statistics with <code class="command"># cat /proc/vmstat | grep
            numa_</code>.
          </p></div></section><section class="sect3" id="sec-vt-best-perf-numa-cpuset" data-id-title="Memory allocation control with the CPUset controller"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.2 </span><span class="title-name">Memory allocation control with the CPUset controller</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-cpuset">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The cgroups cpuset controller can be used confine memory used by a
          process to a NUMA node. There are three cpuset memory policy modes
          available:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <code class="literal">interleave</code>: this is a memory placement policy
              which is also known as round-robin. This policy can provide
              substantial improvements for jobs that need to place thread local
              data on the corresponding node. When the interleave destination
              is not available, it will be moved to another node.
            </p></li><li class="listitem"><p>
              <code class="literal">bind</code>: this will place memory only on one node,
              which means in case of insufficient memory, the allocation will
              fail.
            </p></li><li class="listitem"><p>
              <code class="literal">preferred</code>: this policy will apply a preference
              to allocate memory to a node. If there is not enough space for
              memory on this node, it will fall back to another node.
            </p></li></ul></div><p>
          You can change the memory policy mode with the
          <code class="command">cgset</code> tool from the
          <span class="package">libcgroup-tools</span> package:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  cgset -r cpuset.mems=<em class="replaceable">NODE</em> sysdefault/libvirt/qemu/<em class="replaceable">KVM_NAME</em>/emulator</pre></div><p>
          To migrate pages to a node, use the <code class="command">migratepages</code>
          tool:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>migratepages <em class="replaceable">PID</em> <em class="replaceable">FROM-NODE</em> <em class="replaceable">TO-NODE</em></pre></div><p>
          To check everything is fine. use: <code class="command">cat
          /proc/<em class="replaceable">PID</em>/status | grep Cpus</code>.
        </p><div id="id-1.13.4.5.8.8.9" data-id-title="Kernel NUMA/cpuset memory policy" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Kernel NUMA/cpuset memory policy</div><p>
            For more information see
            <a class="link" href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt" target="_blank">Kernel
            NUMA memory policy</a> and
            <a class="link" href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt" target="_blank">cpusets
            memory policy</a>. Check also the
            <a class="link" href="https://libvirt.org/formatdomain.html#elementsNUMATuning" target="_blank">Libvirt
            NUMA Tuning documentation</a>.
          </p></div></section><section class="sect3" id="sec-vt-best-perf-numa-vmguest" data-id-title="VM Guest: NUMA related configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.3 </span><span class="title-name">VM Guest: NUMA related configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-vmguest">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          <code class="systemitem">libvirt</code> allows to set up virtual NUMA and memory access policies.
          Configuring these settings is not supported by
          <code class="command">virt-install</code> or <code class="command">virt-manager</code>
          and needs to be done manually by editing the VM Guest configuration
          file with <code class="command">virsh edit</code>.
        </p><section class="sect4" id="sec-vt-best-perf-numa-vmguest-topo" data-id-title="VM Guest virtual NUMA topology"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.6.3.1 </span><span class="title-name">VM Guest virtual NUMA topology</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-vmguest-topo">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            Creating a VM Guest virtual NUMA (vNUMA) policy that resembles the
            host NUMA topology can often increase performance of traditional
            large, scale-up workloads. VM Guest vNUMA topology can be
            specified using the <code class="literal">numa</code> element in the XML
            configuration:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<span class="callout" id="co-numa-cell">1</span> id="0"<span class="callout" id="co-numa-id">2</span> cpus='0-1'<span class="callout" id="co-numa-cpus">3</span> memory='512000' unit='KiB'/&gt;
    &lt;cell id="1" cpus='2-3' memory='256000'<span class="callout" id="co-numa-mem">4</span>
    unit='KiB'<span class="callout" id="co-numa-unit">5</span> memAccess='shared'<span class="callout" id="co-numa-memaccess">6</span>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-cell"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Each <code class="literal">cell</code> element specifies a vNUMA cell or
                node
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-id"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                All cells should have an <code class="literal">id</code> attribute,
                allowing to reference the cell in other configuration blocks.
                Otherwise cells are assigned ids in ascending order starting
                from 0.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-cpus"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The CPU or range of CPUs that are part of the node
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-mem"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The node memory
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-unit"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Units in which node memory is specified
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-memaccess"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Optional attribute which can control whether the memory is to
                be mapped as <code class="option">shared</code> or
                <code class="option">private</code>. This is valid only for
                hugepages-backed memory.
              </p></td></tr></table></div><p>
            To find where the VM Guest has allocated its pages. use:
            <code class="command">cat
            /proc/<em class="replaceable">PID</em>/numa_maps</code> and
            <code class="command">cat
            /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<em class="replaceable">KVM_NAME</em>/memory.numa_stat</code>.
          </p><div id="id-1.13.4.5.8.9.3.6" data-id-title="NUMA specification" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: NUMA specification</div><p>
              The <code class="systemitem">libvirt</code> VM Guest NUMA specification is currently only
              available for QEMU/KVM.
            </p></div></section><section class="sect4" id="sec-vt-best-perf-numa-vmguest-alloc-libvirt" data-id-title="Memory allocation control with libvirt"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.6.3.2 </span><span class="title-name">Memory allocation control with <code class="systemitem">libvirt</code></span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-vmguest-alloc-libvirt">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            If the VM Guest has a vNUMA topology (see
            <a class="xref" href="#sec-vt-best-perf-numa-vmguest-topo" title="4.6.3.1. VM Guest virtual NUMA topology">Section 4.6.3.1, “VM Guest virtual NUMA topology”</a>), memory
            can be pinned to host NUMA nodes using the
            <code class="literal">numatune</code> element. This method is currently only
            available for QEMU/KVM guests. See
            <a class="xref" href="#sec-vt-best-perf-numa-alloc-libvirt-non-vnuma" title="Important: Non-vNUMA VM Guest">Important: Non-vNUMA VM Guest</a>
            for how to configure non-vNUMA VM Guests.
          </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
    &lt;memory mode="strict"<span class="callout" id="co-numat-mode">1</span> nodeset="1-4,^3"<span class="callout" id="co-numat-nodeset">2</span>/&gt;
    &lt;memnode<span class="callout" id="co-numat-memnode">3</span> cellid="0"<span class="callout" id="co-numat-cellid">4</span> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<span class="callout" id="co-numat-placement">5</span> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-mode"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Policies available are: <code class="literal">interleave</code>
                (round-robin like), <code class="literal">strict</code> (default) or
                <code class="literal">preferred</code>.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-nodeset"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Specify the NUMA nodes.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-memnode"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Specify memory allocation policies for each guest NUMA node (if
                this element is not defined then this will fall back and use
                the <code class="literal">memory</code> element).
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-cellid"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Addresses the guest NUMA node for which the settings are
                applied.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-placement"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The placement attribute can be used to indicate the memory
                placement mode for a domain process, the value can be
                <code class="literal">auto</code> or <code class="literal">strict</code>.
              </p></td></tr></table></div><div id="sec-vt-best-perf-numa-alloc-libvirt-non-vnuma" data-id-title="Non-vNUMA VM Guest" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Non-vNUMA VM Guest</div><p>
              On a non-vNUMA VM Guest, pinning memory to host NUMA nodes is
              done like in the following example:
            </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</pre></div><p>
              In this example, memory is allocated from the host nodes
              <code class="literal">0</code> and <code class="literal">1</code>. In case these
              memory requirements cannot be fulfilled, starting the VM Guest
              will fail. <code class="command">virt-install</code> also supports this
              configuration with the <code class="option">--numatune</code> option.
            </p></div><div id="id-1.13.4.5.8.9.4.6" data-id-title="Memory and CPU across NUMA nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Memory and CPU across NUMA nodes</div><p>
              You should avoid allocating VM Guest memory across NUMA nodes,
              and prevent virtual CPUs from floating across NUMA nodes.
            </p></div></section></section></section></section><section class="sect1" id="sec-vt-best-img" data-id-title="VM Guest images"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">VM Guest images</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Images are virtual disks used to store the operating system and data of
      VM Guests. They can be created, maintained and queried with the
      <code class="command">qemu-img</code> command. Refer to
      <span class="intraxref">Book “Virtualization Guide”, Chapter 34 “Guest installation”, Section 34.2.2 “Creating, converting, and checking disk images”</span> for more
      information on the <code class="command">qemu-img</code> tool and examples.
    </p><section class="sect2" id="sec-vt-best-img-imageformat" data-id-title="VM Guest image formats"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">VM Guest image formats</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Certain storage formats which QEMU recognizes have their origins in
        other virtualization technologies. By recognizing these formats, QEMU
        can leverage either data stores or entire guests that were originally
        targeted to run under these other virtualization technologies. Some
        formats are supported only in read-only mode. To use them in read/write
        mode, convert them to a fully supported QEMU storage format (using
        <code class="command">qemu-img</code>). Otherwise they can only be used as
        read-only data store in a QEMU guest.
      </p><p>
        Use <code class="command">qemu-img info
        <em class="replaceable">VMGUEST.IMG</em></code> to get information
        about an existing image, such as: the format, the virtual size, the
        physical size, snapshots if available.
      </p><div id="id-1.13.4.6.3.4" data-id-title="Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Performance</div><p>
          It is recommended to convert the disk images to either raw or qcow2
          to achieve good performance.
        </p></div><div id="id-1.13.4.6.3.5" data-id-title="Encrypted images cannot be compressed" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Encrypted images cannot be compressed</div><p>
          When you create an image, you cannot use compression
          (<code class="option">-c</code>) in the output file together with the encryption
          option (<code class="option">-e</code>).
        </p></div><section class="sect3" id="sec-vt-best-img-imageformat-raw" data-id-title="Raw format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.1 </span><span class="title-name">Raw format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-raw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              This format is simple and easily exportable to all other
              emulators/hypervisors.
            </p></li><li class="listitem"><p>
              It provides best performance (least I/O overhead).
            </p></li><li class="listitem"><p>
              It occupies all allocated space on the file system.
            </p></li><li class="listitem"><p>
              The raw format allows to copy a VM Guest image to a physical
              device (<code class="command">dd if=<em class="replaceable">VMGUEST.RAW</em>
              of=<em class="replaceable">/dev/sda</em></code>).
            </p></li><li class="listitem"><p>
              It is byte-for-byte the same as what the VM Guest sees, so this
              wastes a lot of space.
            </p></li></ul></div></section><section class="sect3" id="sec-vt-best-img-imageformat-qcow2" data-id-title="qcow2 format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.2 </span><span class="title-name">qcow2 format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-qcow2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              Use this to have smaller images (useful if your file system does
              not supports holes).
            </p></li><li class="listitem"><p>
              It has optional AES encryption (now deprecated).
            </p></li><li class="listitem"><p>
              Zlib-based compression option.
            </p></li><li class="listitem"><p>
              Support of multiple VM snapshots (internal, external).
            </p></li><li class="listitem"><p>
              Improved performance and stability.
            </p></li><li class="listitem"><p>
              Supports changing the backing file.
            </p></li><li class="listitem"><p>
              Supports consistency checks.
            </p></li><li class="listitem"><p>
              Less performance than raw format.
            </p></li></ul></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.13.4.6.3.7.4.1"><span class="term">l2-cache-size</span></dt><dd><p>
                qcow2 can provide the same performance for random read/write
                access as raw format, but it needs a well-sized cache size. By
                default cache size is set to 1 MB. This will give good
                performance up to a disk size of 8 GB. If you need a bigger
                disk size, you need to adjust the cache size. For a disk size
                of 64 GB (64*1024 = 65536), you need 65536 / 8192B = 8 MB of
                cache (<code class="option">-drive format=qcow2,l2-cache-size=8M</code>).
              </p></dd><dt id="id-1.13.4.6.3.7.4.2"><span class="term">Cluster size</span></dt><dd><p>
                The qcow2 format offers the capability to change the cluster
                size. The value must be between 512 KB and 2 MB.
                Smaller cluster sizes can improve the image file size whereas
                larger cluster sizes generally provide better performance.
              </p></dd><dt id="id-1.13.4.6.3.7.4.3"><span class="term">Preallocation</span></dt><dd><p>
                An image with preallocated metadata is initially larger but can
                improve performance when the image needs to grow.
              </p></dd><dt id="id-1.13.4.6.3.7.4.4"><span class="term">Lazy refcounts</span></dt><dd><p>
                Reference count updates are postponed with the goal of avoiding
                metadata I/O and improving performance. This is particularly
                beneficial with <code class="option">cache=writethrough</code>. This
                option does not batch metadata updates, but if in case of host
                crash, the reference count tables must be rebuilt, this is done
                automatically at the next open with <code class="command">qemu-img check -r
                all</code>. Note that this takes some time.
              </p></dd></dl></div></section><section class="sect3" id="sec-vt-best-img-imageformat-qed" data-id-title="qed format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.3 </span><span class="title-name">qed format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-qed">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          qed is a follow-on qcow (QEMU Copy On Write) format. Because qcow2
          provides all the benefits of qed and more, qed is now deprecated.
        </p></section><section class="sect3" id="sec-vt-best-img-imageformat-vmdk" data-id-title="VMDK format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.4 </span><span class="title-name">VMDK format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-vmdk">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          VMware 3, 4, or 6 image format, for exchanging images with that
          product.
        </p></section></section><section class="sect2" id="sec-vt-best-img-overlay" data-id-title="Overlay disk images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Overlay disk images</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-overlay">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The qcow2 and qed formats provide a way to create a base image (also
        called backing file) and overlay images on top of the base image. A
        backing file is useful to be able to revert to a known state and
        discard the overlay. If you write to the image, the backing image will
        be untouched and all changes will be recorded in the overlay image
        file. The backing file will never be modified unless you use the
        <code class="option">commit</code> monitor command (or <code class="command">qemu-img
        commit</code>).
      </p><p>
        To create an overlay image:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img create -o<span class="callout" id="co-1-minoro">1</span>backing_file=vmguest.raw<span class="callout" id="co-1-backingfile">2</span>,backing_fmt=raw<span class="callout" id="co-1-backingfmt">3</span>\
     -f<span class="callout" id="co-1-minorf">4</span> qcow2 vmguest.cow<span class="callout" id="co-1-imagename">5</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-minoro"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Use <code class="option">-o ?</code> for an overview of available options.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-backingfile"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            The backing file name.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-backingfmt"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Specify the file format for the backing file.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-minorf"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Specify the image format for the VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-imagename"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Image name of the VM Guest, it will only record the differences
            from the backing file.
          </p></td></tr></table></div><div id="id-1.13.4.6.4.6" data-id-title="Backing image path" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Backing image path</div><p>
          You should not change the path to the backing image, otherwise you
          will need to adjust it. The path is stored in the overlay image file.
          To update the path, you should make a symbolic link from the original
          path to the new path and then use the <code class="command">qemu-img</code>
          <code class="option">rebase</code> option.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ln -sf /var/lib/images/<em class="replaceable">OLD_PATH</em>/vmguest.raw  \
 /var/lib/images/<em class="replaceable">NEW_PATH</em>/vmguest.raw
<code class="prompt root"># </code>qemu-img rebase<span class="callout" id="co-2-rebase">1</span> -u<span class="callout" id="co-2-unsafe">2</span> -b<span class="callout" id="co-2-minorb">3</span> \
 /var/lib/images/<em class="replaceable">OLD_PATH</em>/vmguest.raw /var/lib/images/<em class="replaceable">NEW_PATH</em>/vmguest.cow</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-2-rebase"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The <code class="command">rebase</code> subcommand tells
              <code class="command">qemu-img</code> to change the backing file image.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-2-unsafe"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The <code class="option">-u</code> option activates the unsafe mode (see
              note below). There are two different modes in which
              <code class="option">rebase</code> can operate:
            </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                  <span class="emphasis"><em>Safe</em></span>: This is the default mode and
                  performs a real rebase operation. The safe mode is a
                  time-consuming operation.
                </p></li><li class="listitem"><p>
                  <span class="emphasis"><em>Unsafe</em></span>: The unsafe mode
                  (<code class="option">-u</code>) only changes the backing files name and
                  the format of the file name without making any checks on the
                  files contents. You should use this mode to rename or moving
                  a backing file.
                </p></li></ul></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-2-minorb"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The backing image to be used is specified with
              <code class="option">-b</code> and the image path is the last argument of
              the command.
            </p></td></tr></table></div></div><p>
        A common use is to initiate a new guest with the backing file. Let's
        assume we have a <code class="filename">sle15_base.img</code> VM Guest ready to
        be used (fresh installation without any modification). This will be our
        backing file. Now you need to test a new package, on an updated system
        and on a system with a different kernel. We can use
        <code class="filename">sle15_base.img</code> to instantiate the new SUSE Linux Enterprise
        VM Guest by creating a qcow2 overlay file pointing to this backing
        file (<code class="filename">sle15_base.img</code>).
      </p><p>
        In our example we will use <code class="filename">sle15_updated.qcow2</code> for
        the updated system, and <code class="filename">sle15_kernel.qcow2</code> for the
        system with a different kernel.
      </p><p>
        To create the two thin provisioned systems use the
        <code class="command">qemu-img</code> command line with the <code class="option">-b</code>
        option:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_updated.qcow2
Formatting 'sle15_updated.qcow2', fmt=qcow2 size=17179869184
backing_file='sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off
<code class="prompt root"># </code>qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_kernel.qcow2
Formatting 'sle15_kernel.qcow2', fmt=qcow2 size=17179869184
backing_file='vmguest-sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off</pre></div><p>
        The images are now usable, and you can do your test without touching
        the initial <code class="filename">sle15_base.img</code> backing file. All
        changes will be stored in the new overlay images. Additionally, you can
        also use these new images as a backing file, and create a new overlay.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img create -b sle15_kernel.qcow2 -f qcow2 sle15_kernel_TEST.qcow2</pre></div><p>
        When using <code class="command">qemu-img info</code> with the option
        <code class="option">--backing-chain</code>, it will return all information about
        the entire backing chain recursively:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img info --backing-chain
/var/lib/libvirt/images/sle15_kernel_TEST.qcow2
image: sle15_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle15_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE15.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</pre></div><div class="figure" id="fig-qemu-img-overlay"><div class="figure-contents"><div class="mediaobject"><a href="images/qemu-img-overlay.png"><img src="images/qemu-img-overlay.png" width="95%" alt="Understanding image overlay" title="Understanding image overlay"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1: </span><span class="title-name">Understanding image overlay </span></span><a title="Permalink" class="permalink" href="#fig-qemu-img-overlay">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-vt-best-img-open-img" data-id-title="Opening a VM Guest image"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Opening a VM Guest image</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-open-img">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To access the file system of an image, use the
        <span class="package">guestfs-tools</span>. If you do not have this tool
        installed on your system you can mount an image with other Linux tools.
        Avoid accessing an untrusted or unknown VM Guest's image system
        because this can lead to security issues (for more information, read
        <a class="link" href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/" target="_blank">D.
        Berrangé's post</a>).
      </p><section class="sect3" id="sec-vt-best-img-open-img-raw" data-id-title="Opening a raw image"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Opening a raw image</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-open-img-raw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.13.4.6.5.3.2" data-id-title="Mounting a raw image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5: </span><span class="title-name">Mounting a raw image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To be able to mount the image, find a free loop device. The
              following command displays the first unused loop device,
              <code class="filename">/dev/loop1</code> in this example.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup -f
/dev/loop1</pre></div></li><li class="step"><p>
              Associate an image (<code class="filename">SLE15.raw</code> in this
              example) with the loop device:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup /dev/loop1 SLE15.raw</pre></div></li><li class="step"><p>
              Check whether the image has successfully been associated with the
              loop device by getting detailed information about the loop
              device:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup -l
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE15.raw</pre></div></li><li class="step"><p>
              Check the image's partitions with <code class="command">kpartx</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>kpartx -a<span class="callout" id="co-kpartx-a">1</span> -v<span class="callout" id="co-kpartx-v">2</span> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-kpartx-a"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Add partition device mappings.
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-kpartx-v"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Verbose mode.
                </p></td></tr></table></div></li><li class="step"><p>
              Now mount the image partition(s) (to
              <code class="filename">/mnt/sle15mount</code> in the following example):
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /mnt/sle15mount
<code class="prompt root"># </code>mount /dev/mapper/loop1p1 /mnt/sle15mount</pre></div></li></ol></div></div><div id="id-1.13.4.6.5.3.3" data-id-title="Raw image with LVM" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Raw image with LVM</div><p>
            If your raw image contains an LVM volume group you should use LVM
            tools to mount the partition. Refer to
            <a class="xref" href="#sec-lvm-found" title="5.3.3. Opening images containing LVM">Section 5.3.3, “Opening images containing LVM”</a>.
          </p></div><div class="procedure" id="id-1.13.4.6.5.3.4" data-id-title="Unmounting a raw image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6: </span><span class="title-name">Unmounting a raw image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Unmount all mounted partitions of the image, for example:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>umount /mnt/sle15mount</pre></div></li><li class="step" id="st-umount-raw"><p>
              Delete partition device mappings with <code class="command">kpartx</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>kpartx -d /dev/loop1</pre></div></li><li class="step"><p>
              Detach the devices with <code class="command">losetup</code>
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup -d /dev/loop1</pre></div></li></ol></div></div></section><section class="sect3" id="sec-vt-best-img-open-img-qcow2" data-id-title="Opening a qcow2 image"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Opening a qcow2 image</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-open-img-qcow2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.13.4.6.5.4.2" data-id-title="Mounting a qcow2 image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7: </span><span class="title-name">Mounting a qcow2 image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              First you need to load the <code class="literal">nbd</code> (network block
              devices) module. The following example loads it with support for
              16 block devices (<code class="option">max_part=16</code>). Check with
              <code class="command">dmesg</code> whether the operation was successful:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>modprobe nbd max_part=16
<code class="prompt root"># </code>dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</pre></div></li><li class="step"><p>
              Connect the VM Guest image (for example
              <code class="filename">SLE15.qcow2</code>) to an NBD device
              (<code class="filename">/debv/nbd0</code> in the following example) with
              the <code class="command">qemu-nbd</code> command. Make sure to use a free
              NBD device:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-nbd -c<span class="callout" id="co-qemunbd-minusc">1</span> /dev/nbd0<span class="callout" id="co-qemunbd-device">2</span> SLE15.qcow2<span class="callout" id="co-qemunbd-image">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-minusc"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Connect <code class="filename">SLE15.qcow2</code> to the local NBD
                  device <code class="filename">/dev/nbd0</code>
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-device"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  NBD device to use
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-image"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  VM Guest image to use
                </p></td></tr></table></div><div id="id-1.13.4.6.5.4.2.3.4" data-id-title="Checking for a free NBD device" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Checking for a free NBD device</div><p>
                To check whether an NBD device is free, run the following
                command:
              </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</pre></div><p>
                If the command produces an output like in the example above,
                the device is busy (not free). This can also be confirmed by
                the presence of the
                <code class="filename">/sys/devices/virtual/block/nbd0/pid</code> file.
              </p></div></li><li class="step"><p>
              Inform the operating system about partition table changes with
              <code class="command">partprobe</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>partprobe /dev/nbd0 -s
/dev/nbd0: msdos partitions 1 2
<code class="prompt root"># </code>dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</pre></div></li><li class="step"><p>
              In the example above, the <code class="filename">SLE15.qcow2</code>
              contains two partitions: <code class="filename">/dev/nbd0p1</code> and
              <code class="filename">/dev/nbd0p2</code>. Before mounting these
              partitions, use <code class="command">vgscan</code> to check whether they
              belong to an LVM volume:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgscan -v
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Reading all physical volumes. This may take a while...
    Using volume group(s) on command line.
    No volume groups found.</pre></div></li><li class="step"><p>
              If no LVM volume has been found, you can mount the partition with
              <code class="command">mount</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</pre></div><p>
              Refer to <a class="xref" href="#sec-lvm-found" title="5.3.3. Opening images containing LVM">Section 5.3.3, “Opening images containing LVM”</a> for information on how
              to handle LVM volumes.
            </p></li></ol></div></div><div class="procedure" id="id-1.13.4.6.5.4.3" data-id-title="Unmounting a qcow2 image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8: </span><span class="title-name">Unmounting a qcow2 image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Unmount all mounted partitions of the image, for example:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>umount /mnt/nbd0p2</pre></div></li><li class="step" id="st-umount-qcow2"><p>
              Disconnect the image from the <code class="filename">/dev/nbd0</code>
              device.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-nbd -d /dev/nbd0</pre></div></li></ol></div></div></section><section class="sect3" id="sec-lvm-found" data-id-title="Opening images containing LVM"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.3 </span><span class="title-name">Opening images containing LVM</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-found">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.13.4.6.5.5.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
            If your VM Host Server uses VG name <code class="literal">system</code>, and the
            guest image also uses VG name <code class="literal">system</code>, LVM will
            complain during its activation. A workaround is to temporarily
            rename the guest VG, while a correct approach is to use different
            VG names for the guests than for the VM Host Server.
          </p></div><div class="procedure" id="id-1.13.4.6.5.5.3" data-id-title="Mounting images containing LVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 9: </span><span class="title-name">Mounting images containing LVM </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To check images for LVM groups, use <code class="command">vgscan -v</code>.
              If an image contains LVM groups, the output of the command looks
              like the following:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgscan -v
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</pre></div></li><li class="step"><p>
              The <code class="literal">system</code> LVM volume group has been found on
              the system. You can get more information about this volume with
              <code class="command">vgdisplay
              <em class="replaceable">VOLUMEGROUPNAME</em></code> (in our case
              <em class="replaceable">VOLUMEGROUPNAME</em> is
              <code class="literal">system</code>). You should activate this volume group
              to expose LVM partitions as devices so the system can mount them.
              Use <code class="command">vgchange</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgchange -ay -v
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</pre></div></li><li class="step"><p>
              All partitions in the volume group will be listed in the
              <code class="filename">/dev/mapper</code> directory. You can simply mount
              them now.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

<code class="prompt root"># </code>mkdir /mnt/system-root
<code class="prompt root"># </code>mount  /dev/mapper/system-root /mnt/system-root

<code class="prompt root"># </code>ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</pre></div></li></ol></div></div><div class="procedure" id="id-1.13.4.6.5.5.4" data-id-title="Unmounting images containing LVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10: </span><span class="title-name">Unmounting images containing LVM </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Unmount all partitions (with <code class="command">umount</code>)
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>umount /mnt/system-root</pre></div></li><li class="step"><p>
              Deactivate the LVM volume group (with <code class="command">vgchange -an
              <em class="replaceable">VOLUMEGROUPNAME</em></code>)
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgchange -an -v system
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</pre></div></li><li class="step"><p>
              Now you have two choices:
            </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                  In case of a qcow2 image, proceed as described in
                  <a class="xref" href="#st-umount-qcow2" title="Step 2">Step 2</a> (<code class="command">qemu-nbd -d
                  /dev/nbd0</code>).
                </p></li><li class="listitem"><p>
                  In case of a raw image, proceeds as described in
                  <a class="xref" href="#st-umount-raw" title="Step 2">Step 2</a> (<code class="command">kpartx -d
                  /dev/loop1</code>; <code class="command">losetup -d
                  /dev/loop1</code>).
                </p></li></ul></div><div id="id-1.13.4.6.5.5.4.4.3" data-id-title="Check for a successful unmount" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Check for a successful unmount</div><p>
                You should double-check that unmounting succeeded by using a
                system command like <code class="command">losetup</code>,
                <code class="command">qemu-nbd</code>, <code class="command">mount</code> or
                <code class="command">vgscan</code>. If this is not the case you may have
                trouble using the VM Guest because its system image is used in
                different places.
              </p></div></li></ol></div></div></section></section><section class="sect2" id="sec-vt-best-img-share" data-id-title="File system sharing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">File system sharing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-share">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        You can access a host directory in the VM Guest using the
        <code class="sgmltag-element">filesystem</code> element. In the following
        example we will share the <code class="filename">/data/shared</code> directory
        and mount it in the VM Guest. Note that the
        <code class="sgmltag-attribute">accessmode</code> parameter only works with
        <code class="sgmltag-attribute">type='mount'</code> for the QEMU/KVM driver
        (most other values for <code class="sgmltag-attribute">type</code> are
        exclusively used for the LXC driver).
      </p><div class="verbatim-wrap"><pre class="screen">&lt;filesystem type='mount'<span class="callout" id="co-fs-mount">1</span> accessmode='mapped'<span class="callout" id="co-fs-mode">2</span>&gt;
   &lt;source dir='/data/shared'<span class="callout" id="co-fs-sourcedir">3</span>&gt;
   &lt;target dir='shared'<span class="callout" id="co-fs-targetdir">4</span>/&gt;
&lt;/filesystem&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-mount"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            A host directory to mount VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-mode"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Access mode (the security mode) set to <code class="literal">mapped</code>
            will give access with the permissions of the hypervisor. Use
            <code class="literal">passthrough</code> to access this share with the
            permissions of the user inside the VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-sourcedir"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Path to share with the VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-targetdir"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Name or label of the path for the mount command.
          </p></td></tr></table></div><p>
        To mount the <code class="literal">shared</code> directory on the VM Guest, use
        the following commands: Under the VM Guest now you need to mount the
        <code class="literal">target dir='shared'</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /opt/mnt_shared
<code class="prompt root"># </code>mount shared -t 9p /opt/mnt_shared -o trans=virtio</pre></div><p>
        See
        <a class="link" href="https://libvirt.org/formatdomain.html#elementsFilesystems" target="_blank"><code class="systemitem">libvirt</code>
        File System </a> and
        <a class="link" href="http://wiki.qemu.org/Documentation/9psetup" target="_blank">QEMU
        9psetup</a> for more information.
      </p></section></section><section class="sect1" id="sec-vt-best-vmguests" data-id-title="VM Guest configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">VM Guest configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-vmguests-virtio" data-id-title="Virtio driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Virtio driver</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To increase VM Guest performance it is recommended to use
        paravirtualized drivers within the VM Guests. The virtualization
        standard for such drivers for KVM are the <code class="literal">virtio</code>
        drivers, which are designed for running in a virtual environment. Xen
        uses similar paravirtualized device drivers (like
        <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
        in a Windows* guest).
      </p><section class="sect3" id="sec-vt-best-vmguests-virtio-virtio-blk" data-id-title="virtio blk"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.1 </span><span class="title-name"><code class="literal">virtio blk</code></span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-virtio-blk">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          <code class="literal">virtio_blk</code> is the virtio block device for disk. To
          use the <code class="literal">virtio blk</code> driver for a block device,
          specify the <code class="sgmltag-attribute">bus='virtio'</code> attribute in
          the <code class="sgmltag-element">disk</code> definition:
        </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</pre></div><div id="id-1.13.4.7.2.3.4" data-id-title="Disk device names" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Disk device names</div><p>
            <code class="literal">virtio</code> disk devices are named
            <code class="literal">/dev/vd[a-z][1-9]</code>. If you migrate a Linux guest
            from a non-virtio disk you need to adjust the
            <code class="literal">root=</code> parameter in the GRUB configuration, and
            regenerate the <code class="filename">initrd</code> file. Otherwise the
            system cannot boot. On VM Guests with other operating systems, the
            boot loader may need to be adjusted or reinstalled accordingly,
            too.
          </p></div><div id="id-1.13.4.7.2.3.5" data-id-title="Using virtio disks with qemu-system-ARCH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Using <code class="literal">virtio</code> disks with <code class="command">qemu-system-ARCH</code></div><p>
            When running <code class="command">qemu-system-ARCH</code>, use the
            <code class="option">-drive</code> option to add a disk to the VM Guest. See
            <span class="intraxref">Book “Virtualization Guide”, Chapter 34 “Guest installation”, Section 34.1 “Basic installation with <code class="command">qemu-system-ARCH</code>”</span> for an example. The
            <code class="option">-hd[abcd]</code> option will not work for virtio disks.
          </p></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-virtio-net" data-id-title="virtio net"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.2 </span><span class="title-name">virtio net</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-virtio-net">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          <code class="literal">virtio_net</code> is the virtio network device. The
          kernel modules should be loaded automatically in the guest at boot
          time. You need to start the service to make the network available.
        </p><div class="verbatim-wrap"><pre class="screen">&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-balloon" data-id-title="virtio balloon"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.3 </span><span class="title-name">virtio balloon</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-balloon">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The virtio balloon is used for host memory over-commits for guests.
          For Linux guests, the balloon driver runs in the guest kernel,
          whereas for Windows guests, the balloon driver is in the VMDP
          package. <code class="literal">virtio_balloon</code> is a PV driver to give or
          take memory from a VM Guest.
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <span class="emphasis"><em>Inflate balloon</em></span>: Return memory from guest to
              host kernel (for KVM) or to hypervisor (for Xen)
            </p></li><li class="listitem"><p>
              <span class="emphasis"><em>Deflate balloon</em></span>: Guest will have more
              available memory
            </p></li></ul></div><p>
          It is controlled by the <code class="literal">currentMemory</code> and
          <code class="literal">memory</code> options.
        </p><div class="verbatim-wrap"><pre class="screen">&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
        &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</pre></div><p>
          You can also use <code class="command">virsh</code> to change it:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh setmem <em class="replaceable">DOMAIN_ID</em> <em class="replaceable">MEMORY in KB</em></pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-check" data-id-title="Checking virtio presence"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.4 </span><span class="title-name">Checking virtio presence</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-check">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          You can check the virtio block PCI with:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</pre></div><p>
          To find the block device associated with <code class="filename">vdX</code>:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</pre></div><p>
          To get more information on the virtio block:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</pre></div><p>
          To check all virtio drivers being used:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-drv-opt" data-id-title="Find device driver options"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.5 </span><span class="title-name">Find device driver options</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-drv-opt">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Virtio devices and other drivers have various options. To list all of
          them, use the <code class="option">help</code> parameter of
          the<code class="command">qemu-system-ARCH</code> command.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</pre></div></section></section><section class="sect2" id="sec-vt-best-perf-cirrus" data-id-title="Cirrus video driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Cirrus video driver</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cirrus">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To get 16-bit color, high compatibility and better performance it is
        recommended to use the <code class="literal">cirrus</code> video driver.
      </p><div id="id-1.13.4.7.3.3" data-id-title="libvirt" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <code class="systemitem">libvirt</code></div><p>
          <code class="systemitem">libvirt</code> ignores the <code class="literal">vram</code> value because video
          size has been hardcoded in QEMU.
        </p></div><div class="verbatim-wrap"><pre class="screen">&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</pre></div></section><section class="sect2" id="sec-vt-best-entropy" data-id-title="Better entropy"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">Better entropy</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-entropy">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Virtio RNG (random number generator) is a paravirtualized device that
        is exposed as a hardware RNG device to the guest. On the host side, it
        can be wired up to one of several sources of entropy (including a real
        hardware RNG device and the host's <code class="filename">/dev/random</code>) if
        hardware support does not exist. The Linux kernel contains the guest
        driver for the device from version 2.6.26 and higher.
      </p><p>
        The system entropy is collected from various non-deterministic hardware
        events and is mainly used by cryptographic applications. The virtual
        random number generator device (paravirtualized device) allows the host
        to pass through entropy to VM Guest operating systems. This results in
        a better entropy in the VM Guest.
      </p><p>
        To use Virtio RNG, add an <code class="literal">RNG</code> device in
        <code class="command">virt-manager</code> or directly in the VM Guest's XML
        configuration:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;devices&gt;
   &lt;rng model='virtio'&gt;
       &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</pre></div><p>
        The host now should used <code class="filename">/dev/random</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</pre></div><p>
        On the VM Guest, the source of entropy can be checked with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/devices/virtual/misc/hw_random/rng_available</pre></div><p>
        The current device used for entropy can be checked with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</pre></div><p>
        You should install the <span class="package">rng-tools</span> package on the
        VM Guest, enable the service, and start it. Under <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 15, do
        the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in rng-tools
<code class="prompt root"># </code>systemctl enable rng-tools
<code class="prompt root"># </code>systemctl start rng-tools</pre></div></section><section class="sect2" id="sec-vt-best-perf-disable" data-id-title="Disable unused tools and devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Disable unused tools and devices</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-disable">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Per host, use one virtualization technology only. For example, do not
        use KVM and Xen on the same host. Otherwise, you may find yourself
        with a reduced amount of available resources, increased security risk
        and a longer software update queue. Even when the amount of resources
        allocated to each of the technologies is configured carefully, the host
        may suffer from reduced overall availability and degraded performance.
      </p><p>
        Minimize the amount of software and services available on hosts. Most
        default installations of operating systems are not optimized for VM
        usage. Install what you really need and remove all other components in
        the VM Guest.
      </p><p>
        Windows* Guest:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            Disable the screen saver
          </p></li><li class="listitem"><p>
            Remove all graphical effects
          </p></li><li class="listitem"><p>
            Disable indexing of hard disks if not necessary
          </p></li><li class="listitem"><p>
            Check the list of started services and disable the ones you do not
            need
          </p></li><li class="listitem"><p>
            Check and remove all unneeded devices
          </p></li><li class="listitem"><p>
            Disable system update if not needed, or configure it to avoid any
            delay while rebooting or shutting down the host
          </p></li><li class="listitem"><p>
            Check the Firewall rules
          </p></li><li class="listitem"><p>
            Schedule backups and anti-virus updates appropriately
          </p></li><li class="listitem"><p>
            Install the
            <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
            paravirtualized driver for best performance
          </p></li><li class="listitem"><p>
            Check the operating system recommendations, such as on the
            <a class="link" href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7" target="_blank">Microsoft
            Windows* 7 better performance</a> Web page.
          </p></li></ul></div><p>
        Linux Guest:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            Remove or do not start the X Window System if not necessary
          </p></li><li class="listitem"><p>
            Check the list of started services and disable the ones you do not
            need
          </p></li><li class="listitem"><p>
            Check the OS recommendations for kernel parameters that enable
            better performance
          </p></li><li class="listitem"><p>
            Only install software that you really need
          </p></li><li class="listitem"><p>
            Optimize the scheduling of predictable tasks (system updates, hard
            disk checks, etc.)
          </p></li></ul></div></section><section class="sect2" id="sec-vt-best-perf-mtype" data-id-title="Updating the guest machine type"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5 </span><span class="title-name">Updating the guest machine type</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-mtype">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        QEMU machine types define details of the architecture that are
        particularly relevant for migration and session management. As changes
        or improvements to QEMU are made, new machine types are added. Old
        machine types are still supported for compatibility reasons, but to
        take advantage of improvements, we recommend to always migrate to the
        latest machine type when upgrading.
      </p><p>
        Changing the guest's machine type for a Linux guest will mostly be
        transparent. For Windows* guests, we recommend to take a snapshot or
        backup of the guest—in case Windows* has issues with the changes
        it detects and subsequently the user decides to revert to the original
        machine type the guest was created with.
      </p><div id="id-1.13.4.7.6.4" data-id-title="Changing the machine type" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Changing the machine type</div><p>
          Refer to <span class="intraxref">Book “Virtualization Guide”, Chapter 15 “Configuring virtual machines with <code class="command">virsh</code>”, Section 15.2 “Changing the machine type”</span> for
          documentation.
        </p></div></section></section><section class="sect1" id="sec-vt-best-vm-setup-config" data-id-title="VM Guest-specific configurations and settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">VM Guest-specific configurations and settings</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vm-setup-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.13.4.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        This section applies to QEMU / KVM hypervisor only.
      </p></div><section class="sect2" id="sec-vt-best-acpi" data-id-title="ACPI testing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">ACPI testing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-acpi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The ability to change a VM Guest's state heavily depends on the
        operating system. It is very important to test this feature before any
        use of your VM Guests in production. For example, most Linux operating
        systems disable this capability by default, so this requires you to
        enable this operation (mostly through Polkit).
      </p><p>
        ACPI must be enabled in the guest for a graceful shutdown to work. To
        check if ACPI is enabled, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh dumpxml <em class="replaceable">VMNAME</em> | grep acpi</pre></div><p>
        If nothing is printed, ACPI is not enabled for your machine. Use
        <code class="command">virsh edit</code> to add the following XML under
        &lt;domain&gt;:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</pre></div><p>
        If ACPI was enabled during a Windows Server* guest
        installation, it is not sufficient to turn it on in the VM Guest
        configuration only. For more information, see
        <a class="link" href="https://support.microsoft.com/en-us/kb/309283" target="_blank">https://support.microsoft.com/en-us/kb/309283</a>.
        
      </p><p>
        Regardless of the VM Guest's configuration, a graceful shutdown is
        always possible from within the guest operating system.
      </p></section><section class="sect2" id="sec-vt-best-guest-kbd" data-id-title="Keyboard layout"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Keyboard layout</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-guest-kbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Though it is possible to specify the keyboard layout from a
        <code class="command">qemu-system-ARCH</code> command, it is recommended to
        configure it in the <code class="systemitem">libvirt</code> XML file. To change the keyboard layout
        while connecting to a remote VM Guest using vnc, you should edit the
        VM Guest XML configuration file. For example, to add an
        <code class="literal">en-us</code> keymap, add in the
        <code class="literal">&lt;devices&gt;</code> section:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</pre></div><p>
        Check the <code class="literal">vncdisplay</code> configuration and connect to
        your VM Guest:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh vncdisplay sles15 127.0.0.1:0</pre></div></section><section class="sect2" id="sec-vt-best-spice-default-url" data-id-title="Spice default listen URL"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Spice default listen URL</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-spice-default-url">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        If no network interface other than <code class="literal">lo</code> is assigned an
        IPv4 address on the host, the default address on which the spice server
        listens will not work. An error like the following one will occur:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh start sles15
error: Failed to start domain sles15
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</pre></div><p>
        To fix this, you can change the default <code class="literal">spice_listen</code>
        value in <code class="filename">/etc/libvirt/qemu.conf</code> using the local
        IPv6 address <code class="systemitem">::1</code>. The spice
        server listening address can also be changed on a per VM Guest basis,
        use <code class="command">virsh edit</code> to add the listen XML attribute to
        the <code class="literal">graphics type='spice'</code> element:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;&gt;</pre></div></section><section class="sect2" id="sec-vt-best-xml-to-qemu" data-id-title="XML to QEMU command line"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">XML to QEMU command line</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-xml-to-qemu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Sometimes it could be useful to get the QEMU command line to launch
        the VM Guest from the XML file.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh domxml-to-native<span class="callout" id="co-domxml-native">1</span> qemu-argv<span class="callout" id="co-domxml-argv">2</span> SLE15.xml<span class="callout" id="co-domxml-file">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-native"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Convert the XML file in domain XML format to the native guest
            configuration
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-argv"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            For the QEMU/KVM hypervisor, the format argument needs to be
            qemu-argv
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-file"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Domain XML file to use
          </p></td></tr></table></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE15.xml
LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE15 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE15.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE15.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</pre></div></section><section class="sect2" id="sec-vt-best-kernel-parameter" data-id-title="Change kernel parameters at boot time"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">Change kernel parameters at boot time</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-kernel-parameter">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect3" id="sec-vt-best-kernel-parameter-sle11" data-id-title="SUSE Linux Enterprise 11"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.5.1 </span><span class="title-name">SUSE Linux Enterprise 11</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-kernel-parameter-sle11">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          To change the value for SLE 11 products at boot time, you need to
          modify your <code class="filename">/boot/grub/menu.lst</code> file by adding
          the <code class="option">OPTION=parameter</code>. Then reboot your system.
        </p></section><section class="sect3" id="sec-vt-best-kernel-parameter-sle12" data-id-title="SUSE Linux Enterprise 12 and 15"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.5.2 </span><span class="title-name">SUSE Linux Enterprise 12 and 15</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-kernel-parameter-sle12">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          To change the value for SLE 12 and 15 products at boot time, you
          need to modify your <code class="filename">/etc/default/grub</code> file. Find
          the variable starting with
          <code class="option">GRUB_CMDLINE_LINUX_DEFAULT</code> and add at the end
          <code class="option">OPTION=parameter</code> (or change it with the correct
          value if it is already available).
        </p><p>
          Now you need to regenerate your <code class="literal">grub2</code>
          configuration:
        </p><div class="verbatim-wrap"><pre class="screen"># grub2-mkconfig -o /boot/grub2/grub.cfg</pre></div><p>
          Then reboot your system.
        </p></section></section><section class="sect2" id="sec-vt-best-guest-device-to-xml" data-id-title="Add a device to an XML configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6 </span><span class="title-name">Add a device to an XML configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-guest-device-to-xml">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To create a new VM Guest based on an XML file, you can specify the
        QEMU command line using the special tag
        <code class="literal">qemu:commandline</code>. For example, to add a
        virtio-balloon-pci, add this block at the end of the XML configuration
        file (before the &lt;/domain&gt; tag):
      </p><div class="verbatim-wrap"><pre class="screen">&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</pre></div></section><section class="sect2" id="sec-vm-guest-vcpu" data-id-title="Adding and removing CPUs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.7 </span><span class="title-name">Adding and removing CPUs</span></span> <a title="Permalink" class="permalink" href="#sec-vm-guest-vcpu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Some virtualization environments allow adding or removing CPUs while
        the virtual machine is running.
      </p><p>
        For the safe removal of CPUs, deactivate them first by executing
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo 0 &gt; /sys/devices/system/cpu/cpu<em class="replaceable">X</em>/online</code></pre></div><p>
        Replace <em class="replaceable">X</em> with the CPU number. To bring a
        CPU back online, execute
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo 1 &gt; /sys/devices/system/cpu/cpu<em class="replaceable">X</em>/online</code></pre></div></section><section class="sect2" id="sec-vm-guest-scsi-pr" data-id-title="SCSI persistent reservation on a multipathed device"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.8 </span><span class="title-name">SCSI persistent reservation on a multipathed device</span></span> <a title="Permalink" class="permalink" href="#sec-vm-guest-scsi-pr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        SCSI persistent reservations allow restricting access to block devices
        in a shared storage setup. This avoids improper multiple parallel
        accesses to the same block device from software components on local or
        remote hosts, which could lead to device damage and data corruption.
      </p><p>
        Find more information on managing storage multipath I/O in
        <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”</span>. Find more information about SCSI
        persistent reservations in
        <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”, Section 18.6.4 “SCSI persistent reservations and <code class="command">mpathpersist</code>”</span>.
      </p><p>
        For the virtualization scenario, QEMU's SCSI passthrough devices
        <code class="literal">scsi-block</code> and <code class="literal">scsi-generic</code>
        support passing guest persistent reservation requests to a privileged
        external helper program <code class="literal">qemu-pr-helper</code>. It needs to
        start before QEMU and creates a listener socket that accepts incoming
        connections for communication with QEMU.
      </p><div id="id-1.13.4.8.10.5" data-id-title="Live migration scenario with multipathed devices" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Live migration scenario with multipathed devices</div><p>
          We recommend using the multipath alias instead of
          <code class="literal">wwid</code>. It is useful in the VM Guest live migration
          scenario, because it makes sure that the storage paths are identical
          between the source and destination hosts.
        </p></div><div class="procedure" id="id-1.13.4.8.10.6" data-id-title="Adding a SCSI persistent reservation in a VM Guest against the related multipathed device in the VM Host Server:"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11: </span><span class="title-name">Adding a SCSI persistent reservation in a VM Guest against the related multipathed device in the VM Host Server: </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.8.10.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            In the VM Host Server, create a multipath environment. For more
            information, refer to <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”, Section 18.7 “Configuring the system for multipathing”</span> and
            <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”, Section 18.8 “Multipath configuration”</span>.
          </p></li><li class="step"><p>
            In the VM Host Server, configure the
            <code class="literal">&lt;reservations/&gt;</code> sub-element of the
            <code class="literal">&lt;source/&gt;</code> element of the
            <code class="literal">&lt;disk/&gt;</code> element for the passed-through lun in
            your <code class="systemitem">libvirt</code> domain configuration. Refer to
            <a class="link" href="https://libvirt.org/formatdomain.html" target="_blank"><code class="systemitem">libvirt</code>
            Domain XML format</a>.
          </p></li><li class="step"><p>
            In the VM Guest, install the <span class="package">sg3_utils</span> package
            and reserve the SCSI disks on demand by the
            <code class="command">sg_persist</code> command.
          </p></li></ol></div></div><div class="example" id="id-1.13.4.8.10.7" data-id-title="Practical example"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 1: </span><span class="title-name">Practical example </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.8.10.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              In the VM Host Server, verify that the
              <code class="systemitem">multipathd.service</code> is
              running, and that a multipathed disk exists and is named, for
              example, <code class="literal">storage1</code>.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  systemctl status multipathd.service
  ● multipathd.service - Device-Mapper Multipath Device Controller
        Loaded: loaded (/usr/lib/systemd/system/multipathd.service; enabled; preset: disabled)
        Active: active (running) since Sat 2023-08-26 21:34:13 CST; 1 week 1 day ago
  TriggeredBy: ○ multipathd.socket
      Main PID: 79411 (multipathd)
        Status: "up"
        Tasks: 7
          CPU: 1min 43.514s
        CGroup: /system.slice/multipathd.service
                └─79411 /sbin/multipathd -d -s</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  multipath -ll
  storage1 (36589cfc000000537c47ad3eb2b20216e) dm-6 TrueNAS,iSCSI Disk
  size=50G features='0' hwhandler='1 alua' wp=rw
  |-+- policy='service-time 0' prio=50 status=active
  | `- 16:0:0:0 sdg 8:96  active ready running
  `-+- policy='service-time 0' prio=50 status=enabled
    `- 17:0:0:0 sdh 8:112 active ready running</pre></div></li><li class="step"><p>
              In the VM Host Server, add a &lt;disk/&gt; element in the VM Guest
              configuration file by running <code class="command">virsh edit</code>.
            </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='block' device='lun'<span class="callout" id="co-disk-lunpassthrough">1</span>&gt;
  &lt;driver name='qemu' type='raw'/&gt;
  &lt;source dev='/dev/mapper/storage1'&gt;
    &lt;reservations<span class="callout" id="co-disk-reserve">2</span> managed='yes'<span class="callout" id="co-disk-reserve-managed">3</span>/&gt;
  &lt;/source&gt;
  &lt;target dev='sda' bus='scsi'/&gt;
  &lt;address type='drive' controller='0' bus='0' target='0' unit='0'<span class="callout" id="co-disk-scsi-hba-unit">4</span>/&gt;
&lt;/disk&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-lunpassthrough"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  To support persistent reservations, the disks must be marked
                  as <code class="literal">lun</code> with type <code class="literal">block</code>
                  so that QEMU does SCSI passthrough.
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-reserve"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  If present, it enables persistent reservations for SCSI based
                  disks. The element has one mandatory attribute
                  <code class="literal">managed</code> with accepted values
                  <code class="literal">yes</code> and <code class="literal">no</code>.
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-reserve-managed"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  If <code class="literal">managed</code> is <code class="literal">yes</code>,
                  <code class="systemitem">libvirt</code> prepares and manages any resources needed.
                </p><p>
                  When the value of the attribute <code class="literal">managed</code> is
                  <code class="literal">no</code>, then the hypervisor acts as a client
                  and the path to the server socket must be provided in the
                  child element source, which currently accepts only the
                  following attributes:
                </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.13.4.8.10.7.2.2.3.3.3.1"><span class="term">type</span></dt><dd><p>
                        The only valid option is <code class="literal">unix</code>.
                      </p></dd><dt id="id-1.13.4.8.10.7.2.2.3.3.3.2"><span class="term">path</span></dt><dd><p>
                        The path to the server socket.
                      </p></dd><dt id="id-1.13.4.8.10.7.2.2.3.3.3.3"><span class="term">mode</span></dt><dd><p>
                        The role of the hypervisor. Valid is
                        <code class="literal">client</code>.
                      </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-scsi-hba-unit"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Verify that the virtio-scsi HBA that the disk attaches
                  already exists and has available units (the maximum count of
                  units per virtio-scsi HBA is 7). Otherwise, you need to
                  manually add a virtio-scsi HBA to avoid automatically adding
                  the LSI HBA by <code class="systemitem">libvirt</code>. For example:
                </p><div class="verbatim-wrap"><pre class="screen">&lt;controller type='scsi' index='0' model='virtio-scsi'&gt;
  &lt;address type='pci' domain='0x0000' bus='0x03' slot='0x00' function='0x0'/&gt;
&lt;/controller&gt;</pre></div></td></tr></table></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh domblklist sles15sp5
  Target   Source
---------------------------------------------
  vda      /mnt/images/sles15sp5/disk0.qcow2
  sda      /dev/mapper/storage1</pre></div></li><li class="step"><p>
              In the VM Host Server, start the VM Guest. <code class="systemitem">libvirt</code> launches a
              qemu-pr-helper instance as the server role for the VM Guest
              sles15sp5, then launches the VM Guest sles15sp5 as the client
              role.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh start sles15sp5
    Domain 'sles15sp5' started</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh list
     Id   Name        State
    ---------------------------
     4    sles15sp5   running</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  ps -eo pid,args | grep -v grep | grep qemu-pr-helper
    37063 /usr/bin/qemu-pr-helper -k /var/lib/libvirt/qemu/domain-4-sles15sp5/pr-helper0.sock</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh dumpxml sles15sp5 | grep -A11 "&lt;disk type='block' device='lun'&gt;
  &lt;disk type='block' device='lun'&gt;
    &lt;driver name='qemu' type='raw'/&gt;
    &lt;source dev='/dev/mapper/storage1' index='1'&gt;
      &lt;reservations managed='yes'&gt;
        &lt;source type='unix' path='/var/lib/libvirt/qemu/domain-4-sles15sp5/pr-helper0.sock' mode='client'/&gt;
      &lt;/reservations&gt;
    &lt;/source&gt;
    &lt;backingStore/&gt;
    &lt;target dev='sda' bus='scsi'/&gt;
    &lt;alias name='scsi0-0-0-0'/&gt;
    &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;
  &lt;/disk&gt;</pre></div></li><li class="step"><p>
              In the VM Guest, reserve the scsi disk, for example,
              <code class="literal">sda</code>, with the key <code class="literal">123abc</code>.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda      8:0    0   50G  0 disk
vda    253:0    0   20G  0 disk
├─vda1 253:1    0    8M  0 part
├─vda2 253:2    0    2G  0 part [SWAP]
└─vda3 253:3    0   18G  0 part /</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --out --register --param-sark=123abc /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation out cdb: [5f 00 00 00 00 00 00 00 18 00]
PR out: command (Register) successful</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --in -k /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation in cdb: [5e 00 00 00 00 00 00 20 00 00]
  PR generation=0x5, 2 registered reservation keys follow:
    0x123abc
    0x123abc</pre></div></li><li class="step"><p>
              In the VM Guest, release the <code class="literal">sda</code> disk with
              the key <code class="literal">123abc</code>.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --out --clear --param-rk=123abc /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation out cdb: [5f 03 00 00 00 00 00 00 18 00]
PR out: command (Clear) successful</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --in -k /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation in cdb: [5e 00 00 00 00 00 00 20 00 00]
  PR generation=0x6, there are NO registered reservation keys</pre></div></li></ol></div></div></div></div></section></section><section class="sect1" id="sec-vt-best-refs" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-refs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <a class="link" href="https://kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf" target="_blank">Increasing
          memory density using KSM</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="http://www.linux-kvm.org/page/KSM" target="_blank">linux-kvm.org
          KSM</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/Documentation/vm/ksm.txt" target="_blank">KSM's
          kernel documentation</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://lwn.net/Articles/329123/" target="_blank">ksm - dynamic
          page sharing driver for linux v4</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="http://www.espenbraastad.no/post/memory-ballooning/" target="_blank">Memory
          Ballooning</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://wiki.libvirt.org/page/Virtio" target="_blank">libvirt
          virtio</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/html/latest/block/bfq-iosched.html" target="_blank">BFQ
          (Budget Fair Queueing)</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt" target="_blank">Documentation
          for sysctl</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://lwn.net/Articles/525459/" target="_blank">LWN Random
          Number</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="http://wiki.mikejung.biz/KVM_/_Xen" target="_blank">KVM / Xen
          tweaks</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf" target="_blank">Dr.
          Khoa Huynh, IBM Linux Technology Center</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/Documentation/admin-guide/kernel-parameters.txt" target="_blank">Kernel
          Parameters</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://lwn.net/Articles/374424/" target="_blank">Huge pages
          Administration (Mel Gorman)</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_blank">kernel
          hugetlbpage</a>
        </p></li></ul></div></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="#sec-vt-best-scenario"><span class="title-number">1 </span><span class="title-name">Virtualization scenarios</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-intro"><span class="title-number">2 </span><span class="title-name">Before you apply modifications</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-reco"><span class="title-number">3 </span><span class="title-name">Recommendations</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-hostlevel"><span class="title-number">4 </span><span class="title-name">VM Host Server configuration and resource allocation</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-img"><span class="title-number">5 </span><span class="title-name">VM Guest images</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-vmguests"><span class="title-number">6 </span><span class="title-name">VM Guest configuration</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-vm-setup-config"><span class="title-number">7 </span><span class="title-name">VM Guest-specific configurations and settings</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-refs"><span class="title-number">8 </span><span class="title-name">More information</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>