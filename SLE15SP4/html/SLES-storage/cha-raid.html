<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLES 15 SP4 | Storage Administration Guide | Software RAID configuration</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Software RAID configuration | SLES 15 SP4"/>
<meta name="description" content="The purpose of RAID (redundant array of independent disks) is to combine several hard disk partitions into one large virtual hard disk to optimize pe…"/>
<meta name="product-name" content="SUSE Linux Enterprise Server"/>
<meta name="product-number" content="15 SP4"/>
<meta name="book-title" content="Storage Administration Guide"/>
<meta name="chapter-title" content="Chapter 7. Software RAID configuration"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="fs@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise Server 15 SP4"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Storage Administration Guide"/>
<meta property="og:description" content="Administer storage devices on SLES"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Storage Administration Guide"/>
<meta name="twitter:description" content="Administer storage devices on SLES"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    
    "inLanguage": "en",
    

    "headline": "Software RAID configuration",
  
    "description": "Software RAID configuration",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2024-04-26T00:00+02:00",
      
    "datePublished": "2022-06-21T00:00+02:00",
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="part-software-raid.html" title="Part III. Software RAID"/><link rel="next" href="cha-raidroot.html" title="Chapter 8. Configuring software RAID for the root partition"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Storage Administration Guide</a><span> / </span><a class="crumb" href="part-software-raid.html">Software RAID</a><span> / </span><a class="crumb" href="cha-raid.html">Software RAID configuration</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Storage Administration Guide</div><ol><li><a href="storage-preface.html" class=" "><span class="title-number"> </span><span class="title-name">Preface</span></a></li><li><a href="part-filesystems.html" class="has-children "><span class="title-number">I </span><span class="title-name">File systems and mounting</span></a><ol><li><a href="cha-filesystems.html" class=" "><span class="title-number">1 </span><span class="title-name">Overview of file systems in Linux</span></a></li><li><a href="cha-resize-fs.html" class=" "><span class="title-number">2 </span><span class="title-name">Resizing file systems</span></a></li><li><a href="cha-uuid.html" class=" "><span class="title-number">3 </span><span class="title-name">Mounting storage devices</span></a></li><li><a href="cha-multitiercache.html" class=" "><span class="title-number">4 </span><span class="title-name">Multi-tier caching for block device operations</span></a></li></ol></li><li><a href="part-lvm.html" class="has-children "><span class="title-number">II </span><span class="title-name">Logical volumes (LVM)</span></a><ol><li><a href="cha-lvm.html" class=" "><span class="title-number">5 </span><span class="title-name">LVM configuration</span></a></li><li><a href="cha-lvm-snapshots.html" class=" "><span class="title-number">6 </span><span class="title-name">LVM volume snapshots</span></a></li></ol></li><li class="active"><a href="part-software-raid.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Software RAID</span></a><ol><li><a href="cha-raid.html" class=" you-are-here"><span class="title-number">7 </span><span class="title-name">Software RAID configuration</span></a></li><li><a href="cha-raidroot.html" class=" "><span class="title-number">8 </span><span class="title-name">Configuring software RAID for the root partition</span></a></li><li><a href="cha-raid10.html" class=" "><span class="title-number">9 </span><span class="title-name">Creating software RAID 10 devices</span></a></li><li><a href="cha-raid-degraded.html" class=" "><span class="title-number">10 </span><span class="title-name">Creating a degraded RAID array</span></a></li><li><a href="cha-raid-resize.html" class=" "><span class="title-number">11 </span><span class="title-name">Resizing software RAID arrays with mdadm</span></a></li><li><a href="cha-raid-leds.html" class=" "><span class="title-number">12 </span><span class="title-name">Storage enclosure LED utilities for MD software RAIDs</span></a></li><li><a href="cha-raidtroubleshooting.html" class=" "><span class="title-number">13 </span><span class="title-name">Troubleshooting software RAIDs</span></a></li></ol></li><li><a href="part-net-storage.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Network storage</span></a><ol><li><a href="cha-isns.html" class=" "><span class="title-number">14 </span><span class="title-name">iSNS for Linux</span></a></li><li><a href="cha-iscsi.html" class=" "><span class="title-number">15 </span><span class="title-name">Mass storage over IP networks: iSCSI</span></a></li><li><a href="cha-fcoe.html" class=" "><span class="title-number">16 </span><span class="title-name">Fibre Channel storage over Ethernet networks: FCoE</span></a></li><li><a href="cha-nvmeof.html" class=" "><span class="title-number">17 </span><span class="title-name">NVMe-oF</span></a></li><li><a href="cha-multipath.html" class=" "><span class="title-number">18 </span><span class="title-name">Managing multipath I/O for devices</span></a></li><li><a href="cha-nfs.html" class=" "><span class="title-number">19 </span><span class="title-name">Sharing file systems with NFS</span></a></li><li><a href="cha-samba.html" class=" "><span class="title-number">20 </span><span class="title-name">Samba</span></a></li><li><a href="cha-autofs.html" class=" "><span class="title-number">21 </span><span class="title-name">On-demand mounting with autofs</span></a></li></ol></li><li><a href="bk09apa.html" class=" "><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="chapter" id="cha-raid" data-id-title="Software RAID configuration"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">15 SP4</span></span></span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Software RAID configuration</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large virtual hard disk to optimize
  performance, data security, or both. Most RAID controllers use the SCSI
  protocol, because it can address a larger number of hard disks in a more
  effective way than the IDE protocol and is more suitable for parallel
  processing of commands. There are some RAID controllers that support IDE or
  SATA hard disks. Software RAID provides the advantages of RAID systems
  without the additional cost of hardware RAID controllers. However, this
  requires some CPU time and has memory requirements that make it unsuitable
  for real high performance computers.
 </p><div id="id-1.11.5.2.4" data-id-title="RAID on cluster file systems" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: RAID on cluster file systems</div><p>
   Software RAID underneath clustered file systems needs to be set up using a
   cluster multi-device (Cluster MD). Refer to the
   <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/cha-ha-cluster-md.html" target="_blank">
   <em class="citetitle">Administration Guide for SUSE Linux Enterprise High Availability</em></a>.
  </p></div><p>
  SUSE Linux Enterprise offers the option of combining several hard disks into one soft RAID
  system. RAID implies several strategies for combining several hard disks in a
  RAID system, each with different goals, advantages, and characteristics.
  These variations are commonly known as <span class="emphasis"><em>RAID levels</em></span>.
 </p><section class="sect1" id="sec-raid-intro" data-id-title="Understanding RAID levels"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">Understanding RAID levels</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section describes common RAID levels 0, 1, 2, 3, 4, 5, and nested RAID
   levels.
  </p><section class="sect2" id="sec-raid-intro-raid0" data-id-title="RAID 0"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.1 </span><span class="title-name">RAID 0</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid0">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This level improves the performance of your data access by spreading out
    blocks of each file across multiple disks. Actually, this is not a RAID,
    because it does not provide data backup, but the name
    <span class="emphasis"><em>RAID 0</em></span> for this type of system has become the
    norm. With RAID 0, two or more hard disks are pooled together. The
    performance is very good, but the RAID system is destroyed and your data
    lost if even one hard disk fails.
   </p></section><section class="sect2" id="sec-raid-intro-raid1" data-id-title="RAID 1"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.2 </span><span class="title-name">RAID 1</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid1">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This level provides adequate security for your data, because the data is
    copied to another hard disk 1:1. This is known as <span class="emphasis"><em>hard disk
    mirroring</em></span>. If a disk is destroyed, a copy of its contents is
    available on another mirrored disk. All disks except one could be damaged
    without endangering your data. However, if damage is not detected, damaged
    data might be mirrored to the correct disk and the data is corrupted that
    way. The writing performance suffers a little in the copying process
    compared to when using single disk access (10 to 20 percent slower), but
    read access is significantly faster in comparison to any one of the normal
    physical hard disks, because the data is duplicated so can be scanned in
    parallel. RAID 1 generally provides nearly twice the read transaction rate
    of single disks and almost the same write transaction rate as single disks.
   </p></section><section class="sect2" id="sec-raid-intro-raid23" data-id-title="RAID 2 and RAID 3"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.3 </span><span class="title-name">RAID 2 and RAID 3</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid23">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    These are not typical RAID implementations. Level 2 stripes data at
    the bit level rather than the block level. Level 3 provides byte-level
    striping with a dedicated parity disk and cannot service simultaneous
    multiple requests. Both levels are rarely used.
   </p></section><section class="sect2" id="sec-raid-intro-raid4" data-id-title="RAID 4"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.4 </span><span class="title-name">RAID 4</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Level 4 provides block-level striping like Level 0 combined with
    a dedicated parity disk. If a data disk fails, the parity data is used to
    create a replacement disk. However, the parity disk might create a
    bottleneck for write access. Nevertheless, Level 4 is sometimes used.
   </p></section><section class="sect2" id="sec-raid-intro-raid5" data-id-title="RAID 5"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.5 </span><span class="title-name">RAID 5</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    RAID 5 is an optimized compromise between Level 0 and
    Level 1 in terms of performance and redundancy. The hard disk space
    equals the number of disks used minus one. The data is distributed over the
    hard disks as with RAID 0. <span class="emphasis"><em>Parity blocks</em></span>, created
    on one of the partitions, are there for security reasons. They are linked
    to each other with XOR, enabling the contents to be reconstructed by the
    corresponding parity block in case of system failure. With RAID 5, no
    more than one hard disk can fail at the same time. If one hard disk fails,
    it must be replaced when possible to avoid the risk of losing data.
   </p></section><section class="sect2" id="sec-raid-intro-raid6" data-id-title="RAID 6"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.6 </span><span class="title-name">RAID 6</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    RAID 6 is an extension of RAID 5 that allows for additional fault
    tolerance by using a second independent distributed parity scheme (dual
    parity). Even if two of the hard disks fail during the data recovery
    process, the system continues to be operational, with no data loss.
   </p><p>
    RAID 6 provides for extremely high data fault tolerance by sustaining
    multiple simultaneous drive failures. It handles the loss of any two
    devices without data loss. Accordingly, it requires N+2 drives to store N
    drives worth of data. It requires a minimum of four devices.
   </p><p>
    The performance for RAID 6 is slightly lower but comparable to
    RAID 5 in normal mode and single disk failure mode. It is very slow in
    dual disk failure mode. A RAID 6 configuration needs a considerable
    amount of CPU time and memory for write operations.
   </p><div class="table" id="id-1.11.5.2.6.8.5" data-id-title="Comparison of RAID 5 and RAID 6"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 7.1: </span><span class="title-name">Comparison of RAID 5 and RAID 6 </span></span><a title="Permalink" class="permalink" href="cha-raid.html#id-1.11.5.2.6.8.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Feature
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         RAID 5
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         RAID 6
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Number of devices
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N+1, minimum of 3
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         N+2, minimum of 4
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Parity
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Distributed, single
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Distributed, dual
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Performance
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Medium impact on write and rebuild
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         More impact on sequential write than RAID 5
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Fault-tolerance
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         Failure of one component device
        </p>
       </td><td>
        <p>
         Failure of two component devices
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-raid-intro-raid-nested" data-id-title="Nested and complex RAID levels"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.7 </span><span class="title-name">Nested and complex RAID levels</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid-nested">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Other RAID levels have been developed, such as RAIDn, RAID 10,
    RAID 0+1, RAID 30, and RAID 50. Some are proprietary
    implementations created by hardware vendors. Examples for creating
    RAID 10 configurations can be found in <a class="xref" href="cha-raid10.html" title="Chapter 9. Creating software RAID 10 devices">Chapter 9, <em>Creating software RAID 10 devices</em></a>.
   </p></section></section><section class="sect1" id="sec-raid-yast" data-id-title="Soft RAID configuration with YaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Soft RAID configuration with YaST</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The YaST soft RAID configuration can be reached from the YaST Expert
   Partitioner. This partitioning tool also enables you to edit and delete
   existing partitions and create new ones that should be used with soft RAID.
   These instructions apply on setting up RAID levels 0, 1, 5, and 6. Setting
   up RAID 10 configurations is explained in <a class="xref" href="cha-raid10.html" title="Chapter 9. Creating software RAID 10 devices">Chapter 9, <em>Creating software RAID 10 devices</em></a>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     If necessary, create partitions that should be used with your RAID
     configuration. Do not format them and set the partition type to
     <span class="guimenu">0xFD Linux RAID</span>. When using existing partitions it is
     not necessary to change their partition type—YaST will
     automatically do so. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 10 “<span class="guimenu">Expert Partitioner</span>”, Section 10.1 “Using the <span class="guimenu">Expert Partitioner</span>”</span> for
     details.
    </p><p>
     It is strongly recommended to use partitions stored on different hard
     disks to decrease the risk of losing data if one is defective (RAID 1
     and 5) and to optimize the performance of RAID 0.
    </p><p>
     For RAID 0 at least two partitions are needed. RAID 1 requires
     exactly two partitions, while at least three partitions are required for
     RAID 5. A RAID 6 setup requires at least four partitions. It is
     recommended to use only partitions of the same size because each segment
     can contribute only the same amount of space as the smallest sized
     partition.
    </p></li><li class="step"><p>
     In the left panel, select <span class="guimenu">RAID</span>.
    </p><p>
     A list of existing RAID configurations opens in the right panel.
    </p></li><li class="step"><p>
     At the lower left of the RAID page, click <span class="guimenu">Add RAID</span>.
    </p></li><li class="step"><p>
     Select a <span class="guimenu">RAID Type</span> and <span class="guimenu">Add</span> an
     appropriate number of partitions from the <span class="guimenu">Available
     Devices</span> dialog.
    </p><p>
     You can optionally assign a <span class="guimenu">RAID Name</span> to your RAID. It
     will make it available as
     <code class="filename">/dev/md/<em class="replaceable">NAME</em></code>. See
     <a class="xref" href="cha-raid.html#sec-raid-yast-names" title="7.2.1. RAID names">Section 7.2.1, “RAID names”</a> for more information.
    </p><div class="figure" id="fig-yast2-raid3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_raid3_a.png"><img src="images/yast2_raid3_a.png" width="100%" alt="Example RAID 5 configuration" title="Example RAID 5 configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.1: </span><span class="title-name">Example RAID 5 configuration </span></span><a title="Permalink" class="permalink" href="cha-raid.html#fig-yast2-raid3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div><p>
     Proceed with <span class="guimenu">Next</span>.
    </p></li><li class="step"><p>
     Select the <span class="guimenu">Chunk Size</span> and, if applicable, the
     <span class="guimenu">Parity Algorithm</span>. The optimal chunk size depends on the
     type of data and the type of RAID. See
     <a class="link" href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes" target="_blank">https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes</a>
     for more information. More information on parity algorithms can be found
     with <code class="command">man 8 mdadm</code> when searching for the
     <code class="option">--layout</code> option. If unsure, stick with the defaults.
    </p></li><li class="step"><p>
     Choose a <span class="guimenu">Role</span> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <span class="guimenu">Raw Volume
     (Unformatted)</span>.
    </p></li><li class="step"><p>
     Under <span class="guimenu">Formatting Options</span>, select <span class="guimenu">Format
     Partition</span>, then select the <span class="guimenu">File system</span>. The
     content of the <span class="guimenu">Options</span> menu depends on the file system.
     Usually there is no need to change the defaults.
    </p><p>
     Under <span class="guimenu">Mounting Options</span>, select <span class="guimenu">Mount
     partition</span>, then select the mount point. Click <span class="guimenu">Fstab
     Options</span> to add special mounting options for the volume.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Finish</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Next</span>, verify that the changes are listed, then
     click <span class="guimenu">Finish</span>.
    </p></li></ol></div></div><div id="id-1.11.5.2.7.4" data-id-title="RAID on disks" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: RAID on disks</div><p>
    While the partitioner makes it possible to create a RAID on top of disks
    instead of partitions, we do not recommend this approach for a number of
    reasons. Installing a bootloader on such RAID is not supported, so you need
    to use a separate device for booting. Tools like <span class="package">fdisk</span>
    and <span class="package">parted</span> do not work properly with such RAIDs, which
    may lead to incorrect diagnosis and actions by a person who is unaware of
    the RAID's particular setup.
   </p></div><section class="sect2" id="sec-raid-yast-names" data-id-title="RAID names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2.1 </span><span class="title-name">RAID names</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-yast-names">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    By default, software RAID devices have numeric names following the pattern
    <code class="literal">mdN</code>, where <code class="literal">N</code> is a number. As such
    they can be accessed as, for example, <code class="filename">/dev/md127</code> and
    are listed as <code class="literal">md127</code> in <code class="filename">/proc/mdstat</code>
    and <code class="filename">/proc/partitions</code>. Working with these names can be
    clumsy. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers two ways to work around this problem:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.5.2.7.5.3.1"><span class="term">Providing a named link to the device</span></dt><dd><p>
       You can optionally specify a name for the RAID device when creating it
       with YaST or on the command line with <code class="command">mdadm --create
       '/dev/md/</code> <em class="replaceable">NAME</em>'. The device name
       will still be <code class="literal">mdN</code>, but a link
       <code class="filename">/dev/md/<em class="replaceable">NAME</em></code> will be
       created:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>ls -og /dev/md
total 0
lrwxrwxrwx 1 8 Dec  9 15:11 myRAID -&gt; ../md127</pre></div><p>
       The device will still be listed as <code class="literal">md127</code> under
       <code class="filename">/proc</code>.
      </p></dd><dt id="id-1.11.5.2.7.5.3.2"><span class="term">Providing a named device</span></dt><dd><p>
       In case a named link to the device is not sufficient for your setup, add
       the line <code class="literal">CREATE names=yes</code> to
       <code class="filename">/etc/mdadm.conf</code> by running the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>echo "CREATE names=yes" | sudo tee -a  /etc/mdadm.conf</pre></div><p>
       This will cause names like <code class="literal">myRAID</code> to be used as a
       <span class="quote">“<span class="quote">real</span>”</span> device name. The device will not only be accessible
       at <code class="filename">/dev/myRAID</code>, but also be listed as
       <code class="literal">myRAID</code> under <code class="filename">/proc</code>. Note that
       this will only apply to RAIDs configured after the change to the
       configuration file. Active RAIDs will continue to use the
       <code class="literal">mdN</code> names until they get stopped and re-assembled.
      </p><div id="id-1.11.5.2.7.5.3.2.2.4" data-id-title="Incompatible tools" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Incompatible tools</div><p>
        Not all tools may support named RAID devices. In case a tool expects a
        RAID device to be named <code class="literal">mdN</code>, it will fail to
        identify the devices.
       </p></div></dd></dl></div></section></section><section class="sect1" id="sec-arm-raid" data-id-title="Configuring stripe size on RAID 5 on AArch64"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Configuring stripe size on RAID 5 on AArch64</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-arm-raid">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   By default, the stripe size is set to 4kB. If you need to change the default
   stripe size, for example, to match the typical page size of 64kB on
   AArch64, you can configure the stripe size manually using CLI:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> echo 16384  &gt; /sys/block/md1/md/stripe_size</pre></div><p>
   The above command sets the stripe size to 16kB. You can set other values
   such as 4096, 8192; but the value must be a power of 2.
  </p></section><section class="sect1" id="sec-raid-counters" data-id-title="Monitoring software RAIDs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">Monitoring software RAIDs</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-counters">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can run <code class="command">mdadm</code> as a daemon in the
   <code class="literal">monitor</code> mode to monitor your software RAID. In the
   <code class="literal">monitor</code> mode, <code class="command">mdadm</code> performs regular
   checks on the array for disk failures. If there is a failure,
   <code class="command">mdadm</code> sends an email to the administrator. To define the
   time interval of the checks, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen">mdadm --monitor --mail=root@localhost --delay=1800 /dev/md2</pre></div><p>
   The command above turns on monitoring of the <code class="literal">/dev/md2</code>
   array in intervals of 1800 seconds. In the event of a failure, an email
   will be sent to <code class="literal">root@localhost</code>.
  </p><div id="id-1.11.5.2.9.5" data-id-title="RAID checks are enabled by default" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: RAID checks are enabled by default</div><p>
    The RAID checks are enabled by default. It may happen that the interval
    between each check is not long enough and you may encounter warnings. Thus,
    you can increase the interval by setting a higher value with the
    <code class="literal">delay</code> option.
   </p></div></section><section class="sect1" id="sec-raid-more" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Configuration instructions and more details for soft RAID can be found in
   the HOWTOs at:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <em class="citetitle">The Linux RAID wiki</em>:
     <a class="link" href="https://raid.wiki.kernel.org/" target="_blank">https://raid.wiki.kernel.org/</a>
    </p></li><li class="listitem"><p>
     <em class="citetitle">The Software RAID HOWTO</em> in the
     <code class="filename">/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</code>
     file
    </p></li></ul></div><p>
   Linux RAID mailing lists are also available, such as
   <em class="citetitle">linux-raid</em> at
   <a class="link" href="http://marc.info/?l=linux-raid" target="_blank">http://marc.info/?l=linux-raid</a>.
  </p></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="part-software-raid.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part III </span>Software RAID</span></a> </div><div><a class="pagination-link next" href="cha-raidroot.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 8 </span>Configuring software RAID for the root partition</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-raid.html#sec-raid-intro"><span class="title-number">7.1 </span><span class="title-name">Understanding RAID levels</span></a></span></li><li><span class="sect1"><a href="cha-raid.html#sec-raid-yast"><span class="title-number">7.2 </span><span class="title-name">Soft RAID configuration with YaST</span></a></span></li><li><span class="sect1"><a href="cha-raid.html#sec-arm-raid"><span class="title-number">7.3 </span><span class="title-name">Configuring stripe size on RAID 5 on AArch64</span></a></span></li><li><span class="sect1"><a href="cha-raid.html#sec-raid-counters"><span class="title-number">7.4 </span><span class="title-name">Monitoring software RAIDs</span></a></span></li><li><span class="sect1"><a href="cha-raid.html#sec-raid-more"><span class="title-number">7.5 </span><span class="title-name">More information</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>