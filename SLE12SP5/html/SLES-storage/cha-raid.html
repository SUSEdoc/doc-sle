<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLES 12 SP5 | Storage Administration Guide | Software RAID Configuration</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Software RAID Configuration | SLES 12 SP5"/>
<meta name="description" content="The purpose of RAID (redundant array of independent disks) is to combine several hard disk partitions into one large virtual hard disk to optimize pe…"/>
<meta name="product-name" content="SUSE Linux Enterprise Server"/>
<meta name="product-number" content="12 SP5"/>
<meta name="book-title" content="Storage Administration Guide"/>
<meta name="chapter-title" content="Chapter 7. Software RAID Configuration"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="fs@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise Server 12 SP5"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Storage Administration Guide"/>
<meta property="og:description" content="Administer storage devices on SLES"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Storage Administration Guide"/>
<meta name="twitter:description" content="Administer storage devices on SLES"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    
    "inLanguage": "en",
    

    "headline": "Software RAID Configuration",
  
    "description": "Software RAID Configuration",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2019-12-09T00:00+02:00",
      
    "datePublished": "2019-12-09T00:00+02:00",
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="part-software-raid.html" title="Part III. Software RAID"/><link rel="next" href="cha-raidroot.html" title="Chapter 8. Configuring Software RAID for the Root Partition"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Storage Administration Guide</a><span> / </span><a class="crumb" href="part-software-raid.html">Software RAID</a><span> / </span><a class="crumb" href="cha-raid.html">Software RAID Configuration</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Storage Administration Guide</div><ol><li><a href="storage-preface.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-filesystems.html" class="has-children "><span class="title-number">I </span><span class="title-name">File Systems and Mounting</span></a><ol><li><a href="cha-filesystems.html" class=" "><span class="title-number">1 </span><span class="title-name">Overview of File Systems in Linux</span></a></li><li><a href="cha-resize-fs.html" class=" "><span class="title-number">2 </span><span class="title-name">Resizing File Systems</span></a></li><li><a href="cha-uuid.html" class=" "><span class="title-number">3 </span><span class="title-name">Using UUIDs to Mount Devices</span></a></li><li><a href="cha-multitiercache.html" class=" "><span class="title-number">4 </span><span class="title-name">Multi-tier Caching for Block Device Operations</span></a></li></ol></li><li><a href="part-lvm.html" class="has-children "><span class="title-number">II </span><span class="title-name">Logical Volumes (LVM)</span></a><ol><li><a href="cha-lvm.html" class=" "><span class="title-number">5 </span><span class="title-name">LVM Configuration</span></a></li><li><a href="cha-lvm-snapshots.html" class=" "><span class="title-number">6 </span><span class="title-name">LVM Volume Snapshots</span></a></li></ol></li><li class="active"><a href="part-software-raid.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Software RAID</span></a><ol><li><a href="cha-raid.html" class=" you-are-here"><span class="title-number">7 </span><span class="title-name">Software RAID Configuration</span></a></li><li><a href="cha-raidroot.html" class=" "><span class="title-number">8 </span><span class="title-name">Configuring Software RAID for the Root Partition</span></a></li><li><a href="cha-raid10.html" class=" "><span class="title-number">9 </span><span class="title-name">Creating Software RAID 10 Devices</span></a></li><li><a href="cha-raid-degraded.html" class=" "><span class="title-number">10 </span><span class="title-name">Creating a Degraded RAID Array</span></a></li><li><a href="cha-raid-resize.html" class=" "><span class="title-number">11 </span><span class="title-name">Resizing Software RAID Arrays with mdadm</span></a></li><li><a href="cha-raid-leds.html" class=" "><span class="title-number">12 </span><span class="title-name">Storage Enclosure LED Utilities for MD Software RAIDs</span></a></li></ol></li><li><a href="part-net-storage.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Network Storage</span></a><ol><li><a href="cha-isns.html" class=" "><span class="title-number">13 </span><span class="title-name">iSNS for Linux</span></a></li><li><a href="cha-iscsi.html" class=" "><span class="title-number">14 </span><span class="title-name">Mass Storage over IP Networks: iSCSI</span></a></li><li><a href="cha-fcoe.html" class=" "><span class="title-number">15 </span><span class="title-name">Fibre Channel Storage over Ethernet Networks: FCoE</span></a></li><li><a href="cha-nvmeof.html" class=" "><span class="title-number">16 </span><span class="title-name">NVMe-oF</span></a></li><li><a href="cha-multipath.html" class=" "><span class="title-number">17 </span><span class="title-name">Managing multipath I/O for devices</span></a></li><li><a href="cha-nfs4-acls.html" class=" "><span class="title-number">18 </span><span class="title-name">Managing Access Control Lists over NFSv4</span></a></li></ol></li><li><a href="bk08apa.html" class=" "><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="chapter" id="cha-raid" data-id-title="Software RAID Configuration"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber">12 SP5</span></span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Software RAID Configuration</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large virtual hard disk to optimize
  performance, data security, or both. Most RAID controllers use the SCSI
  protocol because it can address a larger number of hard disks in a more
  effective way than the IDE protocol and is more suitable for parallel
  processing of commands. There are some RAID controllers that support IDE or
  SATA hard disks. Software RAID provides the advantages of RAID systems
  without the additional cost of hardware RAID controllers. However, this
  requires some CPU time and has memory requirements that make it unsuitable
  for real high performance computers.
 </p><div id="id-1.10.5.2.4" data-id-title="RAID on Cluster File Systems" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: RAID on Cluster File Systems</div><p>
   Software RAID underneath clustered file systems needs to be set up using a
   cluster multi-device (Cluster MD). Refer to the High Availability documentation at
   <a class="link" href="https://documentation.suse.com/sle-ha/html/SLE-HA-all/cha-ha-cluster-md.html" target="_blank">https://documentation.suse.com/sle-ha/html/SLE-HA-all/cha-ha-cluster-md.html</a>.
  </p></div><p>
  SUSE Linux Enterprise offers the option of combining several hard disks into one soft RAID
  system. RAID implies several strategies for combining several hard disks in a
  RAID system, each with different goals, advantages, and characteristics.
  These variations are commonly known as <span class="emphasis"><em>RAID levels</em></span>.
 </p><section class="sect1" id="sec-raid-intro" data-id-title="Understanding RAID Levels"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">Understanding RAID Levels</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section describes common RAID levels 0, 1, 2, 3, 4, 5, and nested RAID
   levels.
  </p><section class="sect2" id="sec-raid-intro-raid0" data-id-title="RAID 0"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.1 </span><span class="title-name">RAID 0</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid0">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This level improves the performance of your data access by spreading out
    blocks of each file across multiple disks. Actually, this is not really a
    RAID, because it does not provide data backup, but the name
    <span class="emphasis"><em>RAID 0</em></span> for this type of system has become the
    norm. With RAID 0, two or more hard disks are pooled together. The
    performance is very good, but the RAID system is destroyed and your data
    lost if even one hard disk fails.
   </p></section><section class="sect2" id="sec-raid-intro-raid1" data-id-title="RAID 1"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.2 </span><span class="title-name">RAID 1</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid1">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This level provides adequate security for your data, because the data is
    copied to another hard disk 1:1. This is known as <span class="emphasis"><em>hard disk
    mirroring</em></span>. If a disk is destroyed, a copy of its contents is
    available on another mirrored disk. All disks except one could be damaged
    without endangering your data. However, if damage is not detected, damaged
    data might be mirrored to the correct disk and the data is corrupted that
    way. The writing performance suffers a little in the copying process
    compared to when using single disk access (10 to 20 percent slower), but
    read access is significantly faster in comparison to any one of the normal
    physical hard disks, because the data is duplicated so can be scanned in
    parallel. RAID 1 generally provides nearly twice the read transaction rate
    of single disks and almost the same write transaction rate as single disks.
   </p></section><section class="sect2" id="sec-raid-intro-raid23" data-id-title="RAID 2 and RAID 3"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.3 </span><span class="title-name">RAID 2 and RAID 3</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid23">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    These are not typical RAID implementations. Level 2 stripes data at
    the bit level rather than the block level. Level 3 provides byte-level
    striping with a dedicated parity disk and cannot service simultaneous
    multiple requests. Both levels are rarely used.
   </p></section><section class="sect2" id="sec-raid-intro-raid4" data-id-title="RAID 4"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.4 </span><span class="title-name">RAID 4</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Level 4 provides block-level striping like Level 0 combined with
    a dedicated parity disk. If a data disk fails, the parity data is used to
    create a replacement disk. However, the parity disk might create a
    bottleneck for write access. Nevertheless, Level 4 is sometimes used.
   </p></section><section class="sect2" id="sec-raid-intro-raid5" data-id-title="RAID 5"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.5 </span><span class="title-name">RAID 5</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    RAID 5 is an optimized compromise between Level 0 and
    Level 1 in terms of performance and redundancy. The hard disk space
    equals the number of disks used minus one. The data is distributed over the
    hard disks as with RAID 0. <span class="emphasis"><em>Parity blocks</em></span>, created
    on one of the partitions, are there for security reasons. They are linked
    to each other with XOR, enabling the contents to be reconstructed by the
    corresponding parity block in case of system failure. With RAID 5, no
    more than one hard disk can fail at the same time. If one hard disk fails,
    it must be replaced when possible to avoid the risk of losing data.
   </p></section><section class="sect2" id="sec-raid-intro-raid6" data-id-title="RAID 6"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.6 </span><span class="title-name">RAID 6</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    RAID 6 is essentially an extension of RAID 5 that allows for
    additional fault tolerance by using a second independent distributed parity
    scheme (dual parity). Even if two of the hard disks fail during the data
    recovery process, the system continues to be operational, with no data
    loss.
   </p><p>
    RAID 6 provides for extremely high data fault tolerance by sustaining
    multiple simultaneous drive failures. It handles the loss of any two
    devices without data loss. Accordingly, it requires N+2 drives to store N
    drives worth of data. It requires a minimum of four devices.
   </p><p>
    The performance for RAID 6 is slightly lower but comparable to
    RAID 5 in normal mode and single disk failure mode. It is very slow in
    dual disk failure mode. A RAID 6 configuration needs a considerable
    amount of CPU time and memory for write operations.
   </p><div class="table" id="id-1.10.5.2.6.8.5" data-id-title="Comparison of RAID 5 and RAID 6"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 7.1: </span><span class="title-name">Comparison of RAID 5 and RAID 6 </span></span><a title="Permalink" class="permalink" href="cha-raid.html#id-1.10.5.2.6.8.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Feature
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         RAID 5
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         RAID 6
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Number of devices
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N+1, minimum of 3
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         N+2, minimum of 4
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Parity
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Distributed, single
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Distributed, dual
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Performance
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Medium impact on write and rebuild
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         More impact on sequential write than RAID 5
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Fault-tolerance
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         Failure of one component device
        </p>
       </td><td>
        <p>
         Failure of two component devices
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-raid-intro-raid-nested" data-id-title="Nested and Complex RAID Levels"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.7 </span><span class="title-name">Nested and Complex RAID Levels</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-intro-raid-nested">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Other RAID levels have been developed, such as RAIDn, RAID 10,
    RAID 0+1, RAID 30, and RAID 50. Some are proprietary
    implementations created by hardware vendors. Examples for creating
    RAID 10 configurations can be found in <a class="xref" href="cha-raid10.html" title="Chapter 9. Creating Software RAID 10 Devices">Chapter 9, <em>Creating Software RAID 10 Devices</em></a>.
   </p></section></section><section class="sect1" id="sec-raid-yast" data-id-title="Soft RAID Configuration with YaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Soft RAID Configuration with YaST</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The YaST soft RAID configuration can be reached from the YaST Expert
   Partitioner. This partitioning tool also enables you to edit and delete
   existing partitions and create new ones that should be used with soft RAID.
   These instructions apply on setting up RAID levels 0, 1, 5, and 6. Setting
   up RAID 10 configurations is explained in <a class="xref" href="cha-raid10.html" title="Chapter 9. Creating Software RAID 10 Devices">Chapter 9, <em>Creating Software RAID 10 Devices</em></a>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     If necessary, create partitions that should be used with your RAID
     configuration. Do not format them and set the partition type to
     <span class="guimenu">0xFD Linux RAID</span>. When using existing partitions it is
     not necessary to change their partition type—YaST will
     automatically do so. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 13 “Advanced Disk Setup”, Section 13.1 “Using the YaST Partitioner”</span>
     for details.
    </p><p>
     It is strongly recommended to use partitions stored on different hard
     disks to decrease the risk of losing data if one is defective (RAID 1
     and 5) and to optimize the performance of RAID 0.
    </p><p>
     For RAID 0 at least two partitions are needed. RAID 1 requires
     exactly two partitions, while at least three partitions are required for
     RAID 5. A RAID 6 setup requires at least four partitions. It is
     recommended to use only partitions of the same size because each segment
     can contribute only the same amount of space as the smallest sized
     partition.
    </p></li><li class="step"><p>
     In the left panel, select <span class="guimenu">RAID</span>.
    </p><p>
     A list of existing RAID configurations opens in the right panel.
    </p></li><li class="step"><p>
     At the lower left of the RAID page, click <span class="guimenu">Add RAID</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_raid2_a.png"><img src="images/yast2_raid2_a.png" width="100%" alt="Add RAID" title="Add RAID"/></a></div></div></li><li class="step"><p>
     Select a <span class="guimenu">RAID Type</span> and <span class="guimenu">Add</span> an
     appropriate number of partitions from the <span class="guimenu">Available
     Devices</span> dialog.
    </p><p>
     You can optionally assign a <span class="guimenu">RAID Name</span> to your RAID. It
     will make it available as
     <code class="filename">/dev/md/<em class="replaceable">NAME</em></code>. See
     <a class="xref" href="cha-raid.html#sec-raid-yast-names" title="7.2.1. RAID Names">Section 7.2.1, “RAID Names”</a> for more information.
    </p><div class="figure" id="fig-yast2-raid3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_raid3_a.png"><img src="images/yast2_raid3_a.png" width="100%" alt="Example RAID 5 Configuration" title="Example RAID 5 Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.1: </span><span class="title-name">Example RAID 5 Configuration </span></span><a title="Permalink" class="permalink" href="cha-raid.html#fig-yast2-raid3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div><p>
     Proceed with <span class="guimenu">Next</span>.
    </p></li><li class="step"><p>
     Select the <span class="guimenu">Chunk Size</span> and, if applicable, the
     <span class="guimenu">Parity Algorithm</span>. The optimal chunk size depends on the
     type of data and the type of RAID. See
     <a class="link" href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes" target="_blank">https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes</a>
     for more information. More information on parity algorithms can be found
     with <code class="command">man 8 mdadm</code> when searching for the
     <code class="option">--layout</code> option. If unsure, stick with the defaults.
    </p></li><li class="step"><p>
     Choose a <span class="guimenu">Role</span> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <span class="guimenu">Raw Volume
     (Unformatted)</span>.
    </p></li><li class="step"><p>
     Under <span class="guimenu">Formatting Options</span>, select <span class="guimenu">Format
     Partition</span>, then select the <span class="guimenu">File system</span>. The
     content of the <span class="guimenu">Options</span> menu depends on the file system.
     Usually there is no need to change the defaults.
    </p><p>
     Under <span class="guimenu">Mounting Options</span>, select <span class="guimenu">Mount
     partition</span>, then select the mount point. Click <span class="guimenu">Fstab
     Options</span> to add special mounting options for the volume.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Finish</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Next</span>, verify that the changes are listed, then
     click <span class="guimenu">Finish</span>.
    </p></li></ol></div></div><section class="sect2" id="sec-raid-yast-names" data-id-title="RAID Names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2.1 </span><span class="title-name">RAID Names</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-yast-names">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    By default, software RAID devices have numeric names following the pattern
    <code class="literal">mdN</code>, where <code class="literal">N</code> is a number. As such
    they can be accessed as, for example, <code class="filename">/dev/md127</code> and
    are listed as <code class="literal">md127</code> in <code class="filename">/proc/mdstat</code>
    and <code class="filename">/proc/partitions</code>. Working with these names can be
    clumsy. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers two ways to work around this problem:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.10.5.2.7.4.3.1"><span class="term">Providing a Named Link to the Device</span></dt><dd><p>
       You can optionally specify a name for the RAID device when creating it
       with YaST or on the command line with <code class="command">mdadm --create
       '/dev/md/</code> <em class="replaceable">NAME</em>'. The device name
       will still be <code class="literal">mdN</code>, but a link
       <code class="filename">/dev/md/<em class="replaceable">NAME</em></code> will be
       created:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ls -og /dev/md
total 0
lrwxrwxrwx 1 8 Dec  9 15:11 myRAID -&gt; ../md127</pre></div><p>
       The device will still be listed as <code class="literal">md127</code> under
       <code class="filename">/proc</code>.
      </p></dd><dt id="id-1.10.5.2.7.4.3.2"><span class="term">Providing a Named Device</span></dt><dd><p>
       In case a named link to the device is not sufficient for your setup, add
       the line CREATE names=yes to <code class="filename">/etc/mdadm.conf</code> by
       running the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>echo "CREATE names=yes" | sudo tee -a  /etc/mdadm.conf</pre></div><p>
       It will cause names like <code class="literal">myRAID</code> to be used as a
       <span class="quote">“<span class="quote">real</span>”</span> device name. The device will not only be accessible
       at <code class="filename">/dev/myRAID</code>, but also be listed as
       <code class="literal">myRAID</code> under <code class="filename">/proc</code>. Note that
       this will only apply to RAIDs configured after the change to the
       configuration file. Active RAIDS will continue to use the
       <code class="literal">mdN</code> names until they get stopped and re-assembled.
      </p><div id="id-1.10.5.2.7.4.3.2.2.4" data-id-title="Incompatible Tools" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Incompatible Tools</div><p>
        Not all tools may support named RAID devices. In case a tool expects a
        RAID device to be named <code class="literal">mdN</code>, it will fail to
        identify the devices.
       </p></div></dd></dl></div></section></section><section class="sect1" id="sec-raid-trouble" data-id-title="Troubleshooting Software RAIDs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Troubleshooting Software RAIDs</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-trouble">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Check the <code class="filename">/proc/mdstat</code> file to find out whether a RAID
   partition has been damaged. If a disk fails, shut down your Linux system and
   replace the defective hard disk with a new one partitioned the same way.
   Then restart your system and enter the command <code class="command">mdadm /dev/mdX --add
   /dev/sdX</code>. Replace <code class="literal">X</code> with your particular device
   identifiers. This integrates the hard disk automatically into the RAID
   system and fully reconstructs it (for all RAID levels except for
   RAID 0).
  </p><p>
   Although you can access all data during the rebuild, you might encounter
   some performance issues until the RAID has been fully rebuilt.
  </p><section class="sect2" id="sec-raid-trouble-autorecovery" data-id-title="Recovery after Failing Disk is Back Again"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3.1 </span><span class="title-name">Recovery after Failing Disk is Back Again</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-trouble-autorecovery">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are several reasons a disk included in a RAID array may fail. Here is
    a list of the most common ones:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Problems with the disk media.
     </p></li><li class="listitem"><p>
      Disk drive controller failure.
     </p></li><li class="listitem"><p>
      Broken connection to the disk.
     </p></li></ul></div><p>
    In the case of the disk media or controller failure, the device needs to be
    replaced or repaired. If a hot-spare was not configured within the RAID,
    then manual intervention is required.
   </p><p>
    In the last case, the failed device can be automatically re-added by the
    <code class="command">mdadm</code> command after the connection is repaired (which
    might be automatic).
   </p><p>
    Because <code class="command">md</code>/<code class="command">mdadm</code> cannot reliably
    determine what caused the disk failure, it assumes a serious disk error and
    treats any failed device as faulty until it is explicitly told that the
    device is reliable.
   </p><p>
    Under some circumstances—such as storage devices with the internal
    RAID array— the connection problems are very often the cause of the
    device failure. In such case, you can tell <code class="command">mdadm</code> that it
    is safe to automatically <code class="option">--re-add</code> the device after it
    appears. You can do this by adding the following line to
    <code class="filename">/etc/mdadm.conf</code>:
   </p><div class="verbatim-wrap"><pre class="screen">POLICY action=re-add</pre></div><p>
    Note that the device will be automatically re-added after re-appearing only
    if the <code class="systemitem">udev</code> rules cause <code class="command">mdadm -I
    <em class="replaceable">DISK_DEVICE_NAME</em></code> to be run on any
    device that spontaneously appears (default behavior), and if write-intent
    bitmaps are configured (they are by default).
   </p><p>
    If you want this policy to only apply to some devices and not to the
    others, then the <code class="literal">path=</code> option can be added to the
    <code class="literal">POLICY</code> line in <code class="filename">/etc/mdadm.conf</code> to
    restrict the non-default action to only selected devices. Wild cards can be
    used to identify groups of devices. See <code class="command">man 5 mdadm.conf</code>
    for more information.
   </p></section></section><section class="sect1" id="sec-raid-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="cha-raid.html#sec-raid-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP5/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Configuration instructions and more details for soft RAID can be found in
   the Howtos at:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <em class="citetitle">The Linux RAID wiki</em>:
     <a class="link" href="https://raid.wiki.kernel.org/" target="_blank">https://raid.wiki.kernel.org/</a>
    </p></li><li class="listitem"><p>
     <em class="citetitle">The Software RAID HOWTO</em> in the
     <code class="filename">/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</code>
     file
    </p></li></ul></div><p>
   Linux RAID mailing lists are also available, such as
   <em class="citetitle">linux-raid</em> at
   <a class="link" href="http://marc.info/?l=linux-raid" target="_blank">http://marc.info/?l=linux-raid</a>.
  </p></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="part-software-raid.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part III </span>Software RAID</span></a> </div><div><a class="pagination-link next" href="cha-raidroot.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 8 </span>Configuring Software RAID for the Root Partition</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-raid.html#sec-raid-intro"><span class="title-number">7.1 </span><span class="title-name">Understanding RAID Levels</span></a></span></li><li><span class="sect1"><a href="cha-raid.html#sec-raid-yast"><span class="title-number">7.2 </span><span class="title-name">Soft RAID Configuration with YaST</span></a></span></li><li><span class="sect1"><a href="cha-raid.html#sec-raid-trouble"><span class="title-number">7.3 </span><span class="title-name">Troubleshooting Software RAIDs</span></a></span></li><li><span class="sect1"><a href="cha-raid.html#sec-raid-more"><span class="title-number">7.4 </span><span class="title-name">For More Information</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>