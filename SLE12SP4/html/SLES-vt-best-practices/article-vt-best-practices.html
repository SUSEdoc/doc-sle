<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLES 12 SP4 | Virtualization Best Practices</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Virtualization Best Practices | SLES 12 SP4"/>
<meta name="description" content="Virtualization offers a lot of capabilities to your en…"/>
<meta name="product-name" content="SUSE Linux Enterprise Server"/>
<meta name="product-number" content="12 SP4"/>
<meta name="book-title" content="Virtualization Best Practices"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="fs@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise Server 12 SP4"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Virtualization Best Practices | SLES 12 SP4"/>
<meta property="og:description" content="Virtualization offers a lot of capabilities to your environment. It can be used in multiple scenarios. For more details refe…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Virtualization Best Practices | SLES 12 SP4"/>
<meta name="twitter:description" content="Virtualization offers a lot of capabilities to your environment. It can be used in multiple scenarios. For more details refe…"/>
<link rel="prev" href="art-sles-xen2kvmquick.html" title="Xen to KVM Migration Guide"/><link rel="next" href="bk12apa.html" title="Appendix A. GNU licenses"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Virtualization Best Practices</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Virtualization Best Practices</div> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="article" id="article-vt-best-practices" data-id-title="Virtualization Best Practices"><div class="titlepage"><div><div class="big-version-info"><span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">12 SP4</span></span></span></div><div><div class="title-container"><h1 class="title">Virtualization Best Practices <a title="Permalink" class="permalink" href="article-vt-best-practices.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div><div><h3 class="subtitle"><em><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> <span class="productnumber"><span class="phrase">12 SP4</span></span></em></h3></div><div class="date"><span class="imprint-label">Publication Date: </span>
        February 20, 2024

      </div></div></div><section class="sect1" id="sec-vt-best-scenario" data-id-title="Virtualization Scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Virtualization Scenarios</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-scenario">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Virtualization offers a lot of capabilities to your environment. It can be
   used in multiple scenarios. For more details refer to
   <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization Technology”, Section 1.2 “Virtualization Capabilities”</span> and
   <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization Technology”, Section 1.3 “Virtualization Benefits”</span>.
  </p><p>
   This best practice guide will provide advice for making the right choice in
   your environment. It will recommend or discourage the usage of options
   depending on your workload. Fixing configuration issues and performing
   tuning tasks will increase the performance of VM Guest's near to bare
   metal.
  </p></section><section class="sect1" id="sec-vt-best-intro" data-id-title="Before You Apply Modifications"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Before You Apply Modifications</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-intro-backup" data-id-title="Back Up First"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Back Up First</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-intro-backup">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Changing the configuration of the VM Guest or the VM Host Server can lead to
    data loss or an unstable state. It is really important that you do backups
    of files, data, images, etc. before making any changes. Without backups you
    cannot restore the original state after a data loss or a misconfiguration.
    Do not perform tests or experiments on production systems.
   </p></section><section class="sect2" id="sec-vt-best-intro-testing" data-id-title="Test Your Workloads"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Test Your Workloads</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-intro-testing">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The efficiency of a virtualization environment depends on many factors.
    This guide provides a reference for helping to make good choices when
    configuring virtualization in a production environment. Nothing is
    <span class="emphasis"><em>carved in stone</em></span>. Hardware, workloads, resource
    capacity, etc. should all be considered when planning, testing, and
    deploying your virtualization infra-structure. Testing your virtualized
    workloads is vital to a successful virtualization implementation.
   </p></section></section><section class="sect1" id="sec-vt-best-reco" data-id-title="Recommendations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Recommendations</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-reco">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-intro-libvirt" data-id-title="Prefer the libvirt Framework"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Prefer the <code class="systemitem">libvirt</code> Framework</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-intro-libvirt">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE strongly recommends using the <code class="systemitem">libvirt</code> framework to configure,
    manage, and operate VM Host Servers, containers and VM Guest. It offers a single
    interface (GUI and shell) for all supported virtualization technologies and
    therefore is easier to use than the hypervisor-specific tools.
   </p><p>
    We do not recommend using libvirt and hypervisor-specific tools at the same
    time, because changes done with the hypervisor-specific tools may not be
    recognized by the libvirt tool set. See
    <span class="intraxref">Book “Virtualization Guide”, Chapter 8 “Starting and Stopping <code class="systemitem">libvirtd</code>”</span> for more information on libvirt.
   </p></section><section class="sect2" id="sec-vt-best-intro-qemu" data-id-title="qemu-system-i386 Compared to qemu-system-x86_64"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">qemu-system-i386 Compared to qemu-system-x86_64</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-intro-qemu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Similar to real 64-bit PC hardware, <code class="command">qemu-system-x86_64</code>
    supports VM Guests running a 32-bit or a 64-bit operating system. Because
    <code class="command">qemu-system-x86_64</code> usually also provides better
    performance for 32-bit guests, SUSE generally recommends using
    <code class="command">qemu-system-x86_64</code> for both 32-bit and 64-bit VM Guests
    on KVM. Scenarios where <code class="command">qemu-system-i386</code> is known to
    perform better are not supported by SUSE.
   </p><p>
    Xen also uses binaries from the qemu package but prefers
    <code class="command">qemu-system-i386</code>, which can be used for both 32-bit and
    64-bit Xen VM Guests. To maintain compatibility with the upstream Xen
    Community, SUSE encourages using <code class="command">qemu-system-i386</code> for
    Xen VM Guests.
   </p></section></section><section class="sect1" id="sec-vt-best-hostlevel" data-id-title="VM Host Server Configuration and Resource Allocation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">VM Host Server Configuration and Resource Allocation</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-hostlevel">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Allocation of resources for VM Guests is a crucial point when
   administrating virtual machines. When assigning resources to VM Guests, be
   aware that overcommitting resources may affect the performance of the
   VM Host Server and the VM Guests. If all VM Guests request all their resources
   simultaneously, the host needs to be able to provide all of them. If not,
   the host's performance will be negatively affected and this will in turn
   also have negative effects on the VM Guest's performance.
  </p><section class="sect2" id="sec-vt-best-mem" data-id-title="Memory"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Memory</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-mem">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Linux manages memory in units called pages. On most systems the default
    page size is 4 KB. Linux and the CPU need to know which pages belong to
    which process. That information is stored in a page table. If a lot of
    processes are running, it takes more time to find where the memory is
    mapped, because of the time required to search the page table. To speed up
    the search, the TLB (Translation Lookaside Buffer) was invented. But on a
    system with a lot of memory, the TLB is not enough. To avoid any fallback
    to normal page table (resulting in a cache miss, which is time consuming),
    huge pages can be used. Using huge pages will reduce TLB overhead and TLB
    misses (pagewalk). A host with 32 GB (32*1014*1024 = 33,554,432 KB) of
    memory and a 4 KB page size has a TLB with <span class="emphasis"><em>33,554,432/4 =
    8,388,608</em></span> entries. Using a 2 MB (2048 KB) page size, the TLB
    only has <span class="emphasis"><em>33554432/2048 = 16384</em></span> entries, considerably
    reducing TLB misses.
   </p><section class="sect3" id="sec-vt-best-mem-huge-pages" data-id-title="Configuring the VM Host Server and the VM Guest to use Huge Pages"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.1 </span><span class="title-name">Configuring the VM Host Server and the VM Guest to use Huge Pages</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-mem-huge-pages">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Current CPU architectures support larger pages than 4 KB: huge pages. To
     determine the size of huge pages available on your system (could be 2 MB
     or 1 GB), check the <code class="literal">flags</code> line in the output of
     <code class="filename">/proc/cpuinfo</code> for occurrences of
     <code class="literal">pse</code> and/or <code class="literal">pdpe1gb</code>.
    </p><div class="table" id="id-1.14.4.7.3.3.3" data-id-title="Determine the Available Huge Pages Size"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 1: </span><span class="title-name">Determine the Available Huge Pages Size </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.7.3.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 30%; " class="1"/><col style="width: 70%; " class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          CPU flag
         </p>
        </th><th style="border-bottom: 1px solid ; ">
         <p>
          Huge pages size available
         </p>
        </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Empty string
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          No huge pages available
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          pse
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          2 MB
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          pdpe1gb
         </p>
        </td><td>
         <p>
          1 GB
         </p>
        </td></tr></tbody></table></div></div><p>
     Using huge pages improves performance of VM Guests and reduces host
     memory consumption.
    </p><p>
     By default the system uses THP. To make huge pages available on your
     system, activate it at boot time with <code class="option">hugepages=1</code>,
     and—optionally—add the huge pages size with, for example,
     <code class="option">hugepagesz=2MB</code>.
    </p><div id="id-1.14.4.7.3.3.6" data-id-title="1 GB huge pages" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: 1 GB huge pages</div><p>
      1 GB pages can only be allocated at boot time and cannot be freed
      afterward.
     </p></div><p>
     To allocate and use the huge page table (HugeTlbPage) you need to mount
     <code class="filename">hugetlbfs</code> with correct permissions.
    </p><div id="id-1.14.4.7.3.3.8" data-id-title="Restrictions of Huge Pages" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Restrictions of Huge Pages</div><p>
      Even if huge pages provide the best performance, they do come with some
      drawbacks. You lose features such as Memory ballooning (see
      <a class="xref" href="article-vt-best-practices.html#sec-vt-best-vmguests-virtio-balloon" title="6.1.3. virtio balloon">Section 6.1.3, “virtio balloon”</a>), KSM (see
      <a class="xref" href="article-vt-best-practices.html#sec-vt-best-perf-ksm" title="4.1.4. KSM and Page Sharing">Section 4.1.4, “KSM and Page Sharing”</a>), and huge pages cannot be
      swapped.
     </p></div><div class="procedure" id="id-1.14.4.7.3.3.9" data-id-title="Configuring the use of huge pages"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 2: </span><span class="title-name">Configuring the use of huge pages </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.7.3.3.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Mount <code class="literal">hugetlbfs</code> to
       <code class="filename">/dev/hugepages</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> mount -t hugetlbfs hugetlbfs /dev/hugepages</pre></div></li><li class="step"><p>
       To reserve memory for huge pages use the <code class="command">sysctl</code>
       command. If your system has a huge page size of 2 MB (2048 KB), and you
       want to reserve 1 GB (1,048,576 KB) for your VM Guest, you need
       <span class="emphasis"><em>1,048,576/2048=512</em></span> pages in the pool:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> sysctl vm.nr_hugepages=<em class="replaceable">512</em></pre></div><p>
       The value is written to <code class="filename">/proc/sys/vm/nr_hugepages</code>
       and represents the current number of <span class="emphasis"><em>persistent</em></span>
       huge pages in the kernel's huge page pool.
       <span class="emphasis"><em>Persistent</em></span> huge pages will be returned to the huge
       page pool when freed by a task.
      </p></li><li class="step"><p>
       Add the <code class="literal">memoryBacking</code> element in the VM Guest
       configuration file (by running <code class="command">virsh edit
       <em class="replaceable">CONFIGURATION</em></code>).
      </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</pre></div></li><li class="step"><p>
       Start your VM Guest and check on the host whether it uses hugepages:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cat /proc/meminfo | grep HugePages_
HugePages_Total:<span class="callout" id="co-hp-total">1</span>     512
HugePages_Free:<span class="callout" id="co-hp-free">2</span>       92
HugePages_Rsvd:<span class="callout" id="co-hp-rsvd">3</span>        0
HugePages_Surp:<span class="callout" id="co-hp-surp">4</span>        0</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-total"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Size of the pool of huge pages
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-free"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Number of huge pages in the pool that are not yet allocated
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-rsvd"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Number of huge pages for which a commitment to allocate from the pool
         has been made, but no allocation has yet been made
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-surp"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Number of huge pages in the pool above the value in
         <code class="filename">/proc/sys/vm/nr_hugepages</code>. The maximum number of
         surplus huge pages is controlled by
         <code class="filename">/proc/sys/vm/nr_overcommit_hugepages</code>
        </p></td></tr></table></div></li></ol></div></div></section><section class="sect3" id="sec-vt-best-mem-thp" data-id-title="Transparent Huge Pages"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.2 </span><span class="title-name">Transparent Huge Pages</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-mem-thp">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Transparent huge pages (THP) provide a way to dynamically allocate huge
     pages with the <code class="command">khugepaged</code> kernel thread, rather than
     manually managing their allocation and use. Workloads with contiguous
     memory access patterns can benefit greatly from THP. A 1000 fold decrease
     in page faults can be observed when running synthetic workloads with
     contiguous memory access patterns. Conversely, workloads with sparse
     memory access patterns (like databases) may perform poorly with THP. In
     such cases it may be preferable to disable THP by adding the kernel
     parameter <code class="option">transparent_hugepage=never</code>, rebuild your grub2
     configuration, and reboot. Verify if THP is disabled with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</pre></div><p>
     If disabled, the value <code class="literal">never</code> is shown in square
     brackets like in the example above.
    </p><div id="id-1.14.4.7.3.4.5" data-id-title="Xen" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Xen</div><p>
      THP is not available under Xen.
     </p></div></section><section class="sect3" id="sec-vt-best-mem-xen" data-id-title="Xen-specific Memory Notes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.3 </span><span class="title-name">Xen-specific Memory Notes</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-mem-xen">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect4" id="sec-vt-best-mem-xen-dom-0" data-id-title="Managing Domain-0 Memory"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.1.3.1 </span><span class="title-name">Managing Domain-0 Memory</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-mem-xen-dom-0">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      When using the Xen hypervisor, by default a small percentage of system
      memory is reserved for the hypervisor. All remaining memory is
      automatically allocated to Domain-0. When virtual machines are created,
      memory is ballooned out of Domain-0 to provide memory for the virtual
      machine. This process is called "autoballooning".
     </p><p>
      Autoballooning has several limitations:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Reduced performance while dom0 is ballooning down to free memory for
        the new domain.
       </p></li><li class="listitem"><p>
        Memory freed by ballooning is not confined to a specific NUMA node.
        This can result in performance problems in the new domain because of
        using a non-optimal NUMA configuration.
       </p></li><li class="listitem"><p>
        Failure to start large domains because of delays while ballooning large
        amounts of memory from dom0.
       </p></li></ul></div><p>
      For these reasons, we strongly recommend to disable autoballooning and
      give Domain-0 the memory needed for its workload. Determining Domain-0
      memory and vCPU sizing should follow a similar process as any other
      virtual machine.
     </p><p>
      Autoballooning is controlled by the tool stack used to manage your Xen
      installation. For the xl/libxl tool stack, autoballooning is controlled
      by the <code class="option">autoballoon</code> setting in
      <code class="filename">/etc/xen/xl.conf</code>. For the libvirt+libxl tool stack,
      autoballooning is controlled by the <code class="option">autoballoon</code> setting
      in <code class="filename">/etc/libvirt/libxl.conf</code>.
     </p><p>
      The amount of memory initially allocated to Domain-0 is controlled by the
      Xen hypervisor dom0_mem parameter. For example, to set the initial
      memory allocation of Domain-0 to 8GB, add <code class="option">dom0_mem=8G</code> to
      the Xen hypervisor parameters. The dom0_mem parameter can also be used
      to specify the minimum and maximum memory allocations for Domain-0. For
      example, to set the initial memory of Domain-0 to 8GB, but allow it to be
      changed (ballooned) anywhere between 4GB and 16GB, add the following to
      the Xen hypervisor parameters:
      <code class="option">dom0_mem=8G,min:4G,max:8G</code>.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        To set dom0_mem on SLE 11 products, modify
        <code class="filename">/boot/grub/menu.lst</code>, adding
        <code class="option">dom0_mem=XX</code> to the Xen hypervisor (xen.gz)
        parameters. The change will be applied at next reboot.
       </p></li><li class="listitem"><p>
        To set dom0_mem on SLE 12 products, modify
        <code class="filename">/etc/default/grub</code>, adding
        <code class="option">dom0_mem=XX</code> to
        <code class="option">GRUB_CMDLINE_XEN_DEFAULT</code>. See
        <a class="xref" href="article-vt-best-practices.html#sec-vt-best-kernel-parameter" title="7.5. Change Kernel Parameters at Boot Time">Section 7.5, “Change Kernel Parameters at Boot Time”</a> for more
        information.
       </p></li></ul></div><p>
      Autoballooning is enabled by default since it is extremely difficult to
      determine a predefined amount of memory required by Domain-0. Memory
      needed by Domain-0 is heavily dependent on the number of hosted virtual
      machines and their configuration. Users must ensure Domain-0 has
      sufficient memory resources to accommodate virtual machine workloads.
     </p></section><section class="sect4" id="sec-vt-best-mem-xen-tmpfs" data-id-title="xenstore in tmpfs"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.1.3.2 </span><span class="title-name">xenstore in <code class="systemitem">tmpfs</code></span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-mem-xen-tmpfs">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      When using Xen, we recommend to place the xenstore database on
      <code class="systemitem">tmpfs</code>. xenstore is used as a control plane by
      the xm/xend and xl/libxl tool stacks and the front-end and back-end
      drivers servicing domain I/O devices. The load on xenstore increases
      linearly as the number of running domains increase. If you anticipate
      hosting many VM Guest on a Xen host, move the xenstore database onto
      tmpfs to improve overall performance of the control plane. Mount the
      <code class="filename">/var/lib/xenstored</code> directory on tmpfs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> mount -t tmpfs tmpfs /var/lib/xenstored/</pre></div></section></section><section class="sect3" id="sec-vt-best-perf-ksm" data-id-title="KSM and Page Sharing"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.4 </span><span class="title-name">KSM and Page Sharing</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-ksm">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Kernel Samepage Merging is a kernel feature that allows for lesser memory
     consumption on the VM Host Server by sharing data VM Guests have in common. The
     KSM daemon <code class="systemitem">ksmd</code> periodically scans
     user memory looking for pages of identical content which can be replaced
     by a single write-protected page. To enable KSM, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> echo 1 &gt; /sys/kernel/mm/ksm/run</pre></div><p>
     One advantage of using KSM from a VM Guest's perspective is that all
     guest memory is backed by host anonymous memory. You can share
     <span class="emphasis"><em>pagecache</em></span>, <span class="emphasis"><em>tmpfs</em></span> or any kind of
     memory allocated in the guest.
    </p><p>
     KSM is controlled by <code class="systemitem">sysfs</code>. You can check KSM's
     values in <code class="filename">/sys/kernel/mm/ksm/</code>:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">pages_shared</code>: The number of shared pages that are
       being used (read-only).
      </p></li><li class="listitem"><p>
       <code class="literal">pages_sharing</code>: The number of sites sharing the pages
       (read-only).
      </p></li><li class="listitem"><p>
       <code class="literal">pages_unshared</code>: The number of pages that are unique
       and repeatedly checked for merging (read-only).
      </p></li><li class="listitem"><p>
       <code class="literal">pages_volatile</code>: The number of pages that are changing
       too fast to be considered for merging (read-only).
      </p></li><li class="listitem"><p>
       <code class="literal">full_scans</code>: The number of times all mergeable areas
       have been scanned (read-only).
      </p></li><li class="listitem"><p>
       <code class="literal">sleep_millisecs</code>: The number of milliseconds
       <code class="systemitem">ksmd</code> should sleep before the
       next scan. A low value will overuse the CPU, consuming CPU time that
       could be used for other tasks. We recommend a value greater than
       <code class="literal">1000</code>.
      </p></li><li class="listitem"><p>
       <code class="literal">pages_to_scan</code>: The number of present pages to scan
       before ksmd goes to sleep. A high value will overuse the CPU. We
       recommend to start with a value of <code class="literal">1000</code>, and then
       adjust as necessary based on the KSM results observed while testing your
       deployment.
      </p></li><li class="listitem"><p>
       <code class="literal">merge_across_nodes</code>: By default the system merges
       pages across NUMA nodes. Set this option to <code class="literal">0</code> to
       disable this behavior.
      </p></li></ul></div><div id="id-1.14.4.7.3.6.7" data-id-title="Use Cases" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Use Cases</div><p>
      KSM is a good technique to over-commit host memory when running multiple
      instances of the same application or VM Guest. When applications and
      VM Guest are heterogeneous and do not share any common data, it is
      preferable to disable KSM. In a mixed heterogeneous and homogeneous
      environment, KSM can be enabled on the host but disabled on a per
      VM Guest basis. Use <code class="command">virsh edit</code> to disable page
      sharing of a VM Guest by adding the following to the guest's XML
      configuration:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</pre></div></div><div id="id-1.14.4.7.3.6.8" data-id-title="Avoid Out-of-Memory Conditions" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Avoid Out-of-Memory Conditions</div><p>
      KSM can free up some memory on the host system, but the administrator
      should reserve enough swap to avoid out-of-memory conditions if that
      shareable memory decreases. If the amount of shareable memory decreases,
      the use of physical memory is increased.
     </p></div><div id="id-1.14.4.7.3.6.9" data-id-title="Memory Access Latencies" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Memory Access Latencies</div><p>
      By default, KSM will merge common pages across NUMA nodes. If the merged,
      common page is now located on a distant NUMA node (relative to the node
      running the VM Guest vCPUs), this may degrade VM Guest performance. If
      increased memory access latencies are noticed in the VM Guest, disable
      cross-node merging with the <code class="literal">merge_across_nodes</code> sysfs
      control:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> echo 0 &gt; /sys/kernel/mm/ksm/merge_across_nodes</pre></div></div></section><section class="sect3" id="sec-vt-best-mem-hot" data-id-title="VM Guest: Memory Hotplug"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.5 </span><span class="title-name">VM Guest: Memory Hotplug</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-mem-hot">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To optimize the usage of your host memory, it may be useful to hotplug
     more memory for a running VM Guest when required. To support memory
     hotplugging, you must first configure the
     <code class="literal">&lt;maxMemory&gt;</code> tag in the VM Guest's configuration
     file:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;maxMemory<span class="callout" id="co-mem-hot-max">1</span> slots='16'<span class="callout" id="co-mem-hot-slots">2</span> unit='KiB'&gt;20971520<span class="callout" id="co-mem-hot-size">3</span>&lt;/maxMemory&gt;
  &lt;memory<span class="callout" id="co-mem-hot-mem">4</span> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<span class="callout" id="co-mem-hot-curr">5</span> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-max"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Runtime maximum memory allocation of the guest.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-slots"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Number of slots available for adding memory to the guest
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-size"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Valid units are:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         "KB" for kilobytes (1,000 bytes)
        </p></li><li class="listitem"><p>
         "k" or "KiB" for kibibytes (1,024 bytes)
        </p></li><li class="listitem"><p>
         "MB" for megabytes (1,000,000 bytes)
        </p></li><li class="listitem"><p>
         "M" or "MiB" for mebibytes (1,048,576 bytes)
        </p></li><li class="listitem"><p>
         "GB" for gigabytes (1,000,000,000 bytes)
        </p></li><li class="listitem"><p>
         "G" or "GiB" for gibibytes (1,073,741,824 bytes)
        </p></li><li class="listitem"><p>
         "TB" for terabytes (1,000,000,000,000 bytes)
        </p></li><li class="listitem"><p>
         "T" or "TiB" for tebibytes (1,099,511,627,776 bytes)
        </p></li></ul></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-mem"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Maximum allocation of memory for the guest at boot time
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-curr"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Actual allocation of memory for the guest
      </p></td></tr></table></div><p>
     To hotplug memory devices into the slots, create a file
     <code class="filename">mem-dev.xml</code> like the following:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'&gt;524287&lt;/size&gt;
  &lt;node&gt;0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</pre></div><p>
     And attach it with the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>virsh attach-device vm-name mem-dev.xml</pre></div><p>
     For memory device hotplug, the guest must have at least 1 NUMA cell
     defined (see <a class="xref" href="article-vt-best-practices.html#sec-vt-best-perf-numa-vmguest-topo" title="4.6.3.1. VM Guest Virtual NUMA Topology">Section 4.6.3.1, “VM Guest Virtual NUMA Topology”</a>).
    </p></section></section><section class="sect2" id="sec-vt-best-perf-swap" data-id-title="Swap"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Swap</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-swap">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <span class="emphasis"><em>Swap</em></span> is usually used by the system to store underused
    physical memory (low usage, or not accessed for a long time). To prevent
    the system running out of memory, setting up a minimum swap is highly
    recommended.
   </p><section class="sect3" id="sec-vt-best-perf-swap-swappiness" data-id-title="swappiness"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.2.1 </span><span class="title-name"><code class="literal">swappiness</code></span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-swap-swappiness">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The <code class="literal">swappiness</code> setting controls your system's swap
     behavior. It defines how memory pages are swapped to disk. A high value of
     <span class="emphasis"><em>swappiness</em></span> results in a system that swaps more often.
     Available values range from <code class="literal">0</code> to
     <code class="literal">100</code>. A value of <code class="literal">100</code> tells the system
     to find inactive pages and put them in swap. A value of <code class="option">0</code>
     disables swapping.

    </p><p>
     To do some testing on a live system, change the value of
     <code class="filename">/proc/sys/vm/swappiness</code> on the fly and check the
     memory usage afterward:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> echo 35 &gt; /proc/sys/vm/swappiness</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>free -h
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</pre></div><p>
     To permanently set a swappiness value, add a line in
     <code class="filename">/etc/systcl.conf</code>, for example:
    </p><div class="verbatim-wrap"><pre class="screen">vm.swappiness = 35</pre></div><p>
     You can also control the swap by using the
     <code class="literal">swap_hard_limit</code> element in the XML configuration of
     your VM Guest. Before setting this parameter and using it in a production
     environment, do some testing because the host can terminate the domain if
     the value is too low.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;memtune&gt;<span class="callout" id="co-mem-1">1</span>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<span class="callout" id="co-mem-hard">2</span>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<span class="callout" id="co-mem-soft">3</span>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<span class="callout" id="co-mem-swap">4</span>
&lt;/memtune&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-1"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This element provides memory tunable parameters for the domain. If this
       is omitted, it defaults to the defaults provided b the operating system.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hard"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Maximum memory the guest can use. To avoid any problems on the VM Guest
       it is strongly recommended not to use this parameter.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-soft"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The memory limit to enforce during memory contention.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-swap"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The maximum memory plus swap the VM Guest can use.
      </p></td></tr></table></div></section></section><section class="sect2" id="sec-vt-best-io" data-id-title="I/O"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">I/O</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-io">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect3" id="sec-vt-best-perf-io" data-id-title="I/O Scheduler"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.1 </span><span class="title-name">I/O Scheduler</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-io">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The default I/O scheduler is Completely Fair Queuing (CFQ). The main aim
     of the CFQ scheduler is to provide a fair allocation of the disk I/O
     bandwidth for all processes that request an I/O operation. You can have
     different I/O schedulers for different devices.
    </p><p>
     To get better performance in host and VM Guest, use
     <code class="literal">noop</code> in the VM Guest (disable the I/O scheduler) and
     the <code class="literal">deadline</code> scheduler for a virtualization host.
    </p><div class="procedure" id="id-1.14.4.7.5.3.4" data-id-title="Checking and Changing the I/O Scheduler at Runtime"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3: </span><span class="title-name">Checking and Changing the I/O Scheduler at Runtime </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.7.5.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To check your current I/O scheduler for your disk (replace
       <em class="replaceable">sdX</em> by the disk you want to check), run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cat /sys/block/<em class="replaceable">sdX</em>/queue/scheduler
noop deadline [cfq]</pre></div><p>
       The value in square brackets is the one currently selected
       (<code class="literal">cfq</code> in the example above).
      </p></li><li class="step"><p>
       You can change the scheduler at runtime with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> echo deadline &gt; /sys/block/<em class="replaceable">sdX</em>/queue/scheduler</pre></div></li></ol></div></div><p>
     To permanently set an I/O scheduler for all disks of a system, use the
     kernel parameter <code class="literal">elevator</code>. The respective values are
     <code class="option">elevator=deadline</code> for the VM Host Server and
     <code class="option">elevator=noop</code> for VM Guests. See
     <a class="xref" href="article-vt-best-practices.html#sec-vt-best-kernel-parameter" title="7.5. Change Kernel Parameters at Boot Time">Section 7.5, “Change Kernel Parameters at Boot Time”</a> for further
     instructions.
    </p><p>
     If you need to specify different I/O schedulers for each disk, create the
     file <code class="filename">/usr/lib/tmpfiles.d/IO_ioscheduler.conf</code> with
     content similar to the following example. It defines the
     <code class="literal">deadline</code> scheduler for <code class="filename">/dev/sda</code>
     and the <code class="literal">noop</code> scheduler for
     <code class="filename">/dev/sdb</code>. This feature is available on SLE 12
     only.
    </p><div class="verbatim-wrap"><pre class="screen">w /sys/block/sda/queue/scheduler - - - - deadline
w /sys/block/sdb/queue/scheduler - - - - noop</pre></div></section><section class="sect3" id="sec-vt-best-io-async" data-id-title="Asynchronous I/O"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.2 </span><span class="title-name">Asynchronous I/O</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-io-async">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Many of the virtual disk back-ends use Linux Asynchronous I/O (aio) in
     their implementation. By default, the maximum number of aio contexts is
     set to 65536, which can be exceeded when running hundreds of VM Guests
     using virtual disks serviced by Linux Asynchronous I/O. When running large
     numbers of VM Guests on a VM Host Server, consider increasing
     /proc/sys/fs/aio-max-nr.
    </p><div class="procedure" id="id-1.14.4.7.5.4.3" data-id-title="Checking and Changing aio-max-nr at Runtime"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4: </span><span class="title-name">Checking and Changing aio-max-nr at Runtime </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.7.5.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To check your current aio-max-nr setting run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cat /proc/sys/fs/aio-max-nr
65536</pre></div></li><li class="step"><p>
       You can change aio-max-nr at runtime with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> echo 131072 &gt; /proc/sys/fs/aio-max-nr</pre></div></li></ol></div></div><p>
     To permanently set aio-max-nr, add an entry to a local sysctl file. For
     example, append the following to
     <code class="filename">/etc/sysctl.d/99-sysctl.conf</code>:
    </p><div class="verbatim-wrap"><pre class="screen">fs.aio-max-nr = 1048576</pre></div></section><section class="sect3" id="sec-vt-best-io-techniques" data-id-title="I/O Virtualization"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.3 </span><span class="title-name">I/O Virtualization</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-io-techniques">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     SUSE products support various I/O virtualization technologies. The
     following table lists advantages and disadvantages of each technology. For
     more information about I/O in virtualization refer to
     <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization Technology”, Section 1.5 “I/O Virtualization”</span>.
    </p><div class="table" id="id-1.14.4.7.5.5.3" data-id-title="I/O Virtualization Solutions"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2: </span><span class="title-name">I/O Virtualization Solutions </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.7.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 20%; " class="1"/><col style="width: 40%; " class="2"/><col style="width: 40%; " class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Technology
         </p>
        </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Advantage
         </p>
        </th><th style="border-bottom: 1px solid ; ">
         <p>
          Disadvantage
         </p>
        </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="4">
         <p>
          Device Assignment (pass-through)
         </p>
        </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Device accessed directly by the guest
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          No sharing among multiple guests
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          High performance
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Live migration is complex
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          PCI device limit is 8 per guest
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Limited number of slots on a server
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
         <p>
          Full virtualization (IDE, SATA, SCSI, e1000)
         </p>
        </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          VM Guest compatibility
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Bad performance
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Easy for live migration
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Emulated operation
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; " rowspan="3">
         <p>
          Para-virtualization (virtio-blk, virtio-net, virtio-scsi)
         </p>
        </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Good performance
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Modified guest (PV drivers)
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Easy for live migration
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Efficient host communication with VM Guest
         </p>
        </td><td>
         
        </td></tr></tbody></table></div></div></section></section><section class="sect2" id="sec-vt-best-fs" data-id-title="Storage and File System"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">Storage and File System</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-fs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Storage space for VM Guests can either be a block device (for example, a
    partition on a physical disk), or an image file on the file system:
   </p><div class="table" id="id-1.14.4.7.6.3" data-id-title="Block Devices Compared to Disk Images"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 3: </span><span class="title-name">Block Devices Compared to Disk Images </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.7.6.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 10%; " class="1"/><col style="width: 45%; " class="2"/><col style="width: 45%; " class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Technology
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Advantages
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Disadvantages
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Block devices
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Better performance
          </p></li><li class="listitem"><p>
           Use standard tools for administration/disk modification
          </p></li><li class="listitem"><p>
           Accessible from host (pro and con)
          </p></li></ul></div>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Device management
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Image files
        </p>
       </td><td style="border-right: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Easier system management
          </p></li><li class="listitem"><p>
           Easily move, clone, expand, back up domains
          </p></li><li class="listitem"><p>
           Comprehensive toolkit (guestfs) for image manipulation
          </p></li><li class="listitem"><p>
           Reduce overhead through sparse files
          </p></li><li class="listitem"><p>
           Fully allocate for best performance
          </p></li></ul></div>
       </td><td>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Lower performance than block devices
          </p></li></ul></div>
       </td></tr></tbody></table></div></div><p>
    For detailed information about image formats and maintaining images refer
    to <a class="xref" href="article-vt-best-practices.html#sec-vt-best-img" title="5. VM Guest Images">Section 5, “VM Guest Images”</a>.
   </p><p>
    If your image is stored on an NFS share, you should check some server and
    client parameters to improve access to the VM Guest image.
   </p><section class="sect3" id="sec-vt-best-fs-nfs-rw" data-id-title="NFS Read/Write (Client)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.1 </span><span class="title-name">NFS Read/Write (Client)</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-fs-nfs-rw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Options <code class="option">rsize</code> and <code class="option">wsize</code> specify the size
     of the chunks of data that the client and server pass back and forth to
     each other. You should ensure NFS read/write sizes are sufficiently large,
     especially for large I/O. Change the <code class="option">rsize</code> and
     <code class="option">wsize</code> parameter in your <code class="filename">/etc/fstab</code>
     by increasing the value to 16 KB. This will ensure that all operations can
     be frozen if there is any instance of hanging.
    </p><div class="verbatim-wrap"><pre class="screen">nfs_server:/exported/vm_images<span class="callout" id="co-nfs-server">1</span> /mnt/images<span class="callout" id="co-nfs-mnt">2</span> nfs<span class="callout" id="co-nfs-nfs">3</span> rw<span class="callout" id="co-nfs-rw">4</span>,hard<span class="callout" id="co-nfs-hard">5</span>,sync<span class="callout" id="co-nfs-sync">6</span>, rsize=8192<span class="callout" id="co-nfs-rsize">7</span>,wsize=8192<span class="callout" id="co-nfs-wsize">8</span> 0 0</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-server"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       NFS server's host name and export path name.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-mnt"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Where to mount the NFS exported share.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-nfs"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This is an <code class="option">nfs</code> mount point.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-rw"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This mount point will be accessible in read/write.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-hard"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Determines the recovery behavior of the NFS client after an NFS request
       times out. <code class="option">hard</code> is the best option to avoid data
       corruption.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-sync"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Any system call that writes data to files on that mount point causes
       that data to be flushed to the server before the system call returns
       control to user space.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-rsize"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Maximum number of bytes in each network READ request that the NFS client
       can receive when reading data from a file on an NFS server.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-wsize"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Maximum number of bytes per network WRITE request that the NFS client
       can send when writing data to a file on an NFS server.
      </p></td></tr></table></div></section><section class="sect3" id="sec-vt-best-fs-nfs-threads" data-id-title="NFS Threads (Server)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.2 </span><span class="title-name">NFS Threads (Server)</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-fs-nfs-threads">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Your NFS server should have enough NFS threads to handle multi-threaded
     workloads. Use the <code class="command">nfsstat</code> tool to get some RPC
     statistics on your server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</pre></div><p>
     If the <code class="literal">retrans</code> is equal to 0, everything is fine.
     Otherwise, the client needs to retransmit, so increase the
     <code class="envar">USE_KERNEL_NFSD_NUMBER</code> variable in
     <code class="filename">/etc/sysconfig/nfs</code>, and adjust accordingly until
     <code class="literal">retrans</code> is equal to <code class="literal">0</code>.
    </p></section></section><section class="sect2" id="sec-vt-best-perf-cpu" data-id-title="CPUs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.5 </span><span class="title-name">CPUs</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-cpu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Host CPU <span class="quote">“<span class="quote">components</span>”</span> will be <span class="quote">“<span class="quote">translated</span>”</span> to
    virtual CPUs in a VM Guest when being assigned. These components can
    either be:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>CPU processor</em></span>: this describes the main CPU unit,
      which usually has multiple cores and may support Hyper-Threading.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>CPU core</em></span>: a main CPU unit can provide more than one
      core, and the proximity of cores speeds up the computation process and
      reduces energy costs.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>CPU Hyper-Threading</em></span>: this implementation is used to
      improve parallelization of computations, but this is not as efficient as
      a dedicated core.
     </p></li></ul></div><section class="sect3" id="sec-vt-best-perf-cpu-assign" data-id-title="Assigning CPUs"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.5.1 </span><span class="title-name">Assigning CPUs</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-cpu-assign">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     CPU overcommit occurs when the cumulative number of virtual CPUs of
     all VM Guests becomes higher than the number of host CPUs.
     Best performance is likely to be achieved when there is no overcommit
     and each virtual CPU matches one hardware processor or core on the
     VM Host Server.
     In fact, VM Guests running on an overcommitted host will experience
     increased latency, and a negative effect on per-VM Guest throughput
     is also likely to be observed. Therefore, you should try to avoid
     overcommitting CPUs.
    </p><p>
     Deciding whether to allow CPU overcommit or not requires good a-priori
     knowledge of the workload as a whole.
     For instance, if you know that all the VM Guest's virtual CPUs will not
     be loaded more than 50%, then you can assume that overcommitting the host
     by a factor of 2 (which means having 128 virtual CPUs in total, on a host
     with 64 CPUs) will work well.
     On the other hand, if you know that all the virtual CPUs of the VM Guest
     will try to run at 100% for most of the time then even having one virtual
     CPU more than the host has CPUs is already a misconfiguration.
    </p><p>
     Overcommitting to a point where the cumulative number of virtual CPUs is
     higher than 8 times the number of physical cores of the VM Host Server will most
     likely lead to a malfunctioning and unstable system and should hence be
     avoided.
    </p><p>
     Unless you know exactly how many virtual CPUs are required for a VM Guest,
     you should start with one. A good rule of thumb is to target a CPU workload
     of approximately 70% inside your VM (see
     <span class="intraxref">Book “System Analysis and Tuning Guide”, Chapter 2 “System Monitoring Utilities”, Section 2.3 “Processes”</span> for information on monitoring tools).
     If you allocate more processors than needed in the VM Guest, this will
     negatively affect the performance of host and guest. Cycle efficiency will
     be degraded, as the unused vCPU will still cause timer interrupts.
     In case you primarily run single threaded applications on a VM Guest, a
     single virtual CPU is the best choice.
    </p><p>
      A single VM Guest with more virtual CPUs than the VM Host Server has CPUs
      is always a misconfiguration.
    </p></section><section class="sect3" id="sec-vt-best-perf-cpu-guests" data-id-title="VM Guest CPU Configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.5.2 </span><span class="title-name">VM Guest CPU Configuration</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-cpu-guests">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This section describes how to choose and configure a CPU type for a
     VM Guest. You will also learn how to pin virtual CPUs to physical CPUs on
     the host system. For more information about virtual CPU configuration and
     tuning parameters refer to the libvirt documentation at
     <a class="link" href="https://libvirt.org/formatdomain.html#elementsCPU" target="_blank">https://libvirt.org/formatdomain.html#elementsCPU</a>.
    </p><section class="sect4" id="sec-vt-best-perf-cpu-guests-model" data-id-title="Virtual CPU Models and Features"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.1 </span><span class="title-name">Virtual CPU Models and Features</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-cpu-guests-model">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      The CPU model and topology can be specified individually for each
      VM Guest. Configuration options range from selecting specific CPU models
      to excluding certain CPU features. Predefined CPU models are listed in
      the <code class="filename">/usr/share/libvirt/cpu_map.xml</code>. A CPU model and
      topology that is similar to the host generally provides the best
      performance. The host system CPU model and topology can be displayed by
      running <code class="command">virsh capabilities</code>.
     </p><p>
      Note that changing the default virtual CPU configuration will require a
      VM Guest shutdown when migrating it to a host with different hardware.
      More information on VM Guest migration is available at
      <span class="intraxref">Book “Virtualization Guide”, Chapter 10 “Basic VM Guest Management”, Section 10.7 “Migrating VM Guests”</span>.
     </p><p>
      To specify a particular CPU model for a VM Guest, add a respective entry
      to the VM Guest configuration file. The following example configures a
      Broadwell CPU with the invariant TSC feature:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
  &lt;feature name='invtsc'/&gt;
  &lt;/cpu&gt;</pre></div><p>
      For a virtual CPU that most closely resembles the host physical CPU,
      <code class="literal">&lt;cpu mode='host-passthrough'&gt;</code> can be used. Note
      that a <code class="literal">host-passthrough</code> CPU model may not exactly
      resemble the host physical CPU, since by default KVM will mask any
      non-migratable features. For example invtsc is not included in the
      virtual CPU feature set. Changing the default KVM behavior is not
      directly supported through libvirt, although it does allow arbitrary
      passthrough of KVM command line arguments. Continuing with the
      <code class="literal">invtsc</code> example, you can achieve passthrough of the
      host CPU (including <code class="literal">invtsc</code>) with the following command
      line passthrough in the VM Guest configuration file:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
     &lt;qemu:commandline&gt;
     &lt;qemu:arg value='-cpu'/&gt;
     &lt;qemu:arg value='host,migratable=off,+invtsc'/&gt;
     &lt;/qemu:commandline&gt;
     ...
     &lt;/domain&gt;</pre></div><div id="id-1.14.4.7.7.5.3.8" data-id-title="The host-passthrough Mode" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: The <code class="literal">host-passthrough</code> Mode</div><p>
       Since <code class="literal">host-passthrough</code> exposes the physical CPU
       details to the virtual CPU, migration to dissimilar hardware is not
       possible. See
       <a class="xref" href="article-vt-best-practices.html#sec-vt-best-perf-cpu-guests-vcpumigration" title="4.5.2.3. Virtual CPU Migration Considerations">Section 4.5.2.3, “Virtual CPU Migration Considerations”</a> for more
       information.
      </p></div></section><section class="sect4" id="sec-vt-best-perf-cpu-guests-vcpupin" data-id-title="Virtual CPU Pinning"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.2 </span><span class="title-name">Virtual CPU Pinning</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-cpu-guests-vcpupin">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Virtual CPU pinning is used to constrain virtual CPU threads to a set of
      physical CPUs. The <code class="literal">vcpupin</code> element specifies the
      physical host CPUs that a virtual CPU can use. If this element is not set
      and the attribute <code class="literal">cpuset</code> of the
      <code class="literal">vcpu</code> element is not specified, the virtual CPU is free
      to use any of the physical CPUs.
     </p><p>
      CPU intensive workloads can benefit from virtual CPU pinning by
      increasing the physical CPU cache hit ratio. To pin a virtual CPU to a
      specific physical CPU, run the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>virsh vcpupin <em class="replaceable">DOMAIN_ID</em> --vcpu <em class="replaceable">vCPU_NUMBER</em>
VCPU: CPU Affinity
----------------------------------
0: 0-7
<code class="prompt root">root # </code>virsh vcpupin SLE12 --vcpu 0 0 --config</pre></div><p>
      The last command generates the following entry in the XML configuration:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</pre></div><div id="id-1.14.4.7.7.5.4.7" data-id-title="Virtual CPU Pinning on NUMA Nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Virtual CPU Pinning on NUMA Nodes</div><p>
       To confine a VM Guest's CPUs and its memory to a NUMA node, you can use
       virtual CPU pinning and memory allocation policies on a NUMA system. See
       <a class="xref" href="article-vt-best-practices.html#sec-vt-best-perf-numa" title="4.6. NUMA Tuning">Section 4.6, “NUMA Tuning”</a> for more information related to
       NUMA tuning.
      </p></div><div id="id-1.14.4.7.7.5.4.8" data-id-title="Virtual CPU Pinning and Live Migration" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Virtual CPU Pinning and Live Migration</div><p>
       Even though <code class="literal">vcpupin</code> can improve performance, it can
       complicate live migration. See
       <a class="xref" href="article-vt-best-practices.html#sec-vt-best-perf-cpu-guests-vcpumigration" title="4.5.2.3. Virtual CPU Migration Considerations">Section 4.5.2.3, “Virtual CPU Migration Considerations”</a> for more
       information on virtual CPU migration considerations.
      </p></div></section><section class="sect4" id="sec-vt-best-perf-cpu-guests-vcpumigration" data-id-title="Virtual CPU Migration Considerations"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.3 </span><span class="title-name">Virtual CPU Migration Considerations</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-cpu-guests-vcpumigration">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Selecting a virtual CPU model containing all the latest features may
      improve performance of a VM Guest workload, but often at the expense of
      migratability. Unless all hosts in the cluster contain the latest CPU
      features, migration can fail when a destination host lacks the new
      features. If migratability of a virtual CPU is preferred over the latest
      CPU features, a normalized CPU model and feature set should be used. The
      <code class="command">virsh cpu-baseline</code> command can help define a
      normalized virtual CPU that can be migrated across all hosts. The
      following command, when run on each host in the migration cluster,
      illustrates collection of all hosts' CPU capabilities in
      <code class="literal">all-hosts-cpu-caps.xml</code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> virsh capabilities | virsh cpu-baseline /dev/stdin &gt;&gt; all-hosts-cpu-caps.xml</pre></div><p>
      With the CPU capabilities from each host collected in
      all-hosts-cpu-caps.xml, use <code class="command">virsh cpu-baseline</code> to
      create a virtual CPU definition that will be compatible across all hosts.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> virsh cpu-baseline all-hosts-cpu-caps.xml</pre></div><p>
      The resulting virtual CPU definition can be used as the
      <code class="literal">cpu</code> element in VM Guest configuration file.
     </p><p>
      At a logical level, virtual CPU pinning is a form of hardware
      passthrough. Pinning couples physical resources to virtual resources, and
      can also be problematic for migration. For example, the migration will
      fail if the requested physical resources are not available on the
      destination host, or if the source and destination hosts have different
      NUMA topologies. For more recommendations about Live Migration see
      <span class="intraxref">Book “Virtualization Guide”, Chapter 10 “Basic VM Guest Management”, Section 10.7.1 “Migration Requirements”</span>.
     </p></section></section></section><section class="sect2" id="sec-vt-best-perf-numa" data-id-title="NUMA Tuning"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.6 </span><span class="title-name">NUMA Tuning</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-numa">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    NUMA is an acronym for Non Uniform Memory Access. A NUMA system has
    multiple physical CPUs, each with local memory attached. Each CPU can also
    access other CPUs' memory, known as <span class="quote">“<span class="quote">remote memory access</span>”</span>,
    but it is much slower than accessing local memory. NUMA systems can
    negatively impact VM Guest performance if not tuned properly. Although
    ultimately tuning is workload dependent, this section describes controls
    that should be considered when deploying VM Guests on NUMA hosts. Always
    consider your host topology when configuring and deploying VMs.
   </p><p>
    <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> contains a NUMA auto-balancer that strives to reduce remote
    memory access by placing memory on the same NUMA node as the CPU processing
    it. In addition, standard tools such as <code class="command">cgset</code> and
    virtualization tools such as libvirt provide mechanisms to constrain
    VM Guest resources to physical resources.
   </p><p>
    <code class="command">numactl</code> is used to check for host NUMA capabilities:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3
0:  10  21  21  21
1:  21  10  21  21
2:  21  21  10  21
3:  21  21  21  10</pre></div><p>
    The <code class="command">numactl</code> output shows this is a NUMA system with 4
    nodes or cells, each containing 36 CPUs and approximately 32G memory.
    <code class="command">virsh capabilities</code> can also be used to examine the
    systems NUMA capabilities and CPU topology.
   </p><section class="sect3" id="sec-vt-best-perf-numa-balancing" data-id-title="NUMA Balancing"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.1 </span><span class="title-name">NUMA Balancing</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-numa-balancing">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     On NUMA machines, there is a performance penalty if remote memory is
     accessed by a CPU. Automatic NUMA balancing scans a task's address space
     and unmaps pages. By doing so, it detects whether pages are properly
     placed or whether to migrate the data to a memory node local to where the
     task is running. In defined intervals (configured with
     <code class="literal">numa_balancing_scan_delay_ms</code>), the task scans the next
     scan size number of pages (configured with
     <code class="literal">numa_balancing_scan_size_mb</code>) in its address space. When
     the end of the address space is reached the scanner restarts from the
     beginning.
    </p><p>
     Higher scan rates cause higher system overhead as page faults must be
     trapped and data needs to be migrated. However, the higher the scan rate,
     the more quickly a task's memory is migrated to a local node when the
     workload pattern changes. This minimizes the performance impact caused by
     remote memory accesses. These <code class="command">sysctl</code> directives control
     the thresholds for scan delays and the number of pages scanned:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<span class="callout" id="co-numa-balancing">1</span>
kernel.numa_balancing_scan_delay_ms = 1000<span class="callout" id="co-numa-delay">2</span>
kernel.numa_balancing_scan_period_max_ms = 60000<span class="callout" id="co-numa-pmax">3</span>
kernel.numa_balancing_scan_period_min_ms = 1000<span class="callout" id="co-numa-pmin">4</span>
kernel.numa_balancing_scan_size_mb = 256<span class="callout" id="co-numa-size">5</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-balancing"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Enables/disables automatic page fault-based NUMA balancing
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-delay"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Starting scan delay used for a task when it initially forks
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-pmax"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Maximum time in milliseconds to scan a task's virtual memory
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-pmin"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minimum time in milliseconds to scan a task's virtual memory
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-size"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Size in megabytes' worth of pages to be scanned for a given scan
      </p></td></tr></table></div><p>
     For more information see <span class="intraxref">Book “System Analysis and Tuning Guide”, Chapter 10 “Automatic Non-Uniform Memory Access (NUMA) Balancing”</span>.
    </p><p>
     The main goal of automatic NUMA balancing is either to reschedule tasks on
     the same node's memory (so the CPU follows the memory), or to copy the
     memory's pages to the same node (so the memory follows the CPU).
    </p><div id="id-1.14.4.7.8.7.8" data-id-title="Task Placement" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Task Placement</div><p>
      There are no rules to define the best place to run a task, because tasks
      could share memory with other tasks. For best performance, it is
      recommended to group tasks sharing memory on the same node. Check NUMA
      statistics with <code class="command"># cat /proc/vmstat | grep numa_</code>.
     </p></div></section><section class="sect3" id="sec-vt-best-perf-numa-cpuset" data-id-title="Memory Allocation Control with the CPUset Controller"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.2 </span><span class="title-name">Memory Allocation Control with the CPUset Controller</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-numa-cpuset">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The cgroups cpuset controller can be used confine memory used by a process
     to a NUMA node. There are three cpuset memory policy modes available:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">interleave</code>: This is a memory placement policy which
       is also known as round-robin. This policy can provide substantial
       improvements for jobs that need to place thread local data on the
       corresponding node. When the interleave destination is not available, it
       will be moved to another node.
      </p></li><li class="listitem"><p>
       <code class="literal">bind</code>: This will place memory only on one node, which
       means in case of insufficient memory, the allocation will fail.
      </p></li><li class="listitem"><p>
       <code class="literal">preferred</code>: This policy will apply a preference to
       allocate memory to a node. If there is not enough space for memory on
       this node, it will fall back to another node.
      </p></li></ul></div><p>
     You can change the memory policy mode with the <code class="command">cgset</code>
     tool from the <span class="package">libcgroup-tools</span> package:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> cgset -r cpuset.mems=<em class="replaceable">NODE</em> sysdefault/libvirt/qemu/<em class="replaceable">KVM_NAME</em>/emulator</pre></div><p>
     To migrate pages to a node, use the <code class="command">migratepages</code> tool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>migratepages <em class="replaceable">PID</em> <em class="replaceable">FROM-NODE</em> <em class="replaceable">TO-NODE</em></pre></div><p>
     To check everything is fine. use: <code class="command">cat
     /proc/<em class="replaceable">PID</em>/status | grep Cpus</code>.
    </p><div id="id-1.14.4.7.8.8.9" data-id-title="Kernel NUMA/cpuset memory policy" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Kernel NUMA/cpuset memory policy</div><p>
      For more information see
      <a class="link" href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt" target="_blank">Kernel
      NUMA memory policy</a> and
      <a class="link" href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt" target="_blank">cpusets
      memory policy</a>. Check also the
      <a class="link" href="https://libvirt.org/formatdomain.html#elementsNUMATuning" target="_blank">Libvirt
      NUMA Tuning documentation</a>.
     </p></div></section><section class="sect3" id="sec-vt-best-perf-numa-vmguest" data-id-title="VM Guest: NUMA Related Configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.3 </span><span class="title-name">VM Guest: NUMA Related Configuration</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-numa-vmguest">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     <code class="systemitem">libvirt</code> allows to set up virtual NUMA and memory access policies.
     Configuring these settings is not supported by
     <code class="command">virt-install</code> or <code class="command">virt-manager</code> and
     needs to be done manually by editing the VM Guest configuration file with
     <code class="command">virsh edit</code>.
    </p><section class="sect4" id="sec-vt-best-perf-numa-vmguest-topo" data-id-title="VM Guest Virtual NUMA Topology"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.6.3.1 </span><span class="title-name">VM Guest Virtual NUMA Topology</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-numa-vmguest-topo">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Creating a VM Guest virtual NUMA (vNUMA) policy that resembles the host
      NUMA topology can often increase performance of traditional large,
      scale-up workloads. VM Guest vNUMA topology can be specified using the
      <code class="literal">numa</code> element in the XML configuration:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<span class="callout" id="co-numa-cell">1</span> id="0"<span class="callout" id="co-numa-id">2</span> cpus='0-1'<span class="callout" id="co-numa-cpus">3</span> memory='512000' unit='KiB'/&gt;
    &lt;cell id="1" cpus='2-3' memory='256000'<span class="callout" id="co-numa-mem">4</span>
    unit='KiB'<span class="callout" id="co-numa-unit">5</span> memAccess='shared'<span class="callout" id="co-numa-memaccess">6</span>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-cell"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Each <code class="literal">cell</code> element specifies a vNUMA cell or node
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-id"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        All cells should have an <code class="literal">id</code> attribute, allowing to
        reference the cell in other configuration blocks. Otherwise cells are
        assigned ids in ascending order starting from 0.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-cpus"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The CPU or range of CPUs that are part of the node
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-mem"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The node memory
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-unit"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Units in which node memory is specified
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-memaccess"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Optional attribute which can control whether the memory is to be mapped
        as <code class="option">shared</code> or <code class="option">private</code>. This is valid
        only for hugepages-backed memory.
       </p></td></tr></table></div><p>
      To find where the VM Guest has allocated its pages. use: <code class="command">cat
      /proc/<em class="replaceable">PID</em>/numa_maps</code> and <code class="command">cat
      /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<em class="replaceable">KVM_NAME</em>/memory.numa_stat</code>.
     </p><div id="id-1.14.4.7.8.9.3.6" data-id-title="NUMA specification" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: NUMA specification</div><p>
       The <code class="systemitem">libvirt</code> VM Guest NUMA specification is currently only available
       for QEMU/KVM.
      </p></div></section><section class="sect4" id="sec-vt-best-perf-numa-vmguest-alloc-libvirt" data-id-title="Memory Allocation Control with libvirt"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.6.3.2 </span><span class="title-name">Memory Allocation Control with <code class="systemitem">libvirt</code></span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-numa-vmguest-alloc-libvirt">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      If the VM Guest has a vNUMA topology (see
      <a class="xref" href="article-vt-best-practices.html#sec-vt-best-perf-numa-vmguest-topo" title="4.6.3.1. VM Guest Virtual NUMA Topology">Section 4.6.3.1, “VM Guest Virtual NUMA Topology”</a>), memory can
      be pinned to host NUMA nodes using the <code class="literal">numatune</code>
      element. This method is currently only available for QEMU/KVM guests.
      See <a class="xref" href="article-vt-best-practices.html#sec-vt-best-perf-numa-alloc-libvirt-non-vnuma" title="Important: Non-vNUMA VM Guest">Important: Non-vNUMA VM Guest</a>
      for how to configure non-vNUMA VM Guests.
     </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
    &lt;memory mode="strict"<span class="callout" id="co-numat-mode">1</span> nodeset="1-4,^3"<span class="callout" id="co-numat-nodeset">2</span>/&gt;
    &lt;memnode<span class="callout" id="co-numat-memnode">3</span> cellid="0"<span class="callout" id="co-numat-cellid">4</span> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<span class="callout" id="co-numat-placement">5</span> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-mode"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Policies available are: <code class="literal">interleave</code> (round-robin
        like), <code class="literal">strict</code> (default) or
        <code class="literal">preferred</code>.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-nodeset"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Specify the NUMA nodes.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-memnode"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Specify memory allocation policies for each guest NUMA node (if this
        element is not defined then this will fall back and use the
        <code class="literal">memory</code> element).
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-cellid"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Addresses the guest NUMA node for which the settings are applied.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-placement"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The placement attribute can be used to indicate the memory placement
        mode for a domain process, the value can be <code class="literal">auto</code> or
        <code class="literal">strict</code>.
       </p></td></tr></table></div><div id="sec-vt-best-perf-numa-alloc-libvirt-non-vnuma" data-id-title="Non-vNUMA VM Guest" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Non-vNUMA VM Guest</div><p>
       On a non-vNUMA VM Guest, pinning memory to host NUMA nodes is done like
       in the following example:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</pre></div><p>
       In this example, memory is allocated from the host nodes
       <code class="literal">0</code> and <code class="literal">1</code>. In case these memory
       requirements cannot be fulfilled, starting the VM Guest will fail.
       <code class="command">virt-install</code> also supports this configuration with
       the <code class="option">--numatune</code> option.
      </p></div><div id="id-1.14.4.7.8.9.4.6" data-id-title="Memory and CPU across NUMA Nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Memory and CPU across NUMA Nodes</div><p>
       You should avoid allocating VM Guest memory across NUMA nodes, and
       prevent virtual CPUs from floating across NUMA nodes.
      </p></div></section></section></section></section><section class="sect1" id="sec-vt-best-img" data-id-title="VM Guest Images"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">VM Guest Images</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Images are virtual disks used to store the operating system and data of
   VM Guests. They can be created, maintained and queried with the
   <code class="command">qemu-img</code> command. Refer to
   <span class="intraxref">Book “Virtualization Guide”, Chapter 28 “Guest Installation”, Section 28.2.2 “Creating, Converting and Checking Disk Images”</span> for more
   information on the <code class="command">qemu-img</code> tool and examples.
  </p><section class="sect2" id="sec-vt-best-img-imageformat" data-id-title="VM Guest Image Formats"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">VM Guest Image Formats</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-imageformat">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Certain storage formats which QEMU recognizes have their origins in other
    virtualization technologies. By recognizing these formats, QEMU can
    leverage either data stores or entire guests that were originally targeted
    to run under these other virtualization technologies. Some formats are
    supported only in read-only mode. To use them in read/write mode, convert
    them to a fully supported QEMU storage format (using
    <code class="command">qemu-img</code>). Otherwise they can only be used as read-only
    data store in a QEMU guest. See SUSE Linux Enterprise
    <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891" target="_blank">Release
    Notes</a> to get the list of supported formats.
   </p><p>
    Use <code class="command">qemu-img info <em class="replaceable">VMGUEST.IMG</em></code>
    to get information about an existing image, such as: the format, the
    virtual size, the physical size, snapshots if available.
   </p><div id="id-1.14.4.8.3.4" data-id-title="Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Performance</div><p>
     It is recommended to convert the disk images to either raw or qcow2 to
     achieve good performance.
    </p></div><div id="id-1.14.4.8.3.5" data-id-title="Encrypted Images Cannot Be Compressed" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Encrypted Images Cannot Be Compressed</div><p>
     When you create an image, you cannot use compression (<code class="option">-c</code>)
     in the output file together with the encryption option
     (<code class="option">-e</code>).
    </p></div><section class="sect3" id="sec-vt-best-img-imageformat-raw" data-id-title="Raw Format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.1 </span><span class="title-name">Raw Format</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-imageformat-raw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       This format is simple and easily exportable to all other
       emulators/hypervisors.
      </p></li><li class="listitem"><p>
       It provides best performance (least I/O overhead).
      </p></li><li class="listitem"><p>
       If your file system supports holes (for example in Ext2 or Ext3 on Linux
       or NTFS on Windows*), then only the written sectors will reserve space.
      </p></li><li class="listitem"><p>
       The raw format allows to copy a VM Guest image to a physical device
       (<code class="command">dd if=<em class="replaceable">VMGUEST.RAW</em>
       of=<em class="replaceable">/dev/sda</em></code>).
      </p></li><li class="listitem"><p>
       It is byte-for-byte the same as what the VM Guest sees, so this wastes
       a lot of space.
      </p></li></ul></div></section><section class="sect3" id="sec-vt-best-img-imageformat-qcow2" data-id-title="qcow2 Format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.2 </span><span class="title-name">qcow2 Format</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-imageformat-qcow2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Use this to have smaller images (useful if your file system does not
       supports holes, for example on Windows*).
      </p></li><li class="listitem"><p>
       It has optional AES encryption.
      </p></li><li class="listitem"><p>
       Zlib-based compression option.
      </p></li><li class="listitem"><p>
       Support of multiple VM snapshots (internal, external).
      </p></li><li class="listitem"><p>
       Improved performance and stability.
      </p></li><li class="listitem"><p>
       Supports changing the backing file.
      </p></li><li class="listitem"><p>
       Supports consistency checks.
      </p></li><li class="listitem"><p>
       Less performance than raw format.
      </p></li></ul></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.14.4.8.3.7.4.1"><span class="term">l2-cache-size</span></dt><dd><p>
        qcow2 can provide the same performance for random read/write access as
        raw format, but it needs a well-sized cache size. By default cache size
        is set to 1 MB. This will give good performance up to a disk size of 8
        GB. If you need a bigger disk size, you need to adjust the cache size.
        For a disk size of 64 GB (64*1024 = 65536), you need 65536 / 8192B = 8
        MB of cache (<code class="option">-drive format=qcow2,l2-cache-size=8M</code>).
       </p></dd><dt id="id-1.14.4.8.3.7.4.2"><span class="term">Cluster Size</span></dt><dd><p>
        The qcow2 format offers the capability to change the cluster size. The
        value must be between 512 KB and 2 MB. Smaller cluster sizes
        can improve the image file size whereas larger cluster sizes generally
        provide better performance.
       </p></dd><dt id="id-1.14.4.8.3.7.4.3"><span class="term">Preallocation</span></dt><dd><p>
        An image with preallocated metadata is initially larger but can improve
        performance when the image needs to grow.
       </p></dd><dt id="id-1.14.4.8.3.7.4.4"><span class="term">Lazy Refcounts</span></dt><dd><p>
        Reference count updates are postponed with the goal of avoiding
        metadata I/O and improving performance. This is particularly beneficial
        with <code class="option">cache=writethrough</code>. This option does not batch
        metadata updates, but if in case of host crash, the reference count
        tables must be rebuilt, this is done automatically at the next open
        with <code class="command">qemu-img check -r all</code>. Note that this takes
        some time.
       </p></dd></dl></div></section><section class="sect3" id="sec-vt-best-img-imageformat-qed" data-id-title="qed format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.3 </span><span class="title-name">qed format</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-imageformat-qed">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     qed is the next-generation qcow (QEMU Copy On Write). Its
     characteristics include:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Strong data integrity because of simple design.
      </p></li><li class="listitem"><p>
       Retains sparseness over non-sparse channels (for example HTTP).
      </p></li><li class="listitem"><p>
       Supports changing the backing file.
      </p></li><li class="listitem"><p>
       Supports consistency checks.
      </p></li><li class="listitem"><p>
       Fully asynchronous I/O path.
      </p></li><li class="listitem"><p>
       Does not support internal snapshots.
      </p></li><li class="listitem"><p>
       Relies on the host file system and cannot be stored on a logical volume
       directly.
      </p></li></ul></div></section><section class="sect3" id="sec-vt-best-img-imageformat-vmdk" data-id-title="VMDK format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.4 </span><span class="title-name">VMDK format</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-imageformat-vmdk">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     VMware 3, 4, or 6 image format, for exchanging images with that product.
    </p></section></section><section class="sect2" id="sec-vt-best-img-overlay" data-id-title="Overlay Disk Images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Overlay Disk Images</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-overlay">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The qcow2 and qed formats provide a way to create a base image (also called
    backing file) and overlay images on top of the base image. A backing file
    is useful to be able to revert to a known state and discard the overlay. If
    you write to the image, the backing image will be untouched and all changes
    will be recorded in the overlay image file. The backing file will never be
    modified unless you use the <code class="option">commit</code> monitor command (or
    <code class="command">qemu-img commit</code>).
   </p><p>
    To create an overlay image:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-img create -o<span class="callout" id="co-1-minoro">1</span>backing_file=vmguest.raw<span class="callout" id="co-1-backingfile">2</span>,backing_fmt=raw<span class="callout" id="co-1-backingfmt">3</span>\
     -f<span class="callout" id="co-1-minorf">4</span> qcow2 vmguest.cow<span class="callout" id="co-1-imagename">5</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-minoro"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Use <code class="option">-o ?</code> for an overview of available options.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-backingfile"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The backing file name.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-backingfmt"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Specify the file format for the backing file.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-minorf"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Specify the image format for the VM Guest.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-imagename"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Image name of the VM Guest, it will only record the differences from the
      backing file.
     </p></td></tr></table></div><div id="id-1.14.4.8.4.6" data-id-title="Backing Image Path" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Backing Image Path</div><p>
     You should not change the path to the backing image, otherwise you will
     need to adjust it. The path is stored in the overlay image file. To update
     the path, you should make a symbolic link from the original path to the
     new path and then use the <code class="command">qemu-img</code>
     <code class="option">rebase</code> option.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE12/vmguest.raw
<code class="prompt root">root # </code>qemu-img rebase<span class="callout" id="co-2-rebase">1</span>-u<span class="callout" id="co-2-unsafe">2</span> -b<span class="callout" id="co-2-minorb">3</span> /var/lib/images/vmguest.raw /var/lib/images/SLE12/vmguest.cow<span class="callout" id="co-2-image">4</span></pre></div><p>
     The <code class="command">rebase</code> subcommand tells <code class="command">qemu-img</code>
     to change the backing file image. The <code class="option">-u</code> option activates
     the unsafe mode (see note below). The backing image to be used is
     specified with <code class="option">-b</code> and the image path is the last argument
     of the command.
    </p><p>
     There are two different modes in which <code class="option">rebase</code> can
     operate:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>Safe</em></span>: This is the default mode and performs a real
       rebase operation. The safe mode is a time-consuming operation.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Unsafe</em></span>: The unsafe mode (<code class="option">-u</code>) only
       changes the backing files name and the format of the file name without
       making any checks on the files contents. You should use this mode to
       rename or moving a backing file.
      </p></li></ul></div></div><p>
    A common use is to initiate a new guest with the backing file. Let's assume
    we have a <code class="filename">sle12_base.img</code> VM Guest ready to be used
    (fresh installation without any modification). This will be our backing
    file. Now you need to test a new package, on an updated system and on a
    system with a different kernel. We can use
    <code class="filename">sle12_base.img</code> to instantiate the new SUSE Linux Enterprise VM Guest
    by creating a qcow2 overlay file pointing to this backing file
    (<code class="filename">sle12_base.img</code>).
   </p><p>
    In our example we will use <code class="filename">sle12_updated.qcow2</code> for the
    updated system, and <code class="filename">sle12_kernel.qcow2</code> for the system
    with a different kernel.
   </p><p>
    To create the two thin provisioned systems use the
    <code class="command">qemu-img</code> command line with the <code class="option">-b</code>
    option:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
/var/lib/libvirt/sle12_updated.qcow2
Formatting 'sle12_updated.qcow2', fmt=qcow2 size=17179869184
backing_file='sle12_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off
<code class="prompt root">root # </code>qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
/var/lib/libvirt/sle12_kernel.qcow2
Formatting 'sle12_kernel.qcow2', fmt=qcow2 size=17179869184
backing_file='vmguest-sle12_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off</pre></div><p>
    The images are now usable, and you can do your test without touching the
    initial <code class="filename">sle12_base.img</code> backing file, all changes will
    be stored in the new overlay images. Additionally, you can also use these
    new images as a backing file, and create a new overlay.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-img create -b sle12_kernel.qcow2 -f qcow2 sle12_kernel_TEST.qcow2</pre></div><p>
    When using <code class="command">qemu-img info</code> with the option
    <code class="option">--backing-chain</code>, it will return all information about the
    entire backing chain recursively:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-img info --backing-chain
/var/lib/libvirt/images/sle12_kernel_TEST.qcow2
image: sle12_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle12_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE12.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</pre></div><div class="figure" id="fig-qemu-img-overlay"><div class="figure-contents"><div class="mediaobject"><a href="images/qemu-img-overlay.png"><img src="images/qemu-img-overlay.png" width="95%" alt="Understanding Image Overlay" title="Understanding Image Overlay"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1: </span><span class="title-name">Understanding Image Overlay </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#fig-qemu-img-overlay">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-vt-best-img-open-img" data-id-title="Opening a VM Guest Image"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Opening a VM Guest Image</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-open-img">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To access the file system of an image, use the
    <span class="package">guestfs-tools</span>. If you do not have this tool installed on
    your system you can mount an image with other Linux tools. Avoid accessing
    an untrusted or unknown VM Guest's image system because this can lead to
    security issues (for more information, read
    <a class="link" href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/" target="_blank">D.
    Berrangé's post</a>).
   </p><section class="sect3" id="sec-vt-best-img-open-img-raw" data-id-title="Opening a Raw Image"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Opening a Raw Image</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-open-img-raw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.14.4.8.5.3.2" data-id-title="Mounting a Raw Image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5: </span><span class="title-name">Mounting a Raw Image </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.8.5.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To be able to mount the image, find a free loop device. The following
       command displays the first unused loop device,
       <code class="filename">/dev/loop1</code> in this example.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>losetup -f
/dev/loop1</pre></div></li><li class="step"><p>
       Associate an image (<code class="filename">SLE12.raw</code> in this example) with
       the loop device:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>losetup /dev/loop1 SLE12.raw</pre></div></li><li class="step"><p>
       Check whether the image has successfully been associated with the loop
       device by getting detailed information about the loop device:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>losetup -l
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE12.raw</pre></div></li><li class="step"><p>
       Check the image's partitions with <code class="command">kpartx</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>kpartx -a<span class="callout" id="co-kpartx-a">1</span> -v<span class="callout" id="co-kpartx-v">2</span> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-kpartx-a"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Add partition device mappings.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-kpartx-v"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Verbose mode.
        </p></td></tr></table></div></li><li class="step"><p>
       Now mount the image partition(s) (to
       <code class="filename">/mnt/sle12mount</code> in the following example):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>mkdir /mnt/sle12mount
<code class="prompt root">root # </code>mount /dev/mapper/loop1p1 /mnt/sle12mount</pre></div></li></ol></div></div><div id="id-1.14.4.8.5.3.3" data-id-title="Raw image with LVM" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Raw image with LVM</div><p>
      If your raw image contains an LVM volume group you should use LVM tools
      to mount the partition. Refer to <a class="xref" href="article-vt-best-practices.html#sec-lvm-found" title="5.3.3. Opening Images Containing LVM">Section 5.3.3, “Opening Images Containing LVM”</a>.
     </p></div><div class="procedure" id="id-1.14.4.8.5.3.4" data-id-title="Unmounting a Raw Image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6: </span><span class="title-name">Unmounting a Raw Image </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.8.5.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Unmount all mounted partitions of the image, for example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>umount /mnt/sle12mount</pre></div></li><li class="step" id="st-umount-raw"><p>
       Delete partition device mappings with <code class="command">kpartx</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>kpartx -d /dev/loop1</pre></div></li><li class="step"><p>
       Detach the devices with <code class="command">losetup</code>
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>losetup -d /dev/loop1</pre></div></li></ol></div></div></section><section class="sect3" id="sec-vt-best-img-open-img-qcow2" data-id-title="Opening a qcow2 Image"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Opening a qcow2 Image</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-open-img-qcow2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.14.4.8.5.4.2" data-id-title="Mounting a qcow2 Image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7: </span><span class="title-name">Mounting a qcow2 Image </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.8.5.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       First you need to load the <code class="literal">nbd</code> (network block
       devices) module. The following example loads it with support for 16
       block devices (<code class="option">max_part=16</code>). Check with
       <code class="command">dmesg</code> whether the operation was successful:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>modprobe nbd max_part=16
<code class="prompt root">root # </code>dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</pre></div></li><li class="step"><p>
       Connect the VM Guest image (for example
       <code class="filename">SLE12.qcow2</code>) to an NBD device
       (<code class="filename">/debv/nbd0</code> in the following example) with the
       <code class="command">qemu-nbd</code> command. Make sure to use a free NBD device:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-nbd -c<span class="callout" id="co-qemunbd-minusc">1</span> /dev/nbd0<span class="callout" id="co-qemunbd-device">2</span> SLE12.qcow2<span class="callout" id="co-qemunbd-image">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-minusc"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Connect <code class="filename">SLE12.qcow2</code> to the local NBD device
         <code class="filename">/dev/nbd0</code>
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-device"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         NBD device to use
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-image"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         VM Guest image to use
        </p></td></tr></table></div><div id="id-1.14.4.8.5.4.2.3.4" data-id-title="Checking for a free NBD Device" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Checking for a free NBD Device</div><p>
        To check whether an NBD device is free, run the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</pre></div><p>
        If the command produces an output like in the example above, the device
        is busy (not free). This can also be confirmed by the presence of the
        <code class="filename">/sys/devices/virtual/block/nbd0/pid</code> file.
       </p></div></li><li class="step"><p>
       Inform the operating system about partition table changes with
       <code class="command">partprobe</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>partprobe /dev/nbd0 -s
/dev/nbd0: msdos partitions 1 2
<code class="prompt root">root # </code>dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</pre></div></li><li class="step"><p>
       In the example above, the <code class="filename">SLE12.qcow2</code> contains two
       partitions: <code class="filename">/dev/nbd0p1</code> and
       <code class="filename">/dev/nbd0p2</code>. Before mounting these partitions, use
       <code class="command">vgscan</code> to check whether they belong to an LVM volume:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>vgscan -v
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Reading all physical volumes. This may take a while...
    Using volume group(s) on command line.
    No volume groups found.</pre></div></li><li class="step"><p>
       If no LVM volume has been found, you can mount the partition with
       <code class="command">mount</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</pre></div><p>
       Refer to <a class="xref" href="article-vt-best-practices.html#sec-lvm-found" title="5.3.3. Opening Images Containing LVM">Section 5.3.3, “Opening Images Containing LVM”</a> for information on how to
       handle LVM volumes.
      </p></li></ol></div></div><div class="procedure" id="id-1.14.4.8.5.4.3" data-id-title="Unmounting a qcow2 Image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8: </span><span class="title-name">Unmounting a qcow2 Image </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.8.5.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Unmount all mounted partitions of the image, for example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>umount /mnt/nbd0p2</pre></div></li><li class="step" id="st-umount-qcow2"><p>
       Disconnect the image from the <code class="filename">/dev/nbd0</code> device.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-nbd -d /dev/nbd0</pre></div></li></ol></div></div></section><section class="sect3" id="sec-lvm-found" data-id-title="Opening Images Containing LVM"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.3 </span><span class="title-name">Opening Images Containing LVM</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-lvm-found">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.14.4.8.5.5.2" data-id-title="Mounting Images Containing LVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 9: </span><span class="title-name">Mounting Images Containing LVM </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.8.5.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To check images for LVM groups, use <code class="command">vgscan -v</code>. If an
       image contains LVM groups, the output of the command looks like the
       following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>vgscan -v
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</pre></div></li><li class="step"><p>
       The <code class="literal">system</code> LVM volume group has been found on the
       system. You can get more information about this volume with
       <code class="command">vgdisplay <em class="replaceable">VOLUMEGROUPNAME</em></code>
       (in our case <em class="replaceable">VOLUMEGROUPNAME</em> is
       <code class="literal">system</code>). You should activate this volume group to
       expose LVM partitions as devices so the system can mount them. Use
       <code class="command">vgchange</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>vgchange -ay -v
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</pre></div></li><li class="step"><p>
       All partitions in the volume group will be listed in the
       <code class="filename">/dev/mapper</code> directory. You can simply mount them
       now.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

<code class="prompt root">root # </code>mkdir /mnt/system-root
<code class="prompt root">root # </code>mount  /dev/mapper/system-root /mnt/system-root

<code class="prompt root">root # </code>ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</pre></div></li></ol></div></div><div class="procedure" id="id-1.14.4.8.5.5.3" data-id-title="Unmounting Images Containing LVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10: </span><span class="title-name">Unmounting Images Containing LVM </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.8.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Unmount all partitions (with <code class="command">umount</code>)
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>umount /mnt/system-root</pre></div></li><li class="step"><p>
       Deactivate the LVM volume group (with <code class="command">vgchange -an
       <em class="replaceable">VOLUMEGROUPNAME</em></code>)
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>vgchange -an -v system
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</pre></div></li><li class="step"><p>
       Now you have two choices:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         In case of a qcow2 image, proceed as described in
         <a class="xref" href="article-vt-best-practices.html#st-umount-qcow2" title="Step 2">Step 2</a> (<code class="command">qemu-nbd -d
         /dev/nbd0</code>).
        </p></li><li class="listitem"><p>
         In case of a raw image, proceeds as described in
         <a class="xref" href="article-vt-best-practices.html#st-umount-raw" title="Step 2">Step 2</a> (<code class="command">kpartx -d
         /dev/loop1</code>; <code class="command">losetup -d /dev/loop1</code>).
        </p></li></ul></div><div id="id-1.14.4.8.5.5.3.4.3" data-id-title="Check for a Successful Unmount" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Check for a Successful Unmount</div><p>
        You should double-check that umounting succeeded by using a system
        command like <code class="command">losetup</code>, <code class="command">qemu-nbd</code>,
        <code class="command">mount</code> or <code class="command">vgscan</code>. If this is not
        the case you may have trouble using the VM Guest because its system
        image is used in different places.
       </p></div></li></ol></div></div></section></section><section class="sect2" id="sec-vt-best-img-share" data-id-title="File System Sharing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">File System Sharing</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-img-share">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can access a host directory in the VM Guest using the
    <code class="sgmltag-element">filesystem</code> element. In the following example we
    will share the <code class="filename">/data/shared</code> directory and mount it in
    the VM Guest. Note that the <code class="sgmltag-attribute">accessmode</code>
    parameter only works with <code class="sgmltag-attribute">type='mount'</code> for the
    QEMU/KVM drive (most other values for <code class="sgmltag-attribute">type</code>
    are exclusively used for the LXC driver).
   </p><div class="verbatim-wrap"><pre class="screen">&lt;filesystem type='mount'<span class="callout" id="co-fs-mount">1</span> accessmode='mapped'<span class="callout" id="co-fs-mode">2</span>&gt;
   &lt;source dir='/data/shared'<span class="callout" id="co-fs-sourcedir">3</span>&gt;
   &lt;target dir='shared'<span class="callout" id="co-fs-targetdir">4</span>/&gt;
&lt;/filesystem&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-mount"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      A host directory to mount VM Guest.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-mode"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Access mode (the security mode) set to <code class="literal">mapped</code> will
      give access with the permissions of the hypervisor. Use
      <code class="literal">passthrough</code> to access this share with the permissions
      of the user inside the VM Guest.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-sourcedir"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Path to share with the VM Guest.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-targetdir"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Name or label of the path for the mount command.
     </p></td></tr></table></div><p>
    To mount the <code class="literal">shared</code> directory on the VM Guest, use the
    following commands: Under the VM Guest now you need to mount the
    <code class="literal">target dir='shared'</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>mkdir /opt/mnt_shared
<code class="prompt root">root # </code>mount shared -t 9p /opt/mnt_shared -o trans=virtio</pre></div><p>
    See
    <a class="link" href="https://libvirt.org/formatdomain.html#elementsFilesystems" target="_blank"><code class="systemitem">libvirt</code>
    File System </a> and
    <a class="link" href="http://wiki.qemu.org/Documentation/9psetup" target="_blank">QEMU
    9psetup</a> for more information.
   </p></section></section><section class="sect1" id="sec-vt-best-vmguests" data-id-title="VM Guest Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">VM Guest Configuration</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vmguests">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-vmguests-virtio" data-id-title="Virtio Driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Virtio Driver</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vmguests-virtio">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To increase VM Guest performance it is recommended to use paravirtualized
    drivers within the VM Guests. The virtualization standard for such drivers
    for KVM are the <code class="literal">virtio</code> drivers, which are designed for
    running in a virtual environment. Xen uses similar paravirtualized device
    drivers (like
    <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
    in a Windows* guest). For a better understanding of this topic, refer to
    <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization Technology”, Section 1.5 “I/O Virtualization”</span>.
   </p><section class="sect3" id="sec-vt-best-vmguests-virtio-virtio-blk" data-id-title="virtio blk"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.1 </span><span class="title-name"><code class="literal">virtio blk</code></span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vmguests-virtio-virtio-blk">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     <code class="literal">virtio_blk</code> is the virtio block device for disk. To use
     the <code class="literal">virtio blk</code> driver for a block device, specify the
     <code class="sgmltag-attribute">bus='virtio'</code> attribute in the
     <code class="sgmltag-element">disk</code> definition:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</pre></div><div id="id-1.14.4.9.2.3.4" data-id-title="Disk Device Names" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Disk Device Names</div><p>
      <code class="literal">virtio</code> disk devices are named
      <code class="literal">/dev/vd[a-z][1-9]</code>. If you migrate a Linux guest from a
      non-virtio disk you need to adjust the <code class="literal">root=</code> parameter
      in the GRUB configuration, and regenerate the <code class="filename">initrd</code>
      file. Otherwise the system cannot boot. On VM Guests with other
      operating systems, the boot loader may need to be adjusted or reinstalled
      accordingly, too.
     </p></div><div id="id-1.14.4.9.2.3.5" data-id-title="Using virtio Disks with qemu-system-ARCH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Using <code class="literal">virtio</code> Disks with <code class="command">qemu-system-ARCH</code></div><p>
      When running <code class="command">qemu-system-ARCH</code>, use the
      <code class="option">-drive</code> option to add a disk to the VM Guest. See
      <span class="intraxref">Book “Virtualization Guide”, Chapter 28 “Guest Installation”, Section 28.1 “Basic Installation with <code class="command">qemu-system-ARCH</code>”</span> for an example. The <code class="option">-hd[abcd]</code>
      option will not work for virtio disks.
     </p></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-virtio-net" data-id-title="virtio net"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.2 </span><span class="title-name">virtio net</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vmguests-virtio-virtio-net">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     <code class="literal">virtio_net</code> is the virtio network device. The kernel
     modules should be loaded automatically in the guest at boot time. You need
     to start the service to make the network available.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-balloon" data-id-title="virtio balloon"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.3 </span><span class="title-name">virtio balloon</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vmguests-virtio-balloon">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The virtio balloon is used for host memory over-commits for guests. For
     Linux guests, the balloon driver runs in the guest kernel, whereas for
     Windows guests, the balloon driver is in the VMDP package.
     <code class="literal">virtio_balloon</code> is a PV driver to give or take memory
     from a VM Guest.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>Inflate balloon</em></span>: Return memory from guest to host
       kernel (for KVM) or to hypervisor (for Xen)
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Deflate balloon</em></span>: Guest will have more available
       memory
      </p></li></ul></div><p>
     It is controlled by the <code class="literal">currentMemory</code> and
     <code class="literal">memory</code> options.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
        &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</pre></div><p>
     You can also use <code class="command">virsh</code> to change it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>virsh setmem <em class="replaceable">DOMAIN_ID</em> <em class="replaceable">MEMORY in KB</em></pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-check" data-id-title="Checking virtio Presence"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.4 </span><span class="title-name">Checking virtio Presence</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vmguests-virtio-check">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You can check the virtio block PCI with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</pre></div><p>
     To find the block device associated with <code class="filename">vdX</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</pre></div><p>
     To get more information on the virtio block:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</pre></div><p>
     To check all virtio drivers being used:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-drv-opt" data-id-title="Find Device Driver Options"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.5 </span><span class="title-name">Find Device Driver Options</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vmguests-virtio-drv-opt">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Virtio devices and other drivers have various options. To list all of
     them, use the <code class="option">help</code> parameter of
     the<code class="command">qemu-system-ARCH</code> command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</pre></div></section></section><section class="sect2" id="sec-vt-best-perf-cirrus" data-id-title="Cirrus Video Driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Cirrus Video Driver</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-cirrus">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To get 16-bit color, high compatibility and better performance it is
    recommended to use the <code class="literal">cirrus</code> video driver.
   </p><div id="id-1.14.4.9.3.3" data-id-title="libvirt" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <code class="systemitem">libvirt</code></div><p>
     <code class="systemitem">libvirt</code> ignores the <code class="literal">vram</code> value because video size has
     been hardcoded in QEMU.
    </p></div><div class="verbatim-wrap"><pre class="screen">&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</pre></div></section><section class="sect2" id="sec-vt-best-entropy" data-id-title="Better Entropy"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">Better Entropy</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-entropy">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Virtio RNG (random number generator) is a paravirtualized device that is
    exposed as a hardware RNG device to the guest. On the host side, it can be
    wired up to one of several sources of entropy (including a real hardware
    RNG device and the host's <code class="filename">/dev/random</code>) if hardware
    support does not exist. The Linux kernel contains the guest driver for the
    device from version 2.6.26 and higher.
   </p><p>
    The system entropy is collected from various non-deterministic hardware
    events and is mainly used by cryptographic applications. The virtual random
    number generator device (paravirtualized device) allows the host to pass
    through entropy to VM Guest operating systems. This results in a better
    entropy in the VM Guest.
   </p><p>
    To use Virtio RNG, add an <code class="literal">RNG</code> device in
    <code class="command">virt-manager</code> or directly in the VM Guest's XML
    configuration:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;devices&gt;
   &lt;rng model='virtio'&gt;
       &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</pre></div><p>
    The host now should used <code class="filename">/dev/random</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</pre></div><p>
    On the VM Guest, the source of entropy can be checked with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cat /sys/devices/virtual/misc/hw_random/rng_available</pre></div><p>
    The current device used for entropy can be checked with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</pre></div><p>
    You should install the <span class="package">rng-tools</span> package on the
    VM Guest, enable the service, and start it. Under SLE12 do the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>zypper in rng-tools
<code class="prompt root">root # </code>systemctl enable rng-tools
<code class="prompt root">root # </code>systemctl start rng-tools</pre></div></section><section class="sect2" id="sec-vt-best-perf-disable" data-id-title="Disable Unused Tools and Devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Disable Unused Tools and Devices</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-disable">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Per host, use one virtualization technology only. For example, do not use
    KVM and Containers on the same computer. Otherwise, you may find yourself
    with a reduced amount of available resources, increased security risk and a
    longer software update queue. Even when the amount of resources allocated
    to each of the technologies is configured carefully, the host may suffer
    from reduced overall availability and degraded performance.
   </p><p>
    Minimize the amount of software and services available on hosts. Most
    default installations of operating systems are not optimized for VM usage.
    Install what you really need and remove all other components in the
    VM Guest.
   </p><p>
    Windows* Guest:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Disable the screen saver
     </p></li><li class="listitem"><p>
      Remove all graphical effects
     </p></li><li class="listitem"><p>
      Disable indexing of hard disks if not necessary
     </p></li><li class="listitem"><p>
      Check the list of started services and disable the ones you do not need
     </p></li><li class="listitem"><p>
      Check and remove all unneeded devices
     </p></li><li class="listitem"><p>
      Disable system update if not needed, or configure it to avoid any delay
      while rebooting or shutting down the host
     </p></li><li class="listitem"><p>
      Check the Firewall rules
     </p></li><li class="listitem"><p>
      Schedule backups and anti-virus updates appropriately
     </p></li><li class="listitem"><p>
      Install the
      <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
      paravirtualized driver for best performance
     </p></li><li class="listitem"><p>
      Check the operating system recommendations, such as on the
      <a class="link" href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7" target="_blank">Microsoft
      Windows* 7 better performance</a> Web page.
     </p></li></ul></div><p>
    Linux Guest:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Remove or do not start the X Window System if not necessary
     </p></li><li class="listitem"><p>
      Check the list of started services and disable the ones you do not need
     </p></li><li class="listitem"><p>
      Check the OS recommendations for kernel parameters that enable better
      performance
     </p></li><li class="listitem"><p>
      Only install software that you really need
     </p></li><li class="listitem"><p>
      Optimize the scheduling of predictable tasks (system updates, hard disk
      checks, etc.)
     </p></li></ul></div></section><section class="sect2" id="sec-vt-best-perf-mtype" data-id-title="Updating the Guest Machine Type"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5 </span><span class="title-name">Updating the Guest Machine Type</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-perf-mtype">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    QEMU machine types define details of the architecture that are
    particularly relevant for migration and session management. As changes or
    improvements to QEMU are made, new machine types are added. Old machine
    types are still supported for compatibility reasons, but to take advantage
    of improvements, we recommend to always migrate to the latest machine type
    when upgrading.
   </p><p>
    Changing the guest's machine type for a Linux guest will mostly be
    transparent. For Windows* guests, we recommend to take a snapshot or backup
    of the guest—in case Windows* has issues with the changes it detects
    and subsequently the user decides to revert to the original machine type
    the guest was created with.
   </p><div id="id-1.14.4.9.6.4" data-id-title="Changing the Machine Type" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Changing the Machine Type</div><p>
     Refer to <span class="intraxref">Book “Virtualization Guide”, Chapter 14 “Configuring Virtual Machines”, Section 14.10 “Changing the Machine Type with <code class="command">virsh</code>”</span> for documentation.
    </p></div></section></section><section class="sect1" id="sec-vt-best-vm-setup-config" data-id-title="VM Guest-Specific Configurations and Settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">VM Guest-Specific Configurations and Settings</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-vm-setup-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-acpi" data-id-title="ACPI Testing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">ACPI Testing</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-acpi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The ability to change a VM Guest's state heavily depends on the operating
    system. It is very important to test this feature before any use of your
    VM Guests in production. For example, most Linux operating systems disable
    this capability by default, so this requires you to enable this operation
    (mostly through Polkit).
   </p><p>
    ACPI must be enabled in the guest for a graceful shutdown to work. To check
    if ACPI is enabled, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>virsh dumpxml <em class="replaceable">VMNAME</em> | grep acpi</pre></div><p>
    If nothing is printed, ACPI is not enabled for your machine. Use
    <code class="command">virsh edit</code> to add the following XML under
    &lt;domain&gt;:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</pre></div><p>
    If ACPI was enabled during a Windows Server* guest installation, it
    is not sufficient to turn it on in the VM Guest configuration only. For
    more information, see
    <a class="link" href="https://support.microsoft.com/en-us/kb/309283" target="_blank">https://support.microsoft.com/en-us/kb/309283</a>.
    
   </p><p>
    Regardless of the VM Guest's configuration, a graceful shutdown is always
    possible from within the guest operating system.
   </p></section><section class="sect2" id="sec-vt-best-guest-kbd" data-id-title="Keyboard Layout"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Keyboard Layout</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-guest-kbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Though it is possible to specify the keyboard layout from a
    <code class="command">qemu-system-ARCH</code> command, it is recommended to configure
    it in the <code class="systemitem">libvirt</code> XML file. To change the keyboard layout while
    connecting to a remote VM Guest using VNC, you should edit the VM Guest
    XML configuration file. For example, to add an <code class="literal">en-us</code>
    keymap, add in the <code class="literal">&lt;devices&gt;</code> section:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</pre></div><p>
    Check the <code class="literal">vncdisplay</code> configuration and connect to your
    VM Guest:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>virsh vncdisplay sles12 127.0.0.1:0</pre></div></section><section class="sect2" id="sec-vt-best-spice-default-url" data-id-title="Spice default listen URL"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Spice default listen URL</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-spice-default-url">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If no network interface other than <code class="literal">lo</code> is assigned an
    IPv4 address on the host, the default address on which the spice server
    listens will not work. An error like the following one will occur:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>virsh start sles12
error: Failed to start domain sles12
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</pre></div><p>
    To fix this, you can change the default <code class="literal">spice_listen</code>
    value in <code class="filename">/etc/libvirt/qemu.conf</code> using the local IPv6
    address <code class="systemitem">::1</code>. The spice server
    listening address can also be changed on a per VM Guest basis, use
    <code class="command">virsh edit</code> to add the listen XML attribute to the
    <code class="literal">graphics type='spice'</code> element:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;&gt;</pre></div></section><section class="sect2" id="sec-vt-best-xml-to-qemu" data-id-title="XML to QEMU command line"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">XML to QEMU command line</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-xml-to-qemu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Sometimes it could be useful to get the QEMU command line to launch the
    VM Guest from the XML file.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>virsh domxml-to-native<span class="callout" id="co-domxml-native">1</span> qemu-argv<span class="callout" id="co-domxml-argv">2</span> SLE12.xml<span class="callout" id="co-domxml-file">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-native"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Convert the XML file in domain XML format to the native guest
      configuration
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-argv"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      For the QEMU/KVM hypervisor, the format argument needs be qemu-argv
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-file"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Domain XML file to use
     </p></td></tr></table></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE12.xml
LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE12 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE12.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE12.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</pre></div></section><section class="sect2" id="sec-vt-best-kernel-parameter" data-id-title="Change Kernel Parameters at Boot Time"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">Change Kernel Parameters at Boot Time</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-kernel-parameter">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect3" id="sec-vt-best-kernel-parameter-sle11" data-id-title="SUSE Linux Enterprise 11"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.5.1 </span><span class="title-name">SUSE Linux Enterprise 11</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-kernel-parameter-sle11">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To change the value for SLE 11 products at boot time, you need to
     modify your <code class="filename">/boot/grub/menu.lst</code> file by adding the
     <code class="option">OPTION=parameter</code>. Then reboot your system.
    </p></section><section class="sect3" id="sec-vt-best-kernel-parameter-sle12" data-id-title="SUSE Linux Enterprise 12"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.5.2 </span><span class="title-name">SUSE Linux Enterprise 12</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-kernel-parameter-sle12">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To change the value for SLE 12 products at boot time, you need to
     modify your <code class="filename">/etc/default/grub</code> file. Find the variable
     starting with <code class="option">GRUB_CMDLINE_LINUX_DEFAULT</code> and add at the
     end <code class="option">OPTION=parameter</code> (or change it with the correct value
     if it is already available).
    </p><p>
     Now you need to regenerate your <code class="literal">grub2</code> configuration:
    </p><div class="verbatim-wrap"><pre class="screen"># grub2-mkconfig -o /boot/grub2/grub.cfg</pre></div><p>
     Then reboot your system
    </p></section></section><section class="sect2" id="sec-vt-best-guest-device-to-xml" data-id-title="Add a Device to an XML Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6 </span><span class="title-name">Add a Device to an XML Configuration</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-guest-device-to-xml">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To create a new VM Guest based on an XML file, you can specify the QEMU
    command line using the special tag <code class="literal">qemu:commandline</code>. For
    example, to add a virtio-balloon-pci, add this block at the end of the XML
    configuration file (before the &lt;/domain&gt; tag):
   </p><div class="verbatim-wrap"><pre class="screen">&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</pre></div></section></section><section class="sect1" id="sec-vt-best-hypervisors-containers" data-id-title="Hypervisors Compared to Containers"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Hypervisors Compared to Containers</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-hypervisors-containers">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="table" id="id-1.14.4.11.3" data-id-title="Hypervisors Compared to Containers"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 4: </span><span class="title-name">Hypervisors Compared to Containers </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.11.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 20%; "/><col style="width: 30%; "/><col style="width: 30%; "/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Features
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Hypervisors
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Containers
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Technologies
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Emulation of a physical computing environment
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Use kernel host
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        System layer level
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Managed by a virtualization layer (Hypervisor)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Rely on kernel namespaces and cgroups
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Level (layer)
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Hardware level
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Software level
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Virtualization mode available
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        FV or PV
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        None, only user space
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Security
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Strong
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div id="id-1.14.4.11.3.2.5.5.3.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
         Security is very low
        </p></div>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Confinement
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Full isolation
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div id="id-1.14.4.11.3.2.5.6.3.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
         Host kernel (OS must be compatible with kernel version)
        </p></div>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Operating system
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Any operating system
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Only Linux (must be "kernel" compatible)
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Type of system
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Full OS needed
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Scope is an instance of Linux
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Boot time
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Slow to start (OS delay)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Really quick start
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Overhead
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        High
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Very low
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Efficiency
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Depends on OS
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Very efficient
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Sharing with host
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <div id="id-1.14.4.11.3.2.5.12.2.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
         Complex because of isolation
        </p></div>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Sharing is easy (host sees everything; container sees its own objects)
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Migration
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        Supports migration (live mode)
       </p>
      </td><td>
       <div id="id-1.14.4.11.3.2.5.13.3.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
         Not possible
        </p></div>
      </td></tr></tbody></table></div></div><section class="sect2" id="sec-vt-best-hypervisors-containers-both" data-id-title="Getting the Best of Both Worlds"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">Getting the Best of Both Worlds</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-hypervisors-containers-both">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Even if the above table seems to indicate that running a single application
    in a highly secure way is not possible, <code class="command">virt-sandbox</code>
    will allow running a single application in a KVM guest, starting with
    SUSE Linux Enterprise Server 12 SP1. <code class="command">virt-sandbox</code> bootstraps any command within
    a Linux kernel with a minimal root file system.
   </p><p>
    The guest root file system can either be the root file system mounted
    read-only or a disk image. The following steps will show how to set up a
    sandbox with qcow2 disk image as root file system.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the disk image using <code class="command">qemu-img</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-img create -f qcow2 rootfs.qcow2 6G</pre></div></li><li class="step"><p>
      Format the disk image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>modprobe nbd<span class="callout" id="co-vsmkfs-modprobe">1</span>
<code class="prompt root">root # </code>/usr/bin/qemu-nbd --format qcow2 -n -c /dev/nbd0 $PWD/test-base.qcow2<span class="callout" id="co-vsmkfs-qemu-nbd">2</span>
<code class="prompt root">root # </code>mkfs.ext3 /dev/nbd0<span class="callout" id="co-vsmkfs-do">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vsmkfs-modprobe"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Make sure the nbd module is loaded: it is not loaded by default and
        will only be used to format the qcow image.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vsmkfs-qemu-nbd"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Create an NBD device for the qcow2 image. This device will then behave
        like any other block device. The example uses
        <em class="replaceable">/dev/nbd0</em> but any other free NBD device will
        work.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vsmkfs-do"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Format the disk image directly. Note that no partition table has been
        created: <code class="command">virt-sandbox</code> considers the image to be a
        partition, not a disk.
       </p><p>
        The partition formats that can be used are limited: the Linux kernel
        bootstrapping the sandbox needs to have the corresponding features
        built in. The Ext4 module is also available at the sandbox start-up
        time.
       </p></td></tr></table></div></li><li class="step"><p>
      Now populate the newly formatted image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>guestmount -a base.qcow2 -m /dev/sda:/ /mnt<span class="callout" id="co-vsfs-mount">1</span>

<code class="prompt root">root # </code>zypper --root /mnt ar cd:///?devices=/dev/dvd SLES12_DVD
<code class="prompt root">root # </code>zypper --root /mnt in -t pattern Minimal<span class="callout" id="co-vsfs-install">2</span>

<code class="prompt root">root # </code>guestunmount /mnt<span class="callout" id="co-vsfs-unmount">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vsfs-mount"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Mount the qcow2 image using the <code class="command">guestfs</code> tools.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vsfs-install"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Use Zypper with the <code class="literal">--root</code> parameter to add a SUSE Linux Enterprise Server
        repository and install the <code class="literal">Minimal</code> pattern in the
        disk image. Any additional package or configuration change should be
        performed in this step.
       </p><div id="id-1.14.4.11.4.4.3.3.2.2" data-id-title="Using backing chains" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Using backing chains</div><p>
         To share the root file system between several sandboxes, create qcow2
         images with a common disk image as backing chain as described in
         <a class="xref" href="article-vt-best-practices.html#sec-vt-best-img-overlay" title="5.2. Overlay Disk Images">Section 5.2, “Overlay Disk Images”</a>.
        </p></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vsfs-unmount"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Unmount the qcow2 image.
       </p></td></tr></table></div></li><li class="step"><p>
      Run the sandbox, using <code class="command">virt-sandbox</code>. This command has
      many interesting options, read its man page to discover them all. The
      command can be run as <code class="systemitem">root</code> or as an unprivileged user.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>virt-sandbox -n <em class="replaceable">NAME</em> \
     -m host-image:/=$PWD/rootfs.qcow2 \ <span class="callout" id="co-vs-rootfs">1</span>
     -m host-bind:/srv/www=/guests/www \ <span class="callout" id="co-vs-bind">2</span>
     -m ram:/tmp=100MiB \
     -m ram:/run=100MiB \ <span class="callout" id="co-vs-tmpfs">3</span>
     -N source=default,address=192.168.122.12/24 \ <span class="callout" id="co-vs-net">4</span>
     -- \
     <em class="replaceable">/bin/sh</em></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vs-rootfs"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        Mount the created disk image as the root file system. Note that without
        any image being mounted as <code class="filename">/</code>, the host root file
        system is read-only mounted as the guest one.
       </p><p>
        The host-image mount is not reserved for the root file system, it can
        be used to mount any disk image anywhere in the guest.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vs-bind"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The host-bind mount is pretty convenient for sharing files and
        directories between the host and the guest. In this example the host
        directory <code class="filename">/guests/www</code> is mounted as
        <code class="filename">/srv/www</code> in the sandbox.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vs-tmpfs"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The RAM mounts are defining <code class="literal">tmpfs</code> mounts in the
        sandbox.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-vs-net"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The network uses a network defined in libvirt. When running as an
        unprivileged user, the source can be omitted, and the KVM user
        networking feature will be used. Using this option requires the
        <span class="package">dhcp-client</span> and <span class="package">iproute2</span>
        packages, which are part of the SUSE Linux Enterprise Server <code class="literal">Minimal</code>
        pattern.
       </p></td></tr></table></div></li></ol></div></div></section></section><section class="sect1" id="sec-vt-best-xen-pv-fv" data-id-title="Xen: Converting a Paravirtual (PV) Guest to a Fully Virtual (FV/HVM) Guest"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Xen: Converting a Paravirtual (PV) Guest to a Fully Virtual (FV/HVM) Guest</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-xen-pv-fv">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This chapter explains how to convert a Xen paravirtual machine into a
   Xen fully virtualized machine.
  </p><div class="procedure" id="id-1.14.4.12.3" data-id-title="Guest Side"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11: </span><span class="title-name">Guest Side </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.12.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    In order to start the guest in FV mode, you have to run the following steps
    inside the guest.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Prior to converting the guest, apply all pending patches and reboot the
     guest.
    </p></li><li class="step"><p>
     FV machines use the <code class="literal">-default</code> kernel. If this kernel is
     not already installed, install the <code class="literal">kernel-default</code>
     package (while running in PV mode).
    </p></li><li class="step"><p>
     PV machines typically use disk names such as <code class="literal">vda*</code>.
     These names must be changed to the FV <code class="literal">hd*</code> syntax. This
     change must be done in the following files:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="filename">/etc/fstab</code>
      </p></li><li class="listitem"><p>
       <code class="filename">/boot/grub/menu.lst</code> (SLES 11 only)
      </p></li><li class="listitem"><p>
       <code class="filename">/boot/grub*/device.map</code>
      </p></li><li class="listitem"><p>
       <code class="filename">/etc/sysconfig/bootloader</code>
      </p></li><li class="listitem"><p>
       <code class="filename">/etc/default/grub</code> (SLES 12 and later; only)
      </p></li></ul></div><div id="id-1.14.4.12.3.5.3" data-id-title="Prefer UUIDs" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Prefer UUIDs</div><p>
      You should use UUIDs or logical volumes within your
      <code class="filename">/etc/fstab</code>. Using UUID simplifies using attached
      network storage, multipathing, and virtualization. To find the UUID of
      your disk use the command <code class="command">blkid</code>.
     </p></div></li><li class="step"><p>
     To avoid any error regenerating the initrd with the required modules you
     can create a symlink from <code class="filename">/dev/hda2</code> to
     <code class="filename">/dev/xvda2</code> etc. by using the <code class="command">ln</code>:
    </p><div class="verbatim-wrap"><pre class="screen">ln -sf /dev/xvda2 /dev/hda2
ln -sf /dev/xvda1 /dev/hda1
.....</pre></div></li><li class="step"><p>
     PV and FV machines use different disk and network driver modules. These FV
     modules must be added to the initrd manually. The expected modules are
     <code class="literal">xen-vbd</code> (for disk) and <code class="literal">xen-vnif</code> (for
     network). These are the only PV drivers for a fully virtualized VM Guest.
     All other modules, such as <code class="literal">ata_piix</code>,
     <code class="literal">ata_generic</code> and <code class="literal">libata</code>, should be
     added automatically.
    </p><div id="id-1.14.4.12.3.7.2" data-id-title="Adding Modules to the initrd" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Adding Modules to the initrd</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        On SLES 11, you can add modules to the
        <code class="literal">INITRD_MODULES</code> line in the
        <code class="filename">/etc/sysconfig/kernel</code> file. For example:
       </p><div class="verbatim-wrap"><pre class="screen">INITRD_MODULES="xen-vbd xen-vnif"</pre></div><p>
        Run <code class="command">mkinitrd</code> to build a new initrd containing the
        modules.
       </p></li><li class="listitem"><p>
        On SLES 12, open or create
        <code class="filename">/etc/dracut.conf.d/10-virt.conf</code> and add the
        modules with <code class="literal">force_drivers</code> by adding a line as in
        the example below (mind the leading whitespace):
       </p><div class="verbatim-wrap"><pre class="screen">force_drivers+=" xen-vbd xen-vnif"</pre></div><p>
        Run <code class="command">dracut -f --kver
        <em class="replaceable">KERNEL_VERSION</em>-default</code> to build a
        new initrd (for the -default version of the kernel) that contains the
        required modules.
       </p><div id="id-1.14.4.12.3.7.2.2.2.4" data-id-title="Find Your Kernel Version" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Find Your Kernel Version</div><p>
         Use the <code class="command">uname -r</code> command to get the current version
         used on your system.
        </p></div></li></ul></div></div></li><li class="step"><p>
     Before shutting down the guest, set the default boot option to the
     <code class="literal">-default</code> kernel using <code class="command">yast
     bootloader</code>.
    </p></li><li class="step"><p>
     Under <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 11, if you have an X server running on your guest, you
     need to adjust the <code class="filename">/etc/X11/xorg.conf</code> file in order
     to adjust the X driver. Search for <code class="literal">fbdev</code> and change to
     <code class="literal">cirrus</code>.
    </p><div class="verbatim-wrap"><pre class="screen">Section "Device"
          Driver       "cirrus"
          ......
          EndSection</pre></div><div id="id-1.14.4.12.3.9.3" data-id-title="SUSE Linux Enterprise Server 12 and Xorg" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12 and Xorg</div><p>
      Under <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12, Xorg will automatically adjust the driver needed
      to be able to get a working X server.
     </p></div></li><li class="step"><p>
     Shut down the guest.
    </p></li></ol></div></div><div class="procedure" id="id-1.14.4.12.4" data-id-title="Host Side"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 12: </span><span class="title-name">Host Side </span></span><a title="Permalink" class="permalink" href="article-vt-best-practices.html#id-1.14.4.12.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    The following steps explain the action you have to perform on the host.
   </p><ol class="procedure" type="1"><li class="step"><p>
     To start the guest in FV mode, the configuration of the VM must be
     modified to match an FV configuration. Editing the configuration of the VM
     can easily be done using <code class="command">virsh edit [DOMAIN]</code>. The
     following changes are recommended:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Make sure the machine, the type and the <code class="literal">loader</code>
       entries in the OS section are changed from <code class="literal">xenpv</code> to
       <code class="literal">xenfv</code>. The updated OS section should look similar to:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;os&gt;
          &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
          &lt;loader&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
          &lt;boot dev='hd'/&gt;
&lt;/os&gt;</pre></div></li><li class="listitem"><p>
       In the OS section remove anything that is specific to PV guest:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><div class="verbatim-wrap"><pre class="screen">&lt;bootloader&gt;pygrub&lt;/bootloader&gt;</pre></div></li><li class="listitem"><div class="verbatim-wrap"><pre class="screen">&lt;kernel&gt;/usr/lib/grub2/x86_64-xen/grub.xen&lt;/kernel&gt;</pre></div></li><li class="listitem"><div class="verbatim-wrap"><pre class="screen">&lt;cmdline&gt;xen-fbfront.video=4,1024,768&lt;/cmdline&gt;</pre></div></li></ul></div></li><li class="listitem"><p>
       In the devices section, add the qemu emulator as:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</pre></div></li><li class="listitem"><p>
       Update the disk configuration so the target device and bus use the FV
       syntax. This requires replacing the <code class="literal">xen</code> disk bus with
       <code class="literal">ide</code>, and the <code class="literal">vda</code> target device
       with <code class="literal">hda</code>. The changes should look similar to:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;target dev='hda' bus='ide'/&gt;</pre></div></li><li class="listitem"><p>
       Change the bus for the mouse and keyboard from <code class="literal">xen</code> to
       <code class="literal">ps2</code>. Also add a new USB tablet device:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;input type='mouse' bus='ps2'/&gt;
          &lt;input type='keyboard' bus='ps2'/&gt;
&lt;input type='tablet' bus='usb'/&gt;</pre></div></li><li class="listitem"><p>
       Change the console target type from <code class="literal">xen</code> to
       <code class="literal">serial</code>:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;console type='pty'&gt;
          &lt;target type='serial' port='0'/&gt;
&lt;/console&gt;</pre></div></li><li class="listitem"><p>
       Change the video configuration from <code class="literal">xen</code> to
       <code class="literal">cirrus</code>, with 8 M of VRAM:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;video&gt;
          &lt;model type='cirrus' vram='8192' heads='1' primary='yes'/&gt;
&lt;/video&gt;</pre></div></li><li class="listitem"><p>
       If desired, add <code class="literal">acpi</code> and <code class="literal">apic</code> to
       the features of the VM:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;features&gt;
          &lt;acpi/&gt;
          &lt;apic/&gt;
&lt;/features&gt;</pre></div></li></ul></div></li><li class="step"><p>
     Start the guest (using <code class="command">virsh</code> or
     <code class="command">virt-manager</code>). If the guest is running kernel-default
     (as verified through <code class="command">uname -a</code>), the machine is running
     in Fully Virtual mode.
    </p></li></ol></div></div><div id="id-1.14.4.12.5" data-id-title="guestfs-tools" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: guestfs-tools</div><p>
    To script this process, or work on disk images directly, you can use the suite
    described in <span class="intraxref">Book “Virtualization Guide”, Chapter 17 “libguestfs”, Section 17.3 “Guestfs Tools”</span>. Numerous tools exist there to
    help modify disk images.
   </p></div></section><section class="sect1" id="sec-vt-best-refs" data-id-title="External References"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">External References</span></span> <a title="Permalink" class="permalink" href="article-vt-best-practices.html#sec-vt-best-refs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/maintenance/SLE12SP4/xml/vt_best_practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="link" href="https://kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf" target="_blank">Increasing
     memory density using KSM</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://www.linux-kvm.org/page/KSM" target="_blank">linux-kvm.org
     KSM</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/vm/ksm.txt" target="_blank">KSM's
     kernel documentation</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://lwn.net/Articles/329123/" target="_blank">ksm - dynamic page
     sharing driver for linux v4</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://www.espenbraastad.no/post/memory-ballooning/" target="_blank">Memory
     Ballooning</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://wiki.libvirt.org/page/Virtio" target="_blank">libvirt
     virtio</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt" target="_blank">CFQ's
     kernel documentation</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt" target="_blank">Documentation
     for sysctl</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://lwn.net/Articles/525459/" target="_blank">LWN Random
     Number</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf" target="_blank">Dr.
     Khoa Huynh, IBM Linux Technology Center</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/Documentation/admin-guide/kernel-parameters.txt" target="_blank">Kernel
     Parameters</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://lwn.net/Articles/374424/" target="_blank">Huge pages
     Administration (Mel Gorman)</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_blank">kernel
     hugetlbpage</a>
    </p></li></ul></div></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-scenario"><span class="title-number">1 </span><span class="title-name">Virtualization Scenarios</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-intro"><span class="title-number">2 </span><span class="title-name">Before You Apply Modifications</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-reco"><span class="title-number">3 </span><span class="title-name">Recommendations</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-hostlevel"><span class="title-number">4 </span><span class="title-name">VM Host Server Configuration and Resource Allocation</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-img"><span class="title-number">5 </span><span class="title-name">VM Guest Images</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-vmguests"><span class="title-number">6 </span><span class="title-name">VM Guest Configuration</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-vm-setup-config"><span class="title-number">7 </span><span class="title-name">VM Guest-Specific Configurations and Settings</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-hypervisors-containers"><span class="title-number">8 </span><span class="title-name">Hypervisors Compared to Containers</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-xen-pv-fv"><span class="title-number">9 </span><span class="title-name">Xen: Converting a Paravirtual (PV) Guest to a Fully Virtual (FV/HVM) Guest</span></a></span></li><li><span class="sect1"><a href="article-vt-best-practices.html#sec-vt-best-refs"><span class="title-number">10 </span><span class="title-name">External References</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>