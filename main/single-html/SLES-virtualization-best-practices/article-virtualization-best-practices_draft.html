<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><title>SLES 15 SP7 | Virtualization Best Practices</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Virtualization Best Practices | SLES 15 SP7"/>
<meta name="description" content="How to choose the optimal virtualization technology based on workload"/>
<meta name="product-name" content="SUSE Linux Enterprise Server"/>
<meta name="product-number" content="15 SP7"/>
<meta name="book-title" content="Virtualization Best Practices"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="taroth@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise Server 15 SP7"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Virtualization Best Practices"/>
<meta property="og:description" content="Make the best virtualization choice for your setup"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Virtualization Best Practices"/>
<meta name="twitter:description" content="Make the best virtualization choice for your setup"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    
    "inLanguage": "en",
    

    "headline": "Virtualization Best Practices",
  
    "description": "How to choose the optimal virtualization technology based on workload",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2024-06-26T00:00+02:00",
      
    "datePublished": "2024-06-26T00:00+02:00",
      

    "about": [
      {
        "@type": "Thing",
        "name": "Virtualization"
      }
    ],
  
    "mentions": [
      
      { "@type": "SoftwareApplication",
        "name": "SUSE Linux Enterprise Server",
        "softwareVersion": "15 SP7",
        "applicationCategory": "Operating System",
        "operatingSystem": "Linux"
      }
      
    ],
    
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#article-virtualization-best-practices">Virtualization Best Practices</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="article" id="article-virtualization-best-practices" data-id-title="Virtualization Best Practices"><div class="titlepage"><div><div class="big-version-info"><span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">15 SP7</span></span></span></div><div><div class="title-container"><h1 class="title">Virtualization Best Practices <a title="Permalink" class="permalink" href="#article-virtualization-best-practices">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div><div class="date"><span class="imprint-label">Publication Date: </span>
    October 18, 2024

   </div><div><div class="titlepage-revhistory"><a aria-label="Revision History" hreflang="en" href="rh-article-virtualization-best-practices.html" target="_blank">Revision History: Virtualization Best Practices</a></div></div></div></div><section class="sect1" id="sec-vt-best-scenario" data-id-title="Virtualization scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Virtualization scenarios</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-scenario">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Virtualization offers a lot of capabilities for your environment. It can
      be used in multiple scenarios. To get more details about it, refer to the
      <span class="intraxref">Book “Virtualization Guide”</span>, and in particular to the following
      sections:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <span class="intraxref">Book “Virtualization Guide”, Chapter 2 “Virtualization scenarios”, Section 2.1 “Server consolidation”</span>
        </p></li><li class="listitem"><p>
          <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization technology”, Section 1.2 “Virtualization benefits”</span>
        </p></li></ul></div><p>
      This best practice guide provides advice for making the right choice in
      your environment. It recommends or discourages the use of options
      depending on your workload. Fixing configuration issues and performing
      tuning tasks increases the performance of VM Guests near to bare metal.
    </p></section><section class="sect1" id="sec-vt-best-intro" data-id-title="Before you apply modifications"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Before you apply modifications</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-intro-backup" data-id-title="Back up first"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Back up first</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-backup">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Changing the configuration of the VM Guest or the VM Host Server can lead to
        data loss or an unstable state. It is really important that you do
        backups of files, data, images, etc. before making any changes. Without
        backups, you cannot restore the original state after a data loss or a
        misconfiguration. Do not perform tests or experiments on production
        systems.
      </p></section><section class="sect2" id="sec-vt-best-intro-testing" data-id-title="Test your workloads"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Test your workloads</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-testing">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The efficiency of a virtualization environment depends on many factors.
        This guide helps to make good choices when configuring virtualization
        in a production environment. Nothing is <span class="emphasis"><em>carved in
        stone</em></span>. Hardware, workloads, resource capacity, etc. should
        all be considered when planning, testing and deploying your
        virtualization infrastructure. Testing your virtualized workloads is
        vital to a successful virtualization implementation.
      </p></section></section><section class="sect1" id="sec-vt-best-reco" data-id-title="Recommendations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Recommendations</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-reco">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-intro-libvirt" data-id-title="Prefer the libvirt framework"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Prefer the <code class="systemitem">libvirt</code> framework</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-libvirt">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        SUSE strongly recommends using the <code class="systemitem">libvirt</code> framework to configure,
        manage and operate VM Host Servers and VM Guest. It offers a single
        interface (GUI and shell) for all supported virtualization technologies
        and therefore is easier to use than the hypervisor-specific tools.
      </p><p>
        We do not recommend using libvirt and hypervisor-specific tools at the
        same time, because changes done with the hypervisor-specific tools may
        not be recognized by the libvirt tool set. See
        <span class="intraxref">Book “Virtualization Guide”, Chapter 8 “<code class="systemitem">libvirt</code> daemons”</span> for more information on libvirt.
      </p></section><section class="sect2" id="sec-vt-best-intro-qemu" data-id-title="qemu-system-i386 compared to qemu-system-x86_64"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">qemu-system-i386 compared to qemu-system-x86_64</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-intro-qemu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Similar to real 64-bit PC hardware,
        <code class="command">qemu-system-x86_64</code> supports VM Guests running a
        32-bit or a 64-bit operating system. Because
        <code class="command">qemu-system-x86_64</code> usually also provides better
        performance for 32-bit guests, SUSE recommends using
        <code class="command">qemu-system-x86_64</code> for both 32-bit and 64-bit
        VM Guests on KVM. Scenarios where <code class="command">qemu-system-i386</code>
        performs better are not supported by SUSE.
      </p><p>
        Xen also uses binaries from the qemu package but prefers
        <code class="command">qemu-system-i386</code>, which can be used for both 32-bit
        and 64-bit Xen VM Guests. To maintain compatibility with the upstream
        Xen Community, SUSE encourages using
        <code class="command">qemu-system-i386</code> for Xen VM Guests.
      </p></section></section><section class="sect1" id="sec-vt-best-hostlevel" data-id-title="VM Host Server configuration and resource allocation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">VM Host Server configuration and resource allocation</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-hostlevel">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Allocation of resources for VM Guests is a crucial point when
      administrating virtual machines. When assigning resources to VM Guests,
      be aware that overcommitting resources may affect the performance of the
      VM Host Server and the VM Guests. If all VM Guests request all their resources
      simultaneously, the host needs to provide them all. If not, the host's
      performance is negatively affected and this in turn also has negative
      effects on the VM Guest's performance.
    </p><section class="sect2" id="sec-vt-best-mem" data-id-title="Memory"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Memory</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Linux manages memory in units called pages. On most systems the default
        page size is 4 KB. Linux and the CPU need to know which pages belong to
        which process. That information is stored in a page table. If a lot of
        processes are running, it takes more time to find where the memory is
        mapped, because of the time required to search the page table. To speed
        up the search, the TLB (Translation Lookaside Buffer) was invented. But
        on a system with a lot of memory, the TLB is not enough. To avoid any
        fallback to normal page table (resulting in a cache miss, which is time
        consuming), huge pages can be used. Using huge pages will reduce TLB
        overhead and TLB misses (pagewalk). A host with 32 GB
        (32*1014*1024 = 33,554,432 KB) of memory and a 4 KB page size
        has a TLB with <span class="emphasis"><em>33,554,432/4 = 8,388,608</em></span> entries.
        Using a 2 MB (2048 KB) page size, the TLB only has
        <span class="emphasis"><em>33554432/2048 = 16384</em></span> entries, considerably
        reducing the TLB misses.
      </p><section class="sect3" id="sec-vt-best-mem-huge-pages" data-id-title="Configuring the VM Host Server and the VM Guest to use huge pages"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.1 </span><span class="title-name">Configuring the VM Host Server and the VM Guest to use huge pages</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-huge-pages">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The AMD64/Intel 64 CPU architecture supports larger pages than 4 KB: huge
          pages. To determine the size of huge pages available on your system
          (could be 2 MB or 1 GB), check the <code class="literal">flags</code>
          line in the output of <code class="filename">/proc/cpuinfo</code> for
          occurrences of <code class="literal">pse</code> and/or
          <code class="literal">pdpe1gb</code>.
        </p><div class="table" id="id-1.13.4.6.3.3.3" data-id-title="Determine the available huge pages size"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 1: </span><span class="title-name">Determine the available huge pages size </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.3.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 30%; " class="1"/><col style="width: 70%; " class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    CPU flag
                  </p>
                </th><th style="border-bottom: 1px solid ; ">
                  <p>
                    Huge pages size available
                  </p>
                </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Empty string
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    No huge pages available
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    pse
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    2 MB
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; ">
                  <p>
                    pdpe1gb
                  </p>
                </td><td>
                  <p>
                    1 GB
                  </p>
                </td></tr></tbody></table></div></div><p>
          Using huge pages improves the performance of VM Guests and reduces
          host memory consumption.
        </p><p>
          By default, the system uses THP. To make huge pages available on your
          system, activate it at boot time with <code class="option">hugepages=1</code>,
          and—optionally—add the huge pages size with, for example,
          <code class="option">hugepagesz=2MB</code>.
        </p><div id="id-1.13.4.6.3.3.6" data-id-title="1 GB huge pages" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: 1 GB huge pages</div><p>
            1 GB pages can only be allocated at boot time and cannot be
            freed afterward.
          </p></div><p>
          To allocate and use the huge page table (HugeTlbPage), you need to
          mount <code class="filename">hugetlbfs</code> with correct permissions.
        </p><div id="id-1.13.4.6.3.3.8" data-id-title="Restrictions of huge pages" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Restrictions of huge pages</div><p>
            Even if huge pages provide the best performance, they do come with
            some drawbacks. You lose features such as Memory ballooning (see
            <a class="xref" href="#sec-vt-best-vmguests-virtio-balloon" title="6.1.3. virtio balloon">Section 6.1.3, “virtio balloon”</a>), KSM (see
            <a class="xref" href="#sec-vt-best-perf-ksm" title="4.1.4. KSM and page sharing">Section 4.1.4, “KSM and page sharing”</a>), and huge pages cannot be
            swapped.
          </p></div><div class="procedure" id="id-1.13.4.6.3.3.9" data-id-title="Configuring the use of huge pages"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 2: </span><span class="title-name">Configuring the use of huge pages </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.3.3.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Mount <code class="literal">hugetlbfs</code> to
              <code class="filename">/dev/hugepages</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  mount -t hugetlbfs hugetlbfs /dev/hugepages</pre></div></li><li class="step"><p>
              To reserve memory for huge pages, use the
              <code class="command">sysctl</code> command. If your system has a huge page
              size of 2 MB (2048 KB), and you want to reserve
              1 GB (1,048,576 KB) for your VM Guest, you need
              <span class="emphasis"><em>1,048,576/2048=512</em></span> pages in the pool:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sysctl vm.nr_hugepages=<em class="replaceable">512</em></pre></div><p>
              The value is written to
              <code class="filename">/proc/sys/vm/nr_hugepages</code> and represents the
              current number of <span class="emphasis"><em>persistent</em></span> huge pages in
              the kernel's huge page pool. <span class="emphasis"><em>Persistent</em></span> huge
              pages are returned to the huge page pool when freed by a task.
            </p></li><li class="step"><p>
              Add the <code class="literal">memoryBacking</code> element in the VM Guest
              configuration file (by running <code class="command">virsh edit
              <em class="replaceable">CONFIGURATION</em></code>).
            </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</pre></div></li><li class="step"><p>
              Start your VM Guest and check on the host whether it uses
              hugepages:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/meminfo | grep HugePages_
HugePages_Total:<span class="callout" id="co-hp-total">1</span>     512
HugePages_Free:<span class="callout" id="co-hp-free">2</span>       92
HugePages_Rsvd:<span class="callout" id="co-hp-rsvd">3</span>        0
HugePages_Surp:<span class="callout" id="co-hp-surp">4</span>        0</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-total"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Size of the pool of huge pages
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-free"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Number of huge pages in the pool that are not yet allocated
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-rsvd"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Number of huge pages for which a commitment to allocate from
                  the pool has been made, but no allocation has yet been made
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-hp-surp"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Number of huge pages in the pool above the value in
                  <code class="filename">/proc/sys/vm/nr_hugepages</code>. The maximum
                  number of surplus huge pages is controlled by
                  <code class="filename">/proc/sys/vm/nr_overcommit_hugepages</code>
                </p></td></tr></table></div></li></ol></div></div></section><section class="sect3" id="sec-vt-best-mem-thp" data-id-title="Transparent huge pages"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.2 </span><span class="title-name">Transparent huge pages</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-thp">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Transparent huge pages (THP) provide a way to dynamically allocate
          huge pages with the <code class="command">khugepaged</code> kernel thread,
          rather than manually managing their allocation and use. Workloads
          with contiguous memory access patterns can benefit greatly from THP.
          A 1000 fold decrease in page faults can be observed when running
          synthetic workloads with contiguous memory access patterns.
          Conversely, workloads with sparse memory access patterns (like
          databases) may perform poorly with THP. In such cases, it may be
          preferable to disable THP by adding the kernel parameter
          <code class="option">transparent_hugepage=never</code>, rebuild your grub2
          configuration, and reboot. Verify if THP is disabled with:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</pre></div><p>
          If disabled, the value <code class="literal">never</code> is shown in square
          brackets like in the example above.
        </p><div id="id-1.13.4.6.3.4.5" data-id-title="Xen" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Xen</div><p>
            THP is not available under Xen.
          </p></div></section><section class="sect3" id="sec-vt-best-mem-xen" data-id-title="Xen-specific memory notes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.3 </span><span class="title-name">Xen-specific memory notes</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-xen">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect4" id="sec-vt-best-mem-xen-dom-0" data-id-title="Managing domain-0 memory"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.1.3.1 </span><span class="title-name">Managing domain-0 memory</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-xen-dom-0">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            In previous versions of SUSE Linux Enterprise Server, the default memory allocation scheme
            of a Xen host was to allocate all host physical memory to Dom0
            and enable auto-ballooning. Memory was automatically ballooned from
            Dom0 when additional domains were started. This behavior has
            always been error prone and disabling it was strongly encouraged.
            Starting in SUSE Linux Enterprise Server 15 SP1, auto-ballooning has been disabled by
            default and Dom0 is given 10% of host physical memory +
            1 GB. For example, on a host with 32 GB of physical
            memory, 4.2 GB of memory is allocated to Dom0.
          </p><p>
            The use of <code class="option">dom0_mem</code> Xen command-line option in
            <code class="filename">/etc/default/grub</code> is still supported and
            encouraged (see <a class="xref" href="#sec-vt-best-kernel-parameter" title="7.5. Change kernel parameters at boot time">Section 7.5, “Change kernel parameters at boot time”</a> for
            more information). You can restore the old behavior by setting
            <code class="option">dom0_mem</code> to the host physical memory size and
            enabling the <code class="option">autoballoon</code> setting in
            <code class="filename">/etc/xen/xl.conf</code>.
          </p></section></section><section class="sect3" id="sec-vt-best-perf-ksm" data-id-title="KSM and page sharing"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.4 </span><span class="title-name">KSM and page sharing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-ksm">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Kernel Samepage Merging is a kernel feature that reduces memory
          consumption on the VM Host Server by sharing blocks of memory that
          VM Guests have in common. The KSM daemon
          <code class="systemitem">ksmd</code> periodically scans
          user memory, looking for pages with identical contents, which can be
          replaced by a single write-protected page. To enable the KSM service,
          first make sure that the package <span class="package">qemu-ksm</span> is
          installed, then run the command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  systemctl enable --now ksm.service</pre></div><p>
          Alternatively, it can also be started by running the command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 1 &gt; /sys/kernel/mm/ksm/run</pre></div><p>
          One advantage of using KSM from a VM Guest's perspective is that all
          guest memory is backed by host anonymous memory. You can share
          <span class="emphasis"><em>pagecache</em></span>, <span class="emphasis"><em>tmpfs</em></span> or any
          kind of memory allocated in the guest.
        </p><p>
          KSM is controlled by <code class="systemitem">sysfs</code>. You can check
          KSM's values in <code class="filename">/sys/kernel/mm/ksm/</code>:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <code class="literal">pages_shared</code>: the number of shared pages that
              are being used (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">pages_sharing</code>: the number of sites sharing the
              pages (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">pages_unshared</code>: the number of pages that are
              unique and repeatedly checked for merging (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">pages_volatile</code>: the number of pages that are
              changing too fast to be considered for merging (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">full_scans</code>: the number of times all mergeable
              areas have been scanned (read-only).
            </p></li><li class="listitem"><p>
              <code class="literal">sleep_millisecs</code>: the number of milliseconds
              <code class="systemitem">ksmd</code> should sleep before
              the next scan. A low value will overuse the CPU, consuming CPU
              time that could be used for other tasks. We recommend a value
              greater than <code class="literal">1000</code>.
            </p></li><li class="listitem"><p>
              <code class="literal">pages_to_scan</code>: the number of present pages to
              scan before ksmd goes to sleep. A high value will overuse the
              CPU. We recommend starting with a value of
              <code class="literal">1000</code> and then adjusting based on the KSM
              results observed while testing your deployment.
            </p></li><li class="listitem"><p>
              <code class="literal">merge_across_nodes</code>: by default, the system
              merges pages across NUMA nodes. Set this option to
              <code class="literal">0</code> to disable this behavior.
            </p></li></ul></div><div id="id-1.13.4.6.3.6.9" data-id-title="Use cases" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Use cases</div><p>
            KSM is a good technique to over-commit host memory when running
            multiple instances of the same application or VM Guest. When
            applications and VM Guest are heterogeneous and do not share any
            common data, it is preferable to disable KSM. To do that, run:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  systemctl disable --now ksm.service</pre></div><p>
            Alternatively, it can also be disabled by running the command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 0 &gt; /sys/kernel/mm/ksm/run</pre></div><p>
            In a mixed heterogeneous and homogeneous environment, KSM can be
            enabled on the host but disabled on a per VM Guest basis. Use
            <code class="command">virsh edit</code> to disable page sharing of a
            VM Guest by adding the following to the guest's XML configuration:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</pre></div></div><div id="id-1.13.4.6.3.6.10" data-id-title="Avoid out-of-memory conditions" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Avoid out-of-memory conditions</div><p>
            KSM can free up certain memory on the host system, but the
            administrator should reserve enough swap to avoid out-of-memory
            conditions if that shareable memory decreases. If the amount of
            shareable memory decreases, the use of physical memory is
            increased.
          </p></div><div id="id-1.13.4.6.3.6.11" data-id-title="KSM as a side channel" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: KSM as a side channel</div><p>
            Because of its nature, KSM can form a side channel between otherwise
            isolated guests.
            It is discouraged to enable KSM in environments where guests from
            different security domains are executed.
          </p></div><div id="id-1.13.4.6.3.6.12" data-id-title="Memory access latencies" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Memory access latencies</div><p>
            By default, KSM will merge common pages across NUMA nodes. If the
            merged, common page is now located on a distant NUMA node (relative
            to the node running the VM Guest vCPUs), this may degrade
            VM Guest performance. If increased memory access latencies are
            noticed in the VM Guest, disable cross-node merging with the
            <code class="literal">merge_across_nodes</code> sysfs control:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 0 &gt; /sys/kernel/mm/ksm/merge_across_nodes</pre></div></div></section><section class="sect3" id="sec-vt-best-mem-hot" data-id-title="VM Guest: memory hotplug"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.1.5 </span><span class="title-name">VM Guest: memory hotplug</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-mem-hot">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          To optimize the usage of your host memory, it may be useful to
          hotplug more memory for a running VM Guest when required. To support
          memory hotplugging, you must first configure the
          <code class="literal">&lt;maxMemory&gt;</code> tag in the VM Guest's
          configuration file:
        </p><div class="verbatim-wrap"><pre class="screen">&lt;maxMemory<span class="callout" id="co-mem-hot-max">1</span> slots='16'<span class="callout" id="co-mem-hot-slots">2</span> unit='KiB'&gt;20971520<span class="callout" id="co-mem-hot-size">3</span>&lt;/maxMemory&gt;
  &lt;memory<span class="callout" id="co-mem-hot-mem">4</span> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<span class="callout" id="co-mem-hot-curr">5</span> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-max"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Runtime maximum memory allocation of the guest
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-slots"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Number of slots available for adding memory to the guest
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-size"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Valid units are:
            </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                  <span class="quote">“<span class="quote">KB</span>”</span> for kilobytes (1,000 bytes)
                </p></li><li class="listitem"><p>
                  <span class="quote">“<span class="quote">k</span>”</span> or <span class="quote">“<span class="quote">KiB</span>”</span> for kibibytes (1,024
                  bytes)
                </p></li><li class="listitem"><p>
                  <span class="quote">“<span class="quote">MB</span>”</span> for megabytes (1,000,000 bytes)
                </p></li><li class="listitem"><p>
                  <span class="quote">“<span class="quote">M</span>”</span> or <span class="quote">“<span class="quote">MiB</span>”</span> for mebibytes
                  (1,048,576 bytes)
                </p></li><li class="listitem"><p>
                  <span class="quote">“<span class="quote">GB</span>”</span> for gigabytes (1,000,000,000 bytes)
                </p></li><li class="listitem"><p>
                  <span class="quote">“<span class="quote">G</span>”</span> or <span class="quote">“<span class="quote">GiB</span>”</span> for gibibytes
                  (1,073,741,824 bytes)
                </p></li><li class="listitem"><p>
                  <span class="quote">“<span class="quote">TB</span>”</span> for terabytes (1,000,000,000,000 bytes)
                </p></li><li class="listitem"><p>
                  <span class="quote">“<span class="quote">T</span>”</span> or <span class="quote">“<span class="quote">TiB</span>”</span> for tebibytes
                  (1,099,511,627,776 bytes)
                </p></li></ul></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-mem"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum allocation of memory for the guest at boot time
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hot-curr"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Actual allocation of memory for the guest
            </p></td></tr></table></div><p>
          To hotplug memory devices into the slots, create a file
          <code class="filename">mem-dev.xml</code> like the following:
        </p><div class="verbatim-wrap"><pre class="screen">&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'&gt;524287&lt;/size&gt;
  &lt;node&gt;0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</pre></div><p>
          And attach it with the following command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh attach-device vm-name mem-dev.xml</pre></div><p>
          For memory device hotplug, the guest must have at least 1 NUMA cell
          defined (see <a class="xref" href="#sec-vt-best-perf-numa-vmguest-topo" title="4.6.3.1. VM Guest virtual NUMA topology">Section 4.6.3.1, “VM Guest virtual NUMA topology”</a>).
        </p></section></section><section class="sect2" id="sec-vt-best-perf-swap" data-id-title="Swap"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Swap</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-swap">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        <span class="emphasis"><em>Swap</em></span> is used by the system to store underused
        physical memory (low usage, or not accessed for a long time). To
        prevent the system from running out of memory, setting up a minimum
        swap is highly recommended.
      </p><section class="sect3" id="sec-vt-best-perf-swap-swappiness" data-id-title="swappiness"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.2.1 </span><span class="title-name"><code class="literal">swappiness</code></span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-swap-swappiness">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The <code class="literal">swappiness</code> setting controls your system's swap
          behavior. It defines how memory pages are swapped to disk. A high
          value of <span class="emphasis"><em>swappiness</em></span> results in a system that
          swaps more often. Available values range from <code class="literal">0</code> to
          <code class="literal">200</code>. A value of <code class="literal">200</code> tells the
          system to find inactive pages and put them in swap. A value of
          <code class="option">0</code> disables swapping.
          
        </p><p>
          To test on a live system, change the value of
          <code class="filename">/proc/sys/vm/swappiness</code> on the fly and check the
          memory usage afterward:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 35 &gt; /proc/sys/vm/swappiness</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>free -h
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</pre></div><p>
          To permanently set a swappiness value, add a line in
          <code class="filename">/etc/systcl.conf</code>, for example:
        </p><div class="verbatim-wrap"><pre class="screen">vm.swappiness = 35</pre></div><p>
          You can also control the swap by using the
          <code class="literal">swap_hard_limit</code> element in the XML configuration
          of your VM Guest. Before setting this parameter and using it in a
          production environment, test it because the host can stop the domain
          if the value is too low.
        </p><div class="verbatim-wrap"><pre class="screen">&lt;memtune&gt;<span class="callout" id="co-mem-1">1</span>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<span class="callout" id="co-mem-hard">2</span>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<span class="callout" id="co-mem-soft">3</span>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<span class="callout" id="co-mem-swap">4</span>
&lt;/memtune&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-1"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              This element provides memory tunable parameters for the domain.
              If this is omitted, it defaults to the defaults provided b the
              operating system.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-hard"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum memory the guest can use. To avoid any problems on the
              VM Guest, we strongly recommend not to use this parameter.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-soft"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The memory limit to enforce during memory contention.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-mem-swap"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The maximum memory plus swap the VM Guest can use.
            </p></td></tr></table></div></section></section><section class="sect2" id="sec-vt-best-io" data-id-title="I/O"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">I/O</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-io">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect3" id="sec-vt-best-perf-io" data-id-title="I/O scheduler"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.1 </span><span class="title-name">I/O scheduler</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-io">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The I/O scheduler for SUSE Linux Enterprise 15 SP2 and up is Budget Fair Queueing
          (BFQ). The main aim of the BFQ scheduler is to provide a fair
          allocation of the disk I/O bandwidth for all processes that request
          an I/O operation. You can have different I/O schedulers for different
          devices.
        </p><p>
          To get better performance in host and VM Guest, use
          <code class="literal">none</code> in the VM Guest (disable the I/O scheduler)
          and the <code class="literal">mq-deadline</code> scheduler for a virtualization
          host.
        </p><div class="procedure" id="id-1.13.4.6.5.3.4" data-id-title="Checking and changing the I/O scheduler at runtime"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3: </span><span class="title-name">Checking and changing the I/O scheduler at runtime </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To check your current I/O scheduler for your disk (replace
              <em class="replaceable">sdX</em> by the disk you want to check),
              run:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/block/<em class="replaceable">sdX</em>/queue/scheduler
mq-deadline kyber [bfq] none</pre></div><p>
              The value in square brackets is the one currently selected
              (<code class="literal">bfq</code> in the example above).
            </p></li><li class="step"><p>
              You can change the scheduler at runtime by running the following
              command as <code class="systemitem">root</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo mq-deadline &gt; /sys/block/<em class="replaceable">sdX</em>/queue/scheduler</pre></div></li></ol></div></div><p>
          If you need to specify different I/O schedulers for each disk, create
          the file <code class="filename">/usr/lib/tmpfiles.d/IO_ioscheduler.conf</code>
          with content similar to the following example. It defines the
          <code class="literal">mq-deadline</code> scheduler for
          <code class="filename">/dev/sda</code> and the <code class="literal">none</code>
          scheduler for <code class="filename">/dev/sdb</code>. Keep in mind that the
          device name can be different depending on the device type. This
          feature is available on SLE 12 and up.
        </p><div class="verbatim-wrap"><pre class="screen">w /sys/block/sda/queue/scheduler - - - - mq-deadline
w /sys/block/sdb/queue/scheduler - - - - none</pre></div></section><section class="sect3" id="sec-vt-best-io-async" data-id-title="Asynchronous I/O"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.2 </span><span class="title-name">Asynchronous I/O</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-io-async">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Many of the virtual disk back-ends use Linux Asynchronous I/O (aio)
          in their implementation. By default, the maximum number of aio
          contexts is set to 65536, which can be exceeded when running hundreds
          of VM Guests using virtual disks serviced by Linux Asynchronous I/O.
          When running large numbers of VM Guests on a VM Host Server, consider
          increasing /proc/sys/fs/aio-max-nr.
        </p><div class="procedure" id="id-1.13.4.6.5.4.3" data-id-title="Checking and changing aio-max-nr at runtime"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4: </span><span class="title-name">Checking and changing aio-max-nr at runtime </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To check your current aio-max-nr setting run:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/sys/fs/aio-max-nr
65536</pre></div></li><li class="step"><p>
              You can change aio-max-nr at runtime with the following command:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> echo 131072 &gt; /proc/sys/fs/aio-max-nr</pre></div></li></ol></div></div><p>
          To permanently set <code class="option">aio-max-nr</code>, add an entry to a
          custom sysctl file. For example, include the following to
          <code class="filename">/etc/sysctl.d/aio-max-nr.conf</code>:
        </p><div class="verbatim-wrap"><pre class="screen">fs.aio-max-nr = 1048576</pre></div></section><section class="sect3" id="sec-vt-best-io-techniques" data-id-title="I/O Virtualization"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.3.3 </span><span class="title-name">I/O Virtualization</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-io-techniques">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          SUSE products support multiple I/O virtualization technologies. The
          following table lists advantages and disadvantages of each
          technology. For more information about I/O in virtualization refer to
          the <span class="intraxref">Book “Virtualization Guide”, Chapter 1 “Virtualization technology”, Section 1.4 “I/O virtualization”</span>.
        </p><div class="table" id="id-1.13.4.6.5.5.3" data-id-title="I/O Virtualization solutions"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2: </span><span class="title-name">I/O Virtualization solutions </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 20%; " class="1"/><col style="width: 40%; " class="2"/><col style="width: 40%; " class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Technology
                  </p>
                </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Advantage
                  </p>
                </th><th style="border-bottom: 1px solid ; ">
                  <p>
                    Disadvantage
                  </p>
                </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="4">
                  <p>
                    Device Assignment (pass-through)
                  </p>
                </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Device accessed directly by the guest
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    No sharing among multiple guests
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    High performance
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Live migration is complex
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    PCI device limit is 8 per guest
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Limited number of slots on a server
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
                  <p>
                    Full virtualization (IDE, SATA, SCSI, e1000)
                  </p>
                </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    VM Guest compatibility
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Bad performance
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Easy for live migration
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Emulated operation
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; " rowspan="3">
                  <p>
                    Para-virtualization (virtio-blk, virtio-net, virtio-scsi)
                  </p>
                </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Good performance
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  <p>
                    Modified guest (PV drivers)
                  </p>
                </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                  <p>
                    Easy for live migration
                  </p>
                </td><td style="border-bottom: 1px solid ; ">
                  
                </td></tr><tr><td style="border-right: 1px solid ; ">
                  <p>
                    Efficient host communication with VM Guest
                  </p>
                </td><td>
                  
                </td></tr></tbody></table></div></div></section></section><section class="sect2" id="sec-vt-best-fs" data-id-title="Storage and file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">Storage and file system</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-fs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Storage space for VM Guests can either be a block device, for example,
        a partition on a physical disk, or an image file on the file system:
      </p><div class="table" id="id-1.13.4.6.6.3" data-id-title="Block devices compared to disk images"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 3: </span><span class="title-name">Block devices compared to disk images </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.6.6.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="width: 10%; " class="1"/><col style="width: 45%; " class="2"/><col style="width: 45%; " class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <p>
                  Technology
                </p>
              </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <p>
                  Advantages
                </p>
              </th><th style="border-bottom: 1px solid ; ">
                <p>
                  Disadvantages
                </p>
              </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <p>
                  Block devices
                </p>
              </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Better performance
                    </p></li><li class="listitem"><p>
                      Use standard tools for administration/disk modification
                    </p></li><li class="listitem"><p>
                      Accessible from host (pro and con)
                    </p></li></ul></div>
              </td><td style="border-bottom: 1px solid ; ">
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Device management
                    </p></li></ul></div>
              </td></tr><tr><td style="border-right: 1px solid ; ">
                <p>
                  Image files
                </p>
              </td><td style="border-right: 1px solid ; ">
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Easier system management
                    </p></li><li class="listitem"><p>
                      Easily move, clone, expand, back up domains
                    </p></li><li class="listitem"><p>
                      Comprehensive toolkit (guestfs) for image manipulation
                    </p></li><li class="listitem"><p>
                      Reduce overhead through sparse files
                    </p></li><li class="listitem"><p>
                      Fully allocate for best performance
                    </p></li></ul></div>
              </td><td>
                <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                      Lower performance than block devices
                    </p></li></ul></div>
              </td></tr></tbody></table></div></div><p>
        For detailed information about image formats and maintaining images,
        refer to <a class="xref" href="#sec-vt-best-img" title="5. VM Guest images">Section 5, “VM Guest images”</a>.
      </p><p>
        If your image is stored on an NFS share, check certain server and
        client parameters to improve access to the VM Guest image.
      </p><section class="sect3" id="sec-vt-best-fs-nfs-rw" data-id-title="NFS read/write (client)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.1 </span><span class="title-name">NFS read/write (client)</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-fs-nfs-rw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Options <code class="option">rsize</code> and <code class="option">wsize</code> specify the
          size of the chunks of data that the client and server pass back and
          forth to each other. You should ensure NFS read/write sizes are
          sufficiently large, especially for large I/O. Change the
          <code class="option">rsize</code> and <code class="option">wsize</code> parameter in your
          <code class="filename">/etc/fstab</code> by increasing the value to 16 KB.
          This will ensure that all operations can be frozen if there is any
          instance of hanging.
        </p><div class="verbatim-wrap"><pre class="screen">nfs_server:/exported/vm_images<span class="callout" id="co-nfs-server">1</span> /mnt/images<span class="callout" id="co-nfs-mnt">2</span> nfs<span class="callout" id="co-nfs-nfs">3</span> rw<span class="callout" id="co-nfs-rw">4</span>,hard<span class="callout" id="co-nfs-hard">5</span>,sync<span class="callout" id="co-nfs-sync">6</span>, rsize=8192<span class="callout" id="co-nfs-rsize">7</span>,wsize=8192<span class="callout" id="co-nfs-wsize">8</span> 0 0</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-server"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              NFS server's host name and export path.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-mnt"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Where to mount the NFS exported share.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-nfs"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              This is an <code class="option">nfs</code> mount point.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-rw"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              This mount point will be accessible in read/write.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-hard"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Determines the recovery behavior of the NFS client after an NFS
              request times out. <code class="option">hard</code> is the best option to
              avoid data corruption.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-sync"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Any system call that writes data to files on that mount point
              causes that data to be flushed to the server before the system
              call returns control to user space.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-rsize"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum number of bytes in each network READ request that the NFS
              client can receive when reading data from a file on an NFS
              server.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-wsize"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum number of bytes per network WRITE request that the NFS
              client can send when writing data to a file on an NFS server.
            </p></td></tr></table></div></section><section class="sect3" id="sec-vt-best-fs-nfs-threads" data-id-title="NFS threads (server)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.2 </span><span class="title-name">NFS threads (server)</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-fs-nfs-threads">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Your NFS server should have enough NFS threads to handle
          multi-threaded workloads. Use the <code class="command">nfsstat</code> tool to
          get RPC statistics on your server:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</pre></div><p>
          If the <code class="literal">retrans</code> is equal to 0, everything is fine.
          Otherwise, the client needs to retransmit, so increase the
          <code class="envar">USE_KERNEL_NFSD_NUMBER</code> variable in
          <code class="filename">/etc/sysconfig/nfs</code>, and adjust accordingly until
          <code class="literal">retrans</code> is equal to <code class="literal">0</code>.
        </p></section></section><section class="sect2" id="sec-vt-best-perf-cpu" data-id-title="CPUs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.5 </span><span class="title-name">CPUs</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Host CPU <span class="quote">“<span class="quote">components</span>”</span> will be <span class="quote">“<span class="quote">translated</span>”</span> to
        virtual CPUs in a VM Guest when being assigned. These components can
        either be:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            <span class="emphasis"><em>CPU processor</em></span>: this describes the main CPU
            unit, which normally has multiple cores and may support
            Hyper-Threading.
          </p></li><li class="listitem"><p>
            <span class="emphasis"><em>CPU core</em></span>: a main CPU unit can provide more
            than one core, and the proximity of cores speeds up the computation
            process and reduces energy costs.
          </p></li><li class="listitem"><p>
            <span class="emphasis"><em>CPU Hyper-Threading</em></span>: this implementation is
            used to improve the parallelization of computations, but this is
            not as efficient as a dedicated core.
          </p></li></ul></div><section class="sect3" id="sec-vt-best-perf-cpu-assign" data-id-title="Assigning CPUs"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.5.1 </span><span class="title-name">Assigning CPUs</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-assign">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          CPU overcommit occurs when the cumulative number of virtual CPUs of
          all VM Guests becomes higher than the number of host CPUs. Best
          performance is achieved when there is no overcommit and each virtual
          CPU matches one hardware processor or core on the VM Host Server. VM Guests
          running on an overcommitted host will experience increased latency and
          a negative effect on per-VM Guest throughput is often observed.
          Therefore, try to avoid overcommitting CPUs.
        </p><p>
          Deciding whether to allow CPU overcommit or not requires good a
          priori knowledge of workload as a whole. For example, if you know
          that all the VM Guests' virtual CPUs will not be loaded over 50%,
          then you can assume that overcommitting the host by a factor of 2
          (which means having 128 virtual CPUs in total, on a host with 64
          CPUs) will work well. However, if you know that all the virtual CPUs
          of the VM Guests will try to run at 100% for most of the time, then
          even having one virtual CPU more than the host has CPUs is already a
          misconfiguration.
        </p><p>
          Overcommitting to a point where the cumulative number of virtual CPUs
          is higher than 8 times the number of physical cores of the VM Host Server
          may lead to a malfunctioning and unstable system and should hence be
          avoided.
        </p><p>
          Unless you know exactly how many virtual CPUs are required for a
          VM Guest, start with one. Target a CPU workload of approximately 70%
          inside your VM (see <span class="intraxref">Book “System Analysis and Tuning Guide”, Chapter 2 “System monitoring utilities”, Section 2.3 “Processes”</span> for
          information on monitoring tools). If you allocate more processors
          than needed in the VM Guest, this will negatively affect the
          performance of host and guest. Cycle efficiency will be degraded, as
          the unused vCPU will still cause timer interrupts. In case you
          primarily run single threaded applications on a VM Guest, a single
          virtual CPU is the best choice.
        </p><p>
          A single VM Guest with more virtual CPUs than the VM Host Server has CPUs
          is always a misconfiguration.
        </p></section><section class="sect3" id="sec-vt-best-perf-cpu-guests" data-id-title="VM Guest CPU configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.5.2 </span><span class="title-name">VM Guest CPU configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          This section describes how to choose and configure a CPU type for a
          VM Guest. You will also learn how to pin virtual CPUs to physical
          CPUs on the host system. For more information about virtual CPU
          configuration and tuning parameters, refer to the libvirt
          documentation at
          <a class="link" href="https://libvirt.org/formatdomain.html#elementsCPU" target="_blank">https://libvirt.org/formatdomain.html#elementsCPU</a>.
        </p><section class="sect4" id="sec-vt-best-perf-cpu-guests-model" data-id-title="Virtual CPU models and features"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.1 </span><span class="title-name">Virtual CPU models and features</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests-model">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            The CPU model and topology can be specified individually for each
            VM Guest. Configuration options range from selecting specific CPU
            models to excluding certain CPU features. Predefined CPU models are
            listed in files in the directory
            <code class="filename">/usr/share/libvirt/cpu_map/</code>. A CPU model and
            topology that is similar to the host generally provides the best
            performance. The host system CPU model and topology can be
            displayed by running <code class="command">virsh capabilities</code>.
          </p><p>
            Changing the default virtual CPU configuration will require a
            VM Guest shutdown when migrating it to a host with different
            hardware. More information on VM Guest migration is available in
            <span class="intraxref">Book “Virtualization Guide”, Chapter 17 “Migrating VM Guests”</span>.
          </p><p>
            To specify a particular CPU model for a VM Guest, add a respective
            entry to the VM Guest configuration file. The following example
            configures a Broadwell CPU with the invariant TSC feature:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
  &lt;feature name='invtsc'/&gt;
  &lt;/cpu&gt;</pre></div><p>
            For a virtual CPU that most closely resembles the host physical CPU,
            <code class="literal">&lt;cpu mode='host-passthrough'&gt;</code> can be used.
            A <code class="literal">host-passthrough</code> CPU model may not exactly
            resemble the host physical CPU, since, by default, KVM will mask any
            non-migratable features. For example, invtsc is not included in the
            virtual CPU feature set. Changing the default KVM behavior is not
            directly supported through libvirt, although it does allow arbitrary
            pass-through of KVM command-line arguments. Continuing with the
            <code class="literal">invtsc</code> example, you can achieve pass-through of
            the host CPU (including
            <code class="literal">invtsc</code>) with the following command-line
            pass-through in the VM Guest configuration file:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
     &lt;qemu:commandline&gt;
     &lt;qemu:arg value='-cpu'/&gt;
     &lt;qemu:arg value='host,migratable=off,+invtsc'/&gt;
     &lt;/qemu:commandline&gt;
     ...
     &lt;/domain&gt;</pre></div><div id="id-1.13.4.6.7.5.3.8" data-id-title="The host-passthrough mode" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: The <code class="literal">host-passthrough</code> mode</div><p>
              Since <code class="literal">host-passthrough</code> exposes the physical
              CPU details to the virtual CPU, migration to dissimilar hardware
              is not possible. See
              <a class="xref" href="#sec-vt-best-perf-cpu-guests-vcpumigration" title="4.5.2.3. Virtual CPU migration considerations">Section 4.5.2.3, “Virtual CPU migration considerations”</a> for
              more information.
            </p></div></section><section class="sect4" id="sec-vt-best-perf-cpu-guests-vcpupin" data-id-title="Virtual CPU pinning"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.2 </span><span class="title-name">Virtual CPU pinning</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests-vcpupin">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            Virtual CPU pinning is used to constrain virtual CPU threads to a
            set of physical CPUs. The <code class="literal">vcpupin</code> element
            specifies the physical host CPUs that a virtual CPU can use. If
            this element is not set and the attribute <code class="literal">cpuset</code>
            of the <code class="literal">vcpu</code> element is not specified, the
            virtual CPU is free to use any of the physical CPUs.
          </p><p>
            CPU intensive workloads can benefit from virtual CPU pinning by
            increasing the physical CPU cache hit ratio. To pin a virtual CPU
            to a specific physical CPU, run the following commands:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh vcpupin <em class="replaceable">DOMAIN_ID</em> --vcpu <em class="replaceable">vCPU_NUMBER</em>
VCPU: CPU Affinity
----------------------------------
0: 0-7
<code class="prompt root"># </code>virsh vcpupin SLE15 --vcpu 0 0 --config</pre></div><p>
            The last command generates the following entry in the XML
            configuration:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</pre></div><div id="id-1.13.4.6.7.5.4.7" data-id-title="Virtual CPU pinning on NUMA nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Virtual CPU pinning on NUMA nodes</div><p>
              To confine a VM Guest's CPUs and its memory to a NUMA node, you
              can use virtual CPU pinning and memory allocation policies on a
              NUMA system. See <a class="xref" href="#sec-vt-best-perf-numa" title="4.6. NUMA tuning">Section 4.6, “NUMA tuning”</a> for more
              information related to NUMA tuning.
            </p></div><div id="id-1.13.4.6.7.5.4.8" data-id-title="Virtual CPU pinning and live migration" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Virtual CPU pinning and live migration</div><p>
              Even though <code class="literal">vcpupin</code> can improve performance,
              it can complicate live migration. See
              <a class="xref" href="#sec-vt-best-perf-cpu-guests-vcpumigration" title="4.5.2.3. Virtual CPU migration considerations">Section 4.5.2.3, “Virtual CPU migration considerations”</a> for
              more information on virtual CPU migration considerations.
            </p></div></section><section class="sect4" id="sec-vt-best-perf-cpu-guests-vcpumigration" data-id-title="Virtual CPU migration considerations"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.5.2.3 </span><span class="title-name">Virtual CPU migration considerations</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cpu-guests-vcpumigration">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            Selecting a virtual CPU model containing all the latest features
            may improve performance of a VM Guest workload, but often at the
            expense of migratability. Unless all hosts in the cluster contain
            the latest CPU features, migration can fail when a destination host
            lacks the new features. If migratability of a virtual CPU is
            preferred over the latest CPU features, a normalized CPU model and
            feature set should be used. The <code class="command">virsh
            cpu-baseline</code> command can help define a normalized virtual
            CPU that can be migrated across all hosts. The following command,
            when run on each host in the migration cluster, illustrates the
            collection of all hosts' capabilities in
            <code class="literal">all-hosts-caps.xml</code>.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh capabilities &gt;&gt; all-hosts-cpu-caps.xml</pre></div><p>
            With the capabilities of each host collected in all-hosts-caps.xml,
            use <code class="command">virsh cpu-baseline</code> to create a virtual CPU
            definition that will be compatible across all hosts.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh cpu-baseline all-hosts-caps.xml</pre></div><p>
            The resulting virtual CPU definition can be used as the
            <code class="literal">cpu</code> element in the VM Guest configuration file.
          </p><p>
            At a logical level, virtual CPU pinning is a form of hardware
            pass-through. CPU pinning couples physical resources to virtual
            resources, which can also be problematic for migration. For
            example, the migration will fail if the requested physical
            resources are not available on the destination host, or if the
            source and destination hosts have different NUMA topologies. For
            more recommendations about Live Migration, see
            <span class="intraxref">Book “Virtualization Guide”, Chapter 17 “Migrating VM Guests”, Section 17.2 “Migration requirements”</span>.
          </p></section></section></section><section class="sect2" id="sec-vt-best-perf-numa" data-id-title="NUMA tuning"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.6 </span><span class="title-name">NUMA tuning</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        NUMA is an acronym for Non Uniform Memory Access. A NUMA system has
        multiple physical CPUs, each with local memory attached. Each CPU can
        also access other CPUs' memory, known as <span class="quote">“<span class="quote">remote memory
        access</span>”</span>, but it is much slower than accessing local memory. NUMA
        systems can negatively affect VM Guest performance if not tuned
        properly. Although ultimately tuning is workload dependent, this
        section describes controls that should be considered when deploying
        VM Guests on NUMA hosts. Always consider your host topology when
        configuring and deploying VMs.
      </p><p>
        <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> contains a NUMA auto-balancer that strives to reduce
        remote memory access by placing memory on the same NUMA node as the CPU
        processing it. Standard tools such as <code class="command">cgset</code> and
        virtualization tools such as libvirt provide mechanisms to constrain
        VM Guest resources to physical resources.
      </p><p>
        <code class="command">numactl</code> is used to check for host NUMA capabilities:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3
0:  10  21  21  21
1:  21  10  21  21
2:  21  21  10  21
3:  21  21  21  10</pre></div><p>
        The <code class="command">numactl</code> output shows this is a NUMA system with
        4 nodes or cells, each containing 36 CPUs and approximately 32G memory.
        <code class="command">virsh capabilities</code> can also be used to examine the
        systems NUMA capabilities and CPU topology.
      </p><section class="sect3" id="sec-vt-best-perf-numa-balancing" data-id-title="NUMA balancing"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.1 </span><span class="title-name">NUMA balancing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-balancing">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          On NUMA machines, there is a performance penalty if remote memory is
          accessed by a CPU. Automatic NUMA balancing scans a task's address
          space and unmaps pages. By doing so, it detects whether pages are
          properly placed or whether to migrate the data to a memory node local
          to where the task is running. In defined intervals (configured with
          <code class="literal">numa_balancing_scan_delay_ms</code>), the task scans the
          next scan size number of pages (configured with
          <code class="literal">numa_balancing_scan_size_mb</code>) in its address space.
          When the end of the address space is reached, the scanner restarts
          from the beginning.
        </p><p>
          Higher scan rates cause higher system overhead as page faults must be
          trapped and data needs to be migrated. However, the higher the scan
          rate, the more quickly a task's memory migrates to a local node when
          the workload pattern changes. This minimizes the performance impact
          caused by remote memory accesses. These <code class="command">sysctl</code>
          directives control the thresholds for scan delays and the number of
          pages scanned:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<span class="callout" id="co-numa-balancing">1</span>
kernel.numa_balancing_scan_delay_ms = 1000<span class="callout" id="co-numa-delay">2</span>
kernel.numa_balancing_scan_period_max_ms = 60000<span class="callout" id="co-numa-pmax">3</span>
kernel.numa_balancing_scan_period_min_ms = 1000<span class="callout" id="co-numa-pmin">4</span>
kernel.numa_balancing_scan_size_mb = 256<span class="callout" id="co-numa-size">5</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-balancing"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Enables/disables automatic page fault-based NUMA balancing
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-delay"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Starting scan delay used for a task when it initially forks
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-pmax"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Maximum time in milliseconds to scan a task's virtual memory
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-pmin"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Minimum time in milliseconds to scan a task's virtual memory
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-size"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              Size in megabytes' worth of pages to be scanned for a given scan
            </p></td></tr></table></div><p>
          For more information, see <span class="intraxref">Book “System Analysis and Tuning Guide”, Chapter 11 “Automatic Non-Uniform Memory Access (NUMA) balancing”</span>.
        </p><p>
          The main goal of automatic NUMA balancing is either to reschedule
          tasks on the same node's memory (so the CPU follows the memory), or
          to copy the memory's pages to the same node (so the memory follows
          the CPU).
        </p><div id="id-1.13.4.6.8.7.8" data-id-title="Task placement" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Task placement</div><p>
            There are no rules to define the best place to run a task, because
            tasks could share memory with other tasks. For the best
            performance, we recommend to group tasks sharing memory on the same
            node. Check NUMA statistics with <code class="command"># cat /proc/vmstat | grep
            numa_</code>.
          </p></div></section><section class="sect3" id="sec-vt-best-perf-numa-cpuset" data-id-title="Memory allocation control with the CPUset controller"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.2 </span><span class="title-name">Memory allocation control with the CPUset controller</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-cpuset">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The cgroups cpuset controller can be used confine memory used by a
          process to a NUMA node. There are three cpuset memory policy modes
          available:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <code class="literal">interleave</code>: this is a memory placement policy
              which is also known as round-robin. This policy can provide
              substantial improvements for jobs that need to place thread local
              data on the corresponding node. When the interleave destination
              is not available, it will be moved to another node.
            </p></li><li class="listitem"><p>
              <code class="literal">bind</code>: this will place memory only on one node,
              which means in case of insufficient memory, the allocation will
              fail.
            </p></li><li class="listitem"><p>
              <code class="literal">preferred</code>: this policy will apply a preference
              to allocate memory to a node. If there is not enough space for
              memory on this node, it will fall back to another node.
            </p></li></ul></div><p>
          You can change the memory policy mode with the
          <code class="command">cgset</code> tool from the
          <span class="package">libcgroup-tools</span> package:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  cgset -r cpuset.mems=<em class="replaceable">NODE</em> sysdefault/libvirt/qemu/<em class="replaceable">KVM_NAME</em>/emulator</pre></div><p>
          To migrate pages to a node, use the <code class="command">migratepages</code>
          tool:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>migratepages <em class="replaceable">PID</em> <em class="replaceable">FROM-NODE</em> <em class="replaceable">TO-NODE</em></pre></div><p>
          To check everything is fine. use: <code class="command">cat
          /proc/<em class="replaceable">PID</em>/status | grep Cpus</code>.
        </p><div id="id-1.13.4.6.8.8.9" data-id-title="Kernel NUMA/cpuset memory policy" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Kernel NUMA/cpuset memory policy</div><p>
            For more information, see
            <a class="link" href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt" target="_blank">Kernel
            NUMA memory policy</a> and
            <a class="link" href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt" target="_blank">cpusets
            memory policy</a>. Check also the
            <a class="link" href="https://libvirt.org/formatdomain.html#elementsNUMATuning" target="_blank">Libvirt
            NUMA Tuning documentation</a>.
          </p></div></section><section class="sect3" id="sec-vt-best-perf-numa-vmguest" data-id-title="VM Guest: NUMA related configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.6.3 </span><span class="title-name">VM Guest: NUMA related configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-vmguest">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          <code class="systemitem">libvirt</code> allows to set up virtual NUMA and memory access policies.
          Configuring these settings is not supported by
          <code class="command">virt-install</code> or <code class="command">virt-manager</code>
          and needs to be done manually by editing the VM Guest configuration
          file with <code class="command">virsh edit</code>.
        </p><section class="sect4" id="sec-vt-best-perf-numa-vmguest-topo" data-id-title="VM Guest virtual NUMA topology"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.6.3.1 </span><span class="title-name">VM Guest virtual NUMA topology</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-vmguest-topo">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            Creating a VM Guest virtual NUMA (vNUMA) policy that resembles the
            host NUMA topology can often increase performance of traditional
            large, scale-up workloads. VM Guest vNUMA topology can be
            specified using the <code class="literal">numa</code> element in the XML
            configuration:
          </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<span class="callout" id="co-numa-cell">1</span> id="0"<span class="callout" id="co-numa-id">2</span> cpus='0-1'<span class="callout" id="co-numa-cpus">3</span> memory='512000' unit='KiB'/&gt;
    &lt;cell id="1" cpus='2-3' memory='256000'<span class="callout" id="co-numa-mem">4</span>
    unit='KiB'<span class="callout" id="co-numa-unit">5</span> memAccess='shared'<span class="callout" id="co-numa-memaccess">6</span>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-cell"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Each <code class="literal">cell</code> element specifies a vNUMA cell or
                node
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-id"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                All cells should have an <code class="literal">id</code> attribute,
                allowing to reference the cell in other configuration blocks.
                Otherwise, cells are assigned IDs in ascending order starting
                from 0.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-cpus"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The CPU or range of CPUs that are part of the node
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-mem"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The node memory
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-unit"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Units in which node memory is specified
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numa-memaccess"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Optional attribute which can control whether the memory is to
                be mapped as <code class="option">shared</code> or
                <code class="option">private</code>. This is valid only for
                hugepages-backed memory.
              </p></td></tr></table></div><p>
            To find where the VM Guest has allocated its pages. use:
            <code class="command">cat
            /proc/<em class="replaceable">PID</em>/numa_maps</code> and
            <code class="command">cat
            /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<em class="replaceable">KVM_NAME</em>/memory.numa_stat</code>.
          </p><div id="id-1.13.4.6.8.9.3.6" data-id-title="NUMA specification" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: NUMA specification</div><p>
              The <code class="systemitem">libvirt</code> VM Guest NUMA specification is currently only
              available for QEMU/KVM.
            </p></div></section><section class="sect4" id="sec-vt-best-perf-numa-vmguest-alloc-libvirt" data-id-title="Memory allocation control with libvirt"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">4.6.3.2 </span><span class="title-name">Memory allocation control with <code class="systemitem">libvirt</code></span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-numa-vmguest-alloc-libvirt">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            If the VM Guest has a vNUMA topology (see
            <a class="xref" href="#sec-vt-best-perf-numa-vmguest-topo" title="4.6.3.1. VM Guest virtual NUMA topology">Section 4.6.3.1, “VM Guest virtual NUMA topology”</a>), memory
            can be pinned to host NUMA nodes using the
            <code class="literal">numatune</code> element. This method is currently only
            available for QEMU/KVM guests. See
            <a class="xref" href="#sec-vt-best-perf-numa-alloc-libvirt-non-vnuma" title="Important: Non-vNUMA VM Guest">Important: Non-vNUMA VM Guest</a>
            for how to configure non-vNUMA VM Guests.
          </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
    &lt;memory mode="strict"<span class="callout" id="co-numat-mode">1</span> nodeset="1-4,^3"<span class="callout" id="co-numat-nodeset">2</span>/&gt;
    &lt;memnode<span class="callout" id="co-numat-memnode">3</span> cellid="0"<span class="callout" id="co-numat-cellid">4</span> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<span class="callout" id="co-numat-placement">5</span> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-mode"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Policies available are: <code class="literal">interleave</code>
                (round-robin like), <code class="literal">strict</code> (default) or
                <code class="literal">preferred</code>.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-nodeset"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Specify the NUMA nodes.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-memnode"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Specify memory allocation policies for each guest NUMA node (if
                this element is not defined, then this will fall back and use
                the <code class="literal">memory</code> element).
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-cellid"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                Addresses the guest NUMA node for which the settings are
                applied.
              </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-numat-placement"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The placement attribute can be used to indicate the memory
                placement mode for a domain process, the value can be
                <code class="literal">auto</code> or <code class="literal">strict</code>.
              </p></td></tr></table></div><div id="sec-vt-best-perf-numa-alloc-libvirt-non-vnuma" data-id-title="Non-vNUMA VM Guest" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Non-vNUMA VM Guest</div><p>
              On a non-vNUMA VM Guest, pinning memory to host NUMA nodes is
              done as in the following example:
            </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</pre></div><p>
              In this example, memory is allocated from the host nodes
              <code class="literal">0</code> and <code class="literal">1</code>. If these memory
              requirements cannot be fulfilled, starting the VM Guest will
              fail. <code class="command">virt-install</code> also supports this
              configuration with the <code class="option">--numatune</code> option.
            </p></div><div id="id-1.13.4.6.8.9.4.6" data-id-title="Memory and CPU across NUMA nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Memory and CPU across NUMA nodes</div><p>
              You should avoid allocating VM Guest memory across NUMA nodes,
              and prevent virtual CPUs from floating across NUMA nodes.
            </p></div></section></section></section></section><section class="sect1" id="sec-vt-best-img" data-id-title="VM Guest images"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">VM Guest images</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Images are virtual disks used to store the operating system and data of
      VM Guests. They can be created, maintained and queried with the
      <code class="command">qemu-img</code> command. Refer to
      <span class="intraxref">Book “Virtualization Guide”, Chapter 36 “Guest installation”, Section 36.2.2 “Creating, converting, and checking disk images”</span> for more
      information on the <code class="command">qemu-img</code> tool and examples.
    </p><section class="sect2" id="sec-vt-best-img-imageformat" data-id-title="VM Guest image formats"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">VM Guest image formats</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Certain storage formats which QEMU recognizes have their origins in
        other virtualization technologies. By recognizing these formats, QEMU
        can use either data stores or entire guests that were originally
        targeted to run under these other virtualization technologies. Certain
        formats are supported only in read-only mode. To use them in read/write
        mode, convert them to a fully supported QEMU storage format (using
        <code class="command">qemu-img</code>). Otherwise they can only be used as
        read-only data store in a QEMU guest.
      </p><p>
        Use <code class="command">qemu-img info
        <em class="replaceable">VMGUEST.IMG</em></code> to get information
        about an existing image, such as the format, the virtual size, the
        physical size and snapshots, if available.
      </p><div id="id-1.13.4.7.3.4" data-id-title="Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Performance</div><p>
          It is recommended to convert the disk images to either raw or qcow2
          to achieve good performance.
        </p></div><div id="id-1.13.4.7.3.5" data-id-title="Encrypted images cannot be compressed" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Encrypted images cannot be compressed</div><p>
          When you create an image, you cannot use compression
          (<code class="option">-c</code>) in the output file together with the encryption
          option (<code class="option">-e</code>).
        </p></div><section class="sect3" id="sec-vt-best-img-imageformat-raw" data-id-title="Raw format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.1 </span><span class="title-name">Raw format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-raw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              This format is simple and easily exportable to all other
              emulators/hypervisors.
            </p></li><li class="listitem"><p>
              It provides the best performance (least I/O overhead).
            </p></li><li class="listitem"><p>
              It occupies all allocated space on the file system.
            </p></li><li class="listitem"><p>
              The raw format allows to copy a VM Guest image to a physical
              device (<code class="command">dd if=<em class="replaceable">VMGUEST.RAW</em>
              of=<em class="replaceable">/dev/sda</em></code>).
            </p></li><li class="listitem"><p>
              It is byte-for-byte the same as what the VM Guest sees, so this
              wastes a lot of space.
            </p></li></ul></div></section><section class="sect3" id="sec-vt-best-img-imageformat-qcow2" data-id-title="qcow2 format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.2 </span><span class="title-name">qcow2 format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-qcow2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              Use this to have smaller images (useful if your file system does
              not support holes).
            </p></li><li class="listitem"><p>
              It has optional AES encryption (now deprecated).
            </p></li><li class="listitem"><p>
              Zlib-based compression option.
            </p></li><li class="listitem"><p>
              Supports multiple VM snapshots (internal, external).
            </p></li><li class="listitem"><p>
              Improved performance and stability.
            </p></li><li class="listitem"><p>
              Supports changing the backing file.
            </p></li><li class="listitem"><p>
              Supports consistency checks.
            </p></li><li class="listitem"><p>
              Less performance than raw format.
            </p></li></ul></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.13.4.7.3.7.4.1"><span class="term">l2-cache-size</span></dt><dd><p>
                qcow2 can provide the same performance for random read/write
                access as raw format, but it needs a well-sized cache size. By
                default, cache size is set to 1 MB. This will give good
                performance up to a disk size of 8 GB. If you need a
                bigger disk size, you need to adjust the cache size. For a disk
                size of 64 GB (64*1024 = 65536), you need 65536 /
                8192 B = 8 MB of cache (<code class="option">-drive
                format=qcow2,l2-cache-size=8M</code>).
              </p></dd><dt id="id-1.13.4.7.3.7.4.2"><span class="term">Cluster size</span></dt><dd><p>
                The qcow2 format offers the capability to change the cluster
                size. The value must be between 512 KB and 2 MB.
                Smaller cluster sizes can improve the image file size, whereas
                larger cluster sizes provide better performance.
              </p></dd><dt id="id-1.13.4.7.3.7.4.3"><span class="term">Preallocation</span></dt><dd><p>
                An image with preallocated metadata is initially larger but can
                improve performance when the image needs to grow.
              </p></dd><dt id="id-1.13.4.7.3.7.4.4"><span class="term">Lazy refcounts</span></dt><dd><p>
                Reference count updates are postponed with the goal of avoiding
                metadata I/O and improving performance. This is particularly
                beneficial with <code class="option">cache=writethrough</code>. This
                option does not batch metadata updates, but if in case of host
                crash, the reference count tables must be rebuilt, this is done
                automatically at the next open with <code class="command">qemu-img check -r
                all</code> and takes a certain amount of time.
              </p></dd></dl></div></section><section class="sect3" id="sec-vt-best-img-imageformat-qed" data-id-title="qed format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.3 </span><span class="title-name">qed format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-qed">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          qed is a follow-on qcow (QEMU Copy On Write) format. Because qcow2
          provides all the benefits of qed and more, qed is now deprecated.
        </p></section><section class="sect3" id="sec-vt-best-img-imageformat-vmdk" data-id-title="VMDK format"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.1.4 </span><span class="title-name">VMDK format</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-imageformat-vmdk">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          VMware 3, 4 or 6 image format for exchanging images with that
          product.
        </p></section></section><section class="sect2" id="sec-vt-best-img-overlay" data-id-title="Overlay disk images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Overlay disk images</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-overlay">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The qcow2 and qed formats provide a way to create a base image (also
        called backing file) and overlay images on top of the base image. A
        backing file is useful for reverting to a known state and discarding
        the overlay. If you write to the image, the backing image will be
        untouched and all changes will be recorded in the overlay image file.
        The backing file will never be modified unless you use the
        <code class="option">commit</code> monitor command (or <code class="command">qemu-img
        commit</code>).
      </p><p>
        To create an overlay image:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img create -o<span class="callout" id="co-1-minoro">1</span>backing_file=vmguest.raw<span class="callout" id="co-1-backingfile">2</span>,backing_fmt=raw<span class="callout" id="co-1-backingfmt">3</span>\
     -f<span class="callout" id="co-1-minorf">4</span> qcow2 vmguest.cow<span class="callout" id="co-1-imagename">5</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-minoro"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Use <code class="option">-o ?</code> for an overview of available options.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-backingfile"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            The backing file name.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-backingfmt"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Specify the file format for the backing file.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-minorf"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Specify the image format for the VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-1-imagename"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Image name of the VM Guest, it will only record the differences
            from the backing file.
          </p></td></tr></table></div><div id="id-1.13.4.7.4.6" data-id-title="Backing image path" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Backing image path</div><p>
          You should not change the path to the backing image, otherwise you
          will need to adjust it. The path is stored in the overlay image file.
          To update the path, make a symbolic link from the original path to
          the new path and then use the <code class="command">qemu-img</code>
          <code class="option">rebase</code> option.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ln -sf /var/lib/images/<em class="replaceable">OLD_PATH</em>/vmguest.raw  \
 /var/lib/images/<em class="replaceable">NEW_PATH</em>/vmguest.raw
<code class="prompt root"># </code>qemu-img rebase<span class="callout" id="co-2-rebase">1</span> -u<span class="callout" id="co-2-unsafe">2</span> -b<span class="callout" id="co-2-minorb">3</span> \
 /var/lib/images/<em class="replaceable">OLD_PATH</em>/vmguest.raw /var/lib/images/<em class="replaceable">NEW_PATH</em>/vmguest.cow</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-2-rebase"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The <code class="command">rebase</code> subcommand tells
              <code class="command">qemu-img</code> to change the backing file image.
            </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-2-unsafe"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The <code class="option">-u</code> option activates the unsafe mode (see
              note below). There are two different modes in which
              <code class="option">rebase</code> can operate:
            </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                  <span class="emphasis"><em>Safe</em></span>: this is the default mode and
                  performs a real rebase operation. The safe mode is a
                  time-consuming operation.
                </p></li><li class="listitem"><p>
                  <span class="emphasis"><em>Unsafe</em></span>: the unsafe mode
                  (<code class="option">-u</code>) only changes the backing files name and
                  the format of the file name, making no checks on the file's
                  contents. Use this mode to rename or move a backing file.
                </p></li></ul></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-2-minorb"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
              The backing image to be used is specified with
              <code class="option">-b</code> and the image path is the last argument of
              the command.
            </p></td></tr></table></div></div><p>
        A common use is to initiate a new guest with the backing file. Let's
        assume we have a <code class="filename">sle15_base.img</code> VM Guest ready to
        be used (fresh installation without any modification). This will be our
        backing file. Now you need to test a new package, on an updated system
        and on a system with a different kernel. We can use
        <code class="filename">sle15_base.img</code> to instantiate the new SUSE Linux Enterprise
        VM Guest by creating a qcow2 overlay file pointing to this backing
        file (<code class="filename">sle15_base.img</code>).
      </p><p>
        In our example, we will use <code class="filename">sle15_updated.qcow2</code>
        for the updated system, and <code class="filename">sle15_kernel.qcow2</code> for
        the system with a different kernel.
      </p><p>
        To create the two thin provisioned systems, use the
        <code class="command">qemu-img</code> command line with the <code class="option">-b</code>
        option:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_updated.qcow2
Formatting 'sle15_updated.qcow2', fmt=qcow2 size=17179869184
backing_file='sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off
<code class="prompt root"># </code>qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_kernel.qcow2
Formatting 'sle15_kernel.qcow2', fmt=qcow2 size=17179869184
backing_file='vmguest-sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off</pre></div><p>
        The images are now usable, and you can do your test without touching
        the initial <code class="filename">sle15_base.img</code> backing file. All
        changes will be stored in the new overlay images. You can also use
        these new images as a backing file and create a new overlay.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img create -b sle15_kernel.qcow2 -f qcow2 sle15_kernel_TEST.qcow2</pre></div><p>
        When using <code class="command">qemu-img info</code> with the option
        <code class="option">--backing-chain</code>, it will return all information about
        the entire backing chain recursively:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img info --backing-chain
/var/lib/libvirt/images/sle15_kernel_TEST.qcow2
image: sle15_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle15_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE15.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</pre></div><div class="figure" id="fig-qemu-img-overlay"><div class="figure-contents"><div class="mediaobject"><a href="images/qemu-img-overlay.png"><img src="images/qemu-img-overlay.png" width="95%" alt="Understanding image overlay" title="Understanding image overlay"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1: </span><span class="title-name">Understanding image overlay </span></span><a title="Permalink" class="permalink" href="#fig-qemu-img-overlay">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-vt-best-img-open-img" data-id-title="Opening a VM Guest image"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Opening a VM Guest image</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-open-img">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To access the file system of an image, use the
        <span class="package">guestfs-tools</span>. If you do not have this tool
        installed on your system, you can mount an image with other Linux
        tools. Avoid accessing an untrusted or unknown VM Guest's image system
        because this can lead to security issues (for more information, read
        <a class="link" href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/" target="_blank">D.
        Berrangé's post</a>).
      </p><section class="sect3" id="sec-vt-best-img-open-img-raw" data-id-title="Opening a raw image"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Opening a raw image</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-open-img-raw">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.13.4.7.5.3.2" data-id-title="Mounting a raw image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5: </span><span class="title-name">Mounting a raw image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.7.5.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To mount the image, find a free loop device. The following
              command displays the first unused loop device,
              <code class="filename">/dev/loop1</code> in this example.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup -f
/dev/loop1</pre></div></li><li class="step"><p>
              Associate an image (<code class="filename">SLE15.raw</code> in this
              example) with the loop device:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup /dev/loop1 SLE15.raw</pre></div></li><li class="step"><p>
              Check whether the image has successfully been associated with the
              loop device by getting detailed information about the loop
              device:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup -l
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE15.raw</pre></div></li><li class="step"><p>
              Check the image's partitions with <code class="command">kpartx</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>kpartx -a<span class="callout" id="co-kpartx-a">1</span> -v<span class="callout" id="co-kpartx-v">2</span> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-kpartx-a"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Add partition device mappings.
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-kpartx-v"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Verbose mode.
                </p></td></tr></table></div></li><li class="step"><p>
              Now mount the image partitions (to
              <code class="filename">/mnt/sle15mount</code> in the following example):
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /mnt/sle15mount
<code class="prompt root"># </code>mount /dev/mapper/loop1p1 /mnt/sle15mount</pre></div></li></ol></div></div><div id="id-1.13.4.7.5.3.3" data-id-title="Raw image with LVM" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Raw image with LVM</div><p>
            If your raw image contains an LVM volume group, you should use LVM
            tools to mount the partition. Refer to
            <a class="xref" href="#sec-lvm-found" title="5.3.3. Opening images containing LVM">Section 5.3.3, “Opening images containing LVM”</a>.
          </p></div><div class="procedure" id="id-1.13.4.7.5.3.4" data-id-title="Unmounting a raw image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6: </span><span class="title-name">Unmounting a raw image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.7.5.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Unmount all mounted partitions of the image, for example:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>umount /mnt/sle15mount</pre></div></li><li class="step" id="st-umount-raw"><p>
              Delete partition device mappings with <code class="command">kpartx</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>kpartx -d /dev/loop1</pre></div></li><li class="step"><p>
              Detach the devices with <code class="command">losetup</code>
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>losetup -d /dev/loop1</pre></div></li></ol></div></div></section><section class="sect3" id="sec-vt-best-img-open-img-qcow2" data-id-title="Opening a qcow2 image"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Opening a qcow2 image</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-open-img-qcow2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.13.4.7.5.4.2" data-id-title="Mounting a qcow2 image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7: </span><span class="title-name">Mounting a qcow2 image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.7.5.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              First, you need to load the <code class="literal">nbd</code> (network block
              devices) module. The following example loads it with support for
              16 block devices (<code class="option">max_part=16</code>). Check with
              <code class="command">dmesg</code> whether the operation was successful:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>modprobe nbd max_part=16
<code class="prompt root"># </code>dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</pre></div></li><li class="step"><p>
              Connect the VM Guest image, for example,
              <code class="filename">SLE15.qcow2</code>, to an NBD device
              (<code class="filename">/debv/nbd0</code> in the following example) with
              the <code class="command">qemu-nbd</code> command. Use a free NBD device:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-nbd -c<span class="callout" id="co-qemunbd-minusc">1</span> /dev/nbd0<span class="callout" id="co-qemunbd-device">2</span> SLE15.qcow2<span class="callout" id="co-qemunbd-image">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-minusc"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Connect <code class="filename">SLE15.qcow2</code> to the local NBD
                  device <code class="filename">/dev/nbd0</code>
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-device"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  NBD device to use
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-qemunbd-image"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  VM Guest image to use
                </p></td></tr></table></div><div id="id-1.13.4.7.5.4.2.3.4" data-id-title="Checking for a free NBD device" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Checking for a free NBD device</div><p>
                To check whether an NBD device is free, run the following
                command:
              </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</pre></div><p>
                If the command produces an output like in the example above,
                the device is busy (not free). This can also be confirmed by
                the presence of the
                <code class="filename">/sys/devices/virtual/block/nbd0/pid</code> file.
              </p></div></li><li class="step"><p>
              Inform the operating system about partition table changes with
              <code class="command">partprobe</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>partprobe /dev/nbd0 -s
/dev/nbd0: msdos partitions 1 2
<code class="prompt root"># </code>dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</pre></div></li><li class="step"><p>
              In the example above, the <code class="filename">SLE15.qcow2</code>
              contains two partitions: <code class="filename">/dev/nbd0p1</code> and
              <code class="filename">/dev/nbd0p2</code>. Before mounting these
              partitions, use <code class="command">vgscan</code> to check whether they
              belong to an LVM volume:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgscan -v
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Reading all physical volumes. This may take a while...
    Using volume group(s) on command line.
    No volume groups found.</pre></div></li><li class="step"><p>
              If no LVM volume has been found, you can mount the partition with
              <code class="command">mount</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</pre></div><p>
              Refer to <a class="xref" href="#sec-lvm-found" title="5.3.3. Opening images containing LVM">Section 5.3.3, “Opening images containing LVM”</a> for information on how
              to handle LVM volumes.
            </p></li></ol></div></div><div class="procedure" id="id-1.13.4.7.5.4.3" data-id-title="Unmounting a qcow2 image"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8: </span><span class="title-name">Unmounting a qcow2 image </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.7.5.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Unmount all mounted partitions of the image, for example:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>umount /mnt/nbd0p2</pre></div></li><li class="step" id="st-umount-qcow2"><p>
              Disconnect the image from the <code class="filename">/dev/nbd0</code>
              device.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-nbd -d /dev/nbd0</pre></div></li></ol></div></div></section><section class="sect3" id="sec-lvm-found" data-id-title="Opening images containing LVM"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.3 </span><span class="title-name">Opening images containing LVM</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-found">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.13.4.7.5.5.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
            If your VM Host Server uses the VG name <code class="literal">system</code>, and
            the guest image also uses the VG name <code class="literal">system</code>,
            LVM will complain during its activation. A workaround is to
            temporarily rename the guest VG, while a correct approach is to use
            different VG names for the guests than for the VM Host Server.
          </p></div><div class="procedure" id="id-1.13.4.7.5.5.3" data-id-title="Mounting images containing LVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 9: </span><span class="title-name">Mounting images containing LVM </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.7.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              To check images for LVM groups, use <code class="command">vgscan -v</code>.
              If an image contains LVM groups, the output of the command looks
              like the following:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgscan -v
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</pre></div></li><li class="step"><p>
              The <code class="literal">system</code> LVM volume group has been found on
              the system. You can get more information about this volume with
              <code class="command">vgdisplay
              <em class="replaceable">VOLUMEGROUPNAME</em></code> (in our case
              <em class="replaceable">VOLUMEGROUPNAME</em> is
              <code class="literal">system</code>). You should activate this volume group
              to expose LVM partitions as devices so the system can mount them.
              Use <code class="command">vgchange</code>:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgchange -ay -v
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</pre></div></li><li class="step"><p>
              All partitions in the volume group will be listed in the
              <code class="filename">/dev/mapper</code> directory. You can simply mount
              them now.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

<code class="prompt root"># </code>mkdir /mnt/system-root
<code class="prompt root"># </code>mount  /dev/mapper/system-root /mnt/system-root

<code class="prompt root"># </code>ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</pre></div></li></ol></div></div><div class="procedure" id="id-1.13.4.7.5.5.4" data-id-title="Unmounting images containing LVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10: </span><span class="title-name">Unmounting images containing LVM </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.7.5.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Unmount all partitions (with <code class="command">umount</code>)
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>umount /mnt/system-root</pre></div></li><li class="step"><p>
              Deactivate the LVM volume group (with <code class="command">vgchange -an
              <em class="replaceable">VOLUMEGROUPNAME</em></code>)
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgchange -an -v system
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</pre></div></li><li class="step"><p>
              Now you have two choices:
            </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                  With a qcow2 image, proceed as described in
                  <a class="xref" href="#st-umount-qcow2" title="Step 2">Step 2</a> (<code class="command">qemu-nbd -d
                  /dev/nbd0</code>).
                </p></li><li class="listitem"><p>
                  With a raw image, proceed as described in
                  <a class="xref" href="#st-umount-raw" title="Step 2">Step 2</a> (<code class="command">kpartx -d
                  /dev/loop1</code>; <code class="command">losetup -d
                  /dev/loop1</code>).
                </p></li></ul></div><div id="id-1.13.4.7.5.5.4.4.3" data-id-title="Check for a successful unmount" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Check for a successful unmount</div><p>
                You should double-check that unmounting succeeded by using a
                system command like <code class="command">losetup</code>,
                <code class="command">qemu-nbd</code>, <code class="command">mount</code> or
                <code class="command">vgscan</code>. If this is not the case, you may
                have trouble using the VM Guest because its system image is
                used in different places.
              </p></div></li></ol></div></div></section></section><section class="sect2" id="sec-vt-best-img-share" data-id-title="File system sharing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">File system sharing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-img-share">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        You can access a host directory in the VM Guest using the
        <code class="sgmltag-element">filesystem</code> element. In the following
        example we will share the <code class="filename">/data/shared</code> directory
        and mount it in the VM Guest. The
        <code class="sgmltag-attribute">accessmode</code> parameter only works with
        <code class="sgmltag-attribute">type='mount'</code> for the QEMU/KVM driver
        (most other values for <code class="sgmltag-attribute">type</code> are
        exclusively used for the LXC driver).
      </p><div class="verbatim-wrap"><pre class="screen">&lt;filesystem type='mount'<span class="callout" id="co-fs-mount">1</span> accessmode='mapped'<span class="callout" id="co-fs-mode">2</span>&gt;
   &lt;source dir='/data/shared'<span class="callout" id="co-fs-sourcedir">3</span>&gt;
   &lt;target dir='shared'<span class="callout" id="co-fs-targetdir">4</span>/&gt;
&lt;/filesystem&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-mount"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            A host directory to mount VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-mode"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Access mode (the security mode) set to <code class="literal">mapped</code>
            will give access with the permissions of the hypervisor. Use
            <code class="literal">passthrough</code> to access this share with the
            permissions of the user inside the VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-sourcedir"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Path to share with the VM Guest.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-fs-targetdir"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Name or label of the path for the mount command.
          </p></td></tr></table></div><p>
        To mount the <code class="literal">shared</code> directory on the VM Guest, use
        the following commands: under the VM Guest, now you need to mount the
        <code class="literal">target dir='shared'</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /opt/mnt_shared
<code class="prompt root"># </code>mount shared -t 9p /opt/mnt_shared -o trans=virtio</pre></div><p>
        See
        <a class="link" href="https://libvirt.org/formatdomain.html#elementsFilesystems" target="_blank"><code class="systemitem">libvirt</code>
        File System </a> and
        <a class="link" href="https://wiki.qemu.org/Documentation/9psetup" target="_blank">QEMU
        9psetup</a> for more information.
      </p></section></section><section class="sect1" id="sec-vt-best-vmguests" data-id-title="VM Guest configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">VM Guest configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-vt-best-vmguests-virtio" data-id-title="Virtio driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Virtio driver</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To increase VM Guest performance, we recommend using paravirtualized
        drivers within the VM Guests. The virtualization standard for such
        drivers for KVM are the <code class="literal">virtio</code> drivers, which are
        designed for running in a virtual environment. Xen uses similar
        paravirtualized device drivers (like
        <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
        in a Windows* guest).
      </p><section class="sect3" id="sec-vt-best-vmguests-virtio-virtio-blk" data-id-title="virtio blk"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.1 </span><span class="title-name"><code class="literal">virtio blk</code></span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-virtio-blk">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          <code class="literal">virtio_blk</code> is the virtio block device for disk. To
          use the <code class="literal">virtio blk</code> driver for a block device,
          specify the <code class="sgmltag-attribute">bus='virtio'</code> attribute in
          the <code class="sgmltag-element">disk</code> definition:
        </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</pre></div><div id="id-1.13.4.8.2.3.4" data-id-title="Disk device names" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Disk device names</div><p>
            <code class="literal">virtio</code> disk devices are named
            <code class="literal">/dev/vd[a-z][1-9]</code>. If you migrate a Linux guest
            from a non-virtio disk, you need to adjust the
            <code class="literal">root=</code> parameter in the GRUB configuration, and
            regenerate the <code class="filename">initrd</code> file. Otherwise the
            system cannot boot. On VM Guests with other operating systems, the
            boot loader may need to be adjusted or reinstalled accordingly,
            too.
          </p></div><div id="id-1.13.4.8.2.3.5" data-id-title="Using virtio disks with qemu-system-ARCH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Using <code class="literal">virtio</code> disks with <code class="command">qemu-system-ARCH</code></div><p>
            When running <code class="command">qemu-system-ARCH</code>, use the
            <code class="option">-drive</code> option to add a disk to the VM Guest. See
            <span class="intraxref">Book “Virtualization Guide”, Chapter 36 “Guest installation”, Section 36.1 “Basic installation with <code class="command">qemu-system-ARCH</code>”</span> for an example. The
            <code class="option">-hd[abcd]</code> option will not work for virtio disks.
          </p></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-virtio-net" data-id-title="virtio net"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.2 </span><span class="title-name">virtio net</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-virtio-net">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          <code class="literal">virtio_net</code> is the virtio network device. The
          kernel modules should be loaded automatically in the guest at boot
          time. You need to start the service to make the network available.
        </p><div class="verbatim-wrap"><pre class="screen">&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-balloon" data-id-title="virtio balloon"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.3 </span><span class="title-name">virtio balloon</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-balloon">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The virtio balloon is used for host memory over-commits for guests.
          For Linux guests, the balloon driver runs in the guest kernel,
          whereas for Windows guests, the balloon driver is in the VMDP
          package. <code class="literal">virtio_balloon</code> is a PV driver to give or
          take memory from a VM Guest.
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <span class="emphasis"><em>Inflate balloon</em></span>: return memory from guest to
              host kernel (for KVM) or to hypervisor (for Xen)
            </p></li><li class="listitem"><p>
              <span class="emphasis"><em>Deflate balloon</em></span>: Guest will have more
              available memory
            </p></li></ul></div><p>
          It is controlled by the <code class="literal">currentMemory</code> and
          <code class="literal">memory</code> options.
        </p><div class="verbatim-wrap"><pre class="screen">&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
        &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</pre></div><p>
          You can also use <code class="command">virsh</code> to change it:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh setmem <em class="replaceable">DOMAIN_ID</em> <em class="replaceable">MEMORY in KB</em></pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-check" data-id-title="Checking virtio presence"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.4 </span><span class="title-name">Checking virtio presence</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-check">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          You can check the virtio block PCI with:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</pre></div><p>
          To find the block device associated with <code class="filename">vdX</code>:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</pre></div><p>
          To get more information on the virtio block:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</pre></div><p>
          To check all virtio drivers being used:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</pre></div></section><section class="sect3" id="sec-vt-best-vmguests-virtio-drv-opt" data-id-title="Find device driver options"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">6.1.5 </span><span class="title-name">Find device driver options</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vmguests-virtio-drv-opt">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Virtio devices and other drivers have multiple options. To list them
          all, use the <code class="option">help</code> parameter of
          the<code class="command">qemu-system-ARCH</code> command.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</pre></div></section></section><section class="sect2" id="sec-vt-best-perf-cirrus" data-id-title="Cirrus video driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Cirrus video driver</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-cirrus">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To get 16-bit color, high compatibility and better performance, we
        recommend using the <code class="literal">cirrus</code> video driver.
      </p><div id="id-1.13.4.8.3.3" data-id-title="libvirt" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <code class="systemitem">libvirt</code></div><p>
          <code class="systemitem">libvirt</code> ignores the <code class="literal">vram</code> value because video
          size has been hardcoded in QEMU.
        </p></div><div class="verbatim-wrap"><pre class="screen">&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</pre></div></section><section class="sect2" id="sec-vt-best-entropy" data-id-title="Better entropy"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">Better entropy</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-entropy">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Virtio RNG (random number generator) is a paravirtualized device that
        is exposed as a hardware RNG device to the guest. On the host side, it
        can be wired up to one of several sources of entropy (including a real
        hardware RNG device and the host's <code class="filename">/dev/random</code>) if
        hardware support does not exist. The Linux kernel contains the guest
        driver for the device from version 2.6.26 and higher.
      </p><p>
        The system entropy is collected from several non-deterministic hardware
        events and is mainly used by cryptographic applications. The virtual
        random number generator device (paravirtualized device) allows the host
        to pass through entropy to VM Guest operating systems. This results in
        a better entropy in the VM Guest.
      </p><p>
        To use Virtio RNG, add an <code class="literal">RNG</code> device in
        <code class="command">virt-manager</code> or directly in the VM Guest's XML
        configuration:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;devices&gt;
   &lt;rng model='virtio'&gt;
       &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</pre></div><p>
        The host now should used <code class="filename">/dev/random</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</pre></div><p>
        On the VM Guest, the source of entropy can be checked with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/devices/virtual/misc/hw_random/rng_available</pre></div><p>
        The current device used for entropy can be checked with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</pre></div><p>
        You should install the <span class="package">rng-tools</span> package on the
        VM Guest, enable the service, and start it. Under <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 15, do
        the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in rng-tools
<code class="prompt root"># </code>systemctl enable rng-tools
<code class="prompt root"># </code>systemctl start rng-tools</pre></div></section><section class="sect2" id="sec-vt-best-perf-disable" data-id-title="Disable unused tools and devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Disable unused tools and devices</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-disable">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Per host, use one virtualization technology only. For example, do not
        use KVM and Xen on the same host. Otherwise, you may find yourself
        with a reduced amount of available resources, increased security risk
        and a longer software update queue. Even when the amount of resources
        allocated to each of the technologies is configured carefully, the host
        may suffer from reduced overall availability and degraded performance.
      </p><p>
        Minimize the amount of software and services available on hosts. Most
        default installations of operating systems are not optimized for VM
        usage. Install what you really need and remove all other components in
        the VM Guest.
      </p><p>
        Windows* Guest:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            Disable the screen saver
          </p></li><li class="listitem"><p>
            Remove all graphical effects
          </p></li><li class="listitem"><p>
            Disable indexing of hard disks if not necessary
          </p></li><li class="listitem"><p>
            Check the list of started services and disable the ones you do not
            need
          </p></li><li class="listitem"><p>
            Check and remove all unneeded devices
          </p></li><li class="listitem"><p>
            Disable system update if not needed, or configure it to avoid any
            delay while rebooting or shutting down the host
          </p></li><li class="listitem"><p>
            Check the Firewall rules
          </p></li><li class="listitem"><p>
            Schedule backups and anti-virus updates appropriately
          </p></li><li class="listitem"><p>
            Install the
            <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
            paravirtualized driver for best performance
          </p></li><li class="listitem"><p>
            Check the operating system recommendations, such as on the
            <a class="link" href="https://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7" target="_blank">Microsoft
            Windows* 7 better performance</a> Web page.
          </p></li></ul></div><p>
        Linux Guest:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            Remove or do not start the X Window System if not necessary
          </p></li><li class="listitem"><p>
            Check the list of started services and disable the ones you do not
            need
          </p></li><li class="listitem"><p>
            Check the OS recommendations for kernel parameters that enable
            better performance
          </p></li><li class="listitem"><p>
            Only install software that you really need
          </p></li><li class="listitem"><p>
            Optimize the scheduling of predictable tasks (system updates, hard
            disk checks, etc.)
          </p></li></ul></div></section><section class="sect2" id="sec-vt-best-perf-mtype" data-id-title="Updating the guest machine type"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5 </span><span class="title-name">Updating the guest machine type</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-perf-mtype">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        QEMU machine types define details of the architecture that are
        particularly relevant for migration and session management. As changes
        or improvements to QEMU are made, new machine types are added. Old
        machine types are still supported for compatibility reasons, but to
        use improvements, we recommend to always migrate to the
        latest machine type when upgrading.
      </p><p>
        Changing the guest's machine type for a Linux guest is transparent. For
        Windows* guests, we recommend creating a snapshot or backup of the
        guest—in case Windows* has issues with the changes it detects,
        and subsequently the user reverts to the original machine type the
        guest was created with.
      </p><div id="id-1.13.4.8.6.4" data-id-title="Changing the machine type" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Changing the machine type</div><p>
          Refer to <span class="intraxref">Book “Virtualization Guide”, Chapter 15 “Configuring virtual machines with <code class="command">virsh</code>”, Section 15.2 “Changing the machine type”</span> for
          documentation.
        </p></div></section></section><section class="sect1" id="sec-vt-best-vm-setup-config" data-id-title="VM Guest-specific configurations and settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">VM Guest-specific configurations and settings</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-vm-setup-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.13.4.9.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        This section applies to QEMU / KVM hypervisor only.
      </p></div><section class="sect2" id="sec-vt-best-acpi" data-id-title="ACPI testing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">ACPI testing</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-acpi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The ability to change a VM Guest's state heavily depends on the
        operating system. It is important to test this feature before any use
        of your VM Guests in production. For example, most Linux operating
        systems disable this capability by default, so this requires you to
        enable this operation (normally through Polkit).
      </p><p>
        ACPI must be enabled in the guest for a graceful shutdown to work. To
        check if ACPI is enabled, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh dumpxml <em class="replaceable">VMNAME</em> | grep acpi</pre></div><p>
        If nothing is printed, ACPI is not enabled for your machine. Use
        <code class="command">virsh edit</code> to add the following XML under
        &lt;domain&gt;:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</pre></div><p>
        If ACPI was enabled during a Windows Server* guest
        installation, it is not sufficient to turn it on in the VM Guest
        configuration only. For more information, see
        <a class="link" href="https://support.microsoft.com/en-us/kb/309283" target="_blank">https://support.microsoft.com/en-us/kb/309283</a>.
        
      </p><p>
        Regardless of the VM Guest's configuration, a graceful shutdown is
        always possible from within the guest operating system.
      </p></section><section class="sect2" id="sec-vt-best-guest-kbd" data-id-title="Keyboard layout"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Keyboard layout</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-guest-kbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Though it is possible to specify the keyboard layout from a
        <code class="command">qemu-system-ARCH</code> command, we recommend configuring
        it in the <code class="systemitem">libvirt</code> XML file. To change the keyboard layout while
        connecting to a remote VM Guest using vnc, edit the VM Guest XML
        configuration file. For example, to add an <code class="literal">en-us</code>
        keymap, add in the <code class="literal">&lt;devices&gt;</code> section:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</pre></div><p>
        Check the <code class="literal">vncdisplay</code> configuration and connect to
        your VM Guest:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh vncdisplay sles15 127.0.0.1:0</pre></div></section><section class="sect2" id="sec-vt-best-spice-default-url" data-id-title="Spice default listen URL"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Spice default listen URL</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-spice-default-url">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        If no network interface other than <code class="literal">lo</code> is assigned an
        IPv4 address on the host, the default address on which the spice server
        listens will not work. An error like the following one will occur:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh start sles15
error: Failed to start domain sles15
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</pre></div><p>
        To fix this, you can change the default <code class="literal">spice_listen</code>
        value in <code class="filename">/etc/libvirt/qemu.conf</code> using the local
        IPv6 address <code class="systemitem">::1</code>. The spice
        server listening address can also be changed on a per VM Guest basis,
        use <code class="command">virsh edit</code> to add the listen XML attribute to
        the <code class="literal">graphics type='spice'</code> element:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;&gt;</pre></div></section><section class="sect2" id="sec-vt-best-xml-to-qemu" data-id-title="XML to QEMU command line"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">XML to QEMU command line</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-xml-to-qemu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Sometimes it could be useful to get the QEMU command line to launch
        the VM Guest from the XML file.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>virsh domxml-to-native<span class="callout" id="co-domxml-native">1</span> qemu-argv<span class="callout" id="co-domxml-argv">2</span> SLE15.xml<span class="callout" id="co-domxml-file">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-native"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Convert the XML file in domain XML format to the native guest
            configuration
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-argv"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            For the QEMU/KVM hypervisor, the format argument needs to be
            qemu-argv
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-domxml-file"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
            Domain XML file to use
          </p></td></tr></table></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE15.xml
LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE15 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE15.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE15.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</pre></div></section><section class="sect2" id="sec-vt-best-kernel-parameter" data-id-title="Change kernel parameters at boot time"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">Change kernel parameters at boot time</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-kernel-parameter">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect3" id="sec-vt-best-kernel-parameter-sle11" data-id-title="SUSE Linux Enterprise 11"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.5.1 </span><span class="title-name">SUSE Linux Enterprise 11</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-kernel-parameter-sle11">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          To change the value for SLE 11 products at boot time, you need to
          modify your <code class="filename">/boot/grub/menu.lst</code> file by adding
          the <code class="option">OPTION=parameter</code>. Then reboot your system.
        </p></section><section class="sect3" id="sec-vt-best-kernel-parameter-sle12" data-id-title="SUSE Linux Enterprise 12 and 15"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.5.2 </span><span class="title-name">SUSE Linux Enterprise 12 and 15</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-kernel-parameter-sle12">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          To change the value for SLE 12 and 15 products at boot time, you
          need to modify your <code class="filename">/etc/default/grub</code> file. Find
          the variable starting with
          <code class="option">GRUB_CMDLINE_LINUX_DEFAULT</code> and add at the end
          <code class="option">OPTION=parameter</code> (or change it with the correct
          value if it is already available).
        </p><p>
          Now you need to regenerate your <code class="literal">grub2</code>
          configuration:
        </p><div class="verbatim-wrap"><pre class="screen"># grub2-mkconfig -o /boot/grub2/grub.cfg</pre></div><p>
          Then reboot your system.
        </p></section></section><section class="sect2" id="sec-vt-best-guest-device-to-xml" data-id-title="Add a device to an XML configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6 </span><span class="title-name">Add a device to an XML configuration</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-guest-device-to-xml">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To create a new VM Guest based on an XML file, you can specify the
        QEMU command line using the special tag
        <code class="literal">qemu:commandline</code>. For example, to add a
        virtio-balloon-pci, add this block at the end of the XML configuration
        file (before the &lt;/domain&gt; tag):
      </p><div class="verbatim-wrap"><pre class="screen">&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</pre></div></section><section class="sect2" id="sec-vm-guest-vcpu" data-id-title="Adding and removing CPUs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.7 </span><span class="title-name">Adding and removing CPUs</span></span> <a title="Permalink" class="permalink" href="#sec-vm-guest-vcpu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Certain virtualization environments allow adding or removing CPUs while
        the virtual machine is running.
      </p><p>
        For the safe removal of CPUs, deactivate them first by executing
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo 0 &gt; /sys/devices/system/cpu/cpu<em class="replaceable">X</em>/online</code></pre></div><p>
        Replace <em class="replaceable">X</em> with the CPU number. To bring a
        CPU back online, execute
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo 1 &gt; /sys/devices/system/cpu/cpu<em class="replaceable">X</em>/online</code></pre></div></section><section class="sect2" id="sec-vm-guest-scsi-pr" data-id-title="SCSI persistent reservation on a multipathed device"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.8 </span><span class="title-name">SCSI persistent reservation on a multipathed device</span></span> <a title="Permalink" class="permalink" href="#sec-vm-guest-scsi-pr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        SCSI persistent reservations allow restriction of access to block devices
        in a shared storage setup. This avoids improper multiple parallel
        accesses to the same block device from software components on local or
        remote hosts, which could lead to device damage and data corruption.
      </p><p>
        Find more information on managing storage multipath I/O in
        <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”</span>. Find more information about SCSI
        persistent reservations in
        <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”, Section 18.6.4 “SCSI persistent reservations and <code class="command">mpathpersist</code>”</span>.
      </p><p>
        For the virtualization scenario, QEMU's SCSI passthrough devices
        <code class="literal">scsi-block</code> and <code class="literal">scsi-generic</code>
        support passing guest persistent reservation requests to a privileged
        external helper program <code class="literal">qemu-pr-helper</code>. This needs to
        start before QEMU and creates a listener socket that accepts incoming
        connections for communication with QEMU.
      </p><div id="id-1.13.4.9.10.5" data-id-title="Live migration scenario with multipathed devices" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Live migration scenario with multipathed devices</div><p>
          We recommend using the multipath alias instead of
          <code class="literal">wwid</code>. It is useful in the VM Guest live migration
          scenario, because it makes sure that the storage paths are identical
          between the source and destination hosts.
        </p><p>
          Find more information about multipath in
          <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”, Section 18.12.2 “Setting aliases for multipath maps”</span>.
        </p></div><div class="procedure" id="id-1.13.4.9.10.6" data-id-title="Adding a SCSI persistent reservation in a VM Guest against the related multipathed device in the VM Host Server:"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11: </span><span class="title-name">Adding a SCSI persistent reservation in a VM Guest against the related multipathed device in the VM Host Server: </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.9.10.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            In the VM Host Server, create a multipath environment. For more
            information, refer to <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”, Section 18.7 “Configuring the system for multipathing”</span> and
            <span class="intraxref">Book “Storage Administration Guide”, Chapter 18 “Managing multipath I/O for devices”, Section 18.8 “Multipath configuration”</span>.
          </p></li><li class="step"><p>
            In the VM Host Server, configure the
            <code class="literal">&lt;reservations/&gt;</code> sub-element of the
            <code class="literal">&lt;source/&gt;</code> element of the
            <code class="literal">&lt;disk/&gt;</code> element for the passed-through 
            <code class="literal">lun</code> in your <code class="systemitem">libvirt</code> domain configuration. 
            Refer to <a class="link" href="https://libvirt.org/formatdomain.html" target="_blank"><code class="systemitem">libvirt</code>
            Domain XML format</a>.
          </p></li><li class="step"><p>
            In the VM Guest, install the <span class="package">sg3_utils</span> package
            and reserve the SCSI disks on demand by using the
            <code class="command">sg_persist</code> command.
          </p></li></ol></div></div><div class="example" id="id-1.13.4.9.10.7" data-id-title="Practical example"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 1: </span><span class="title-name">Practical example </span></span><a title="Permalink" class="permalink" href="#id-1.13.4.9.10.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              In the VM Host Server, verify that the
              <code class="systemitem">multipathd.service</code> is
              running, and that a multipathed disk exists and is named, for
              example, <code class="literal">storage1</code>.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  systemctl status multipathd.service
  ● multipathd.service - Device-Mapper Multipath Device Controller
        Loaded: loaded (/usr/lib/systemd/system/multipathd.service; enabled; preset: disabled)
        Active: active (running) since Sat 2023-08-26 21:34:13 CST; 1 week 1 day ago
  TriggeredBy: ○ multipathd.socket
      Main PID: 79411 (multipathd)
        Status: "up"
        Tasks: 7
          CPU: 1min 43.514s
        CGroup: /system.slice/multipathd.service
                └─79411 /sbin/multipathd -d -s</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  multipath -ll
  storage1 (36589cfc000000537c47ad3eb2b20216e) dm-6 TrueNAS,iSCSI Disk
  size=50G features='0' hwhandler='1 alua' wp=rw
  |-+- policy='service-time 0' prio=50 status=active
  | `- 16:0:0:0 sdg 8:96  active ready running
  `-+- policy='service-time 0' prio=50 status=enabled
    `- 17:0:0:0 sdh 8:112 active ready running</pre></div></li><li class="step"><p>
              In the VM Host Server, add a &lt;disk/&gt; element in the VM Guest
              configuration file by running <code class="command">virsh edit</code>.
            </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='block' device='lun'<span class="callout" id="co-disk-lunpassthrough">1</span>&gt;
  &lt;driver name='qemu' type='raw'/&gt;
  &lt;source dev='/dev/mapper/storage1'&gt;
    &lt;reservations<span class="callout" id="co-disk-reserve">2</span> managed='yes'<span class="callout" id="co-disk-reserve-managed">3</span>/&gt;
  &lt;/source&gt;
  &lt;target dev='sda' bus='scsi'/&gt;
  &lt;address type='drive' controller='0' bus='0' target='0' unit='0'<span class="callout" id="co-disk-scsi-hba-unit">4</span>/&gt;
&lt;/disk&gt;</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-lunpassthrough"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  To support persistent reservations, the disks must be marked
                  as <code class="literal">lun</code> with type <code class="literal">block</code>
                  so that QEMU does SCSI passthrough.
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-reserve"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  If present, it enables persistent reservations for SCSI-based
                  disks. The element has one mandatory attribute
                  <code class="literal">managed</code> with accepted values
                  <code class="literal">yes</code> and <code class="literal">no</code>.
                </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-reserve-managed"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  If <code class="literal">managed</code> is <code class="literal">yes</code>,
                  <code class="systemitem">libvirt</code> prepares and manages any resources needed.
                </p><p>
                  When the value of the attribute <code class="literal">managed</code> is
                  <code class="literal">no</code>, then the hypervisor acts as a client
                  and the path to the server socket must be provided in the
                  child element source, which currently accepts only the
                  following attributes:
                </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.13.4.9.10.7.2.2.3.3.3.1"><span class="term">type</span></dt><dd><p>
                        The only valid option is <code class="literal">unix</code>.
                      </p></dd><dt id="id-1.13.4.9.10.7.2.2.3.3.3.2"><span class="term">path</span></dt><dd><p>
                        The path to the server socket.
                      </p></dd><dt id="id-1.13.4.9.10.7.2.2.3.3.3.3"><span class="term">mode</span></dt><dd><p>
                        The role of the hypervisor. Valid is
                        <code class="literal">client</code>.
                      </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-scsi-hba-unit"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  Verify that the virtio-scsi HBA that the disk attaches
                  already exists and has available units (the maximum count of
                  units per virtio-scsi HBA is 7). Otherwise, you need to
                  manually add a virtio-scsi HBA to avoid automatically adding
                  the LSI HBA by <code class="systemitem">libvirt</code>. For example:
                </p><div class="verbatim-wrap"><pre class="screen">&lt;controller type='scsi' index='0' model='virtio-scsi'&gt;
  &lt;address type='pci' domain='0x0000' bus='0x03' slot='0x00' function='0x0'/&gt;
&lt;/controller&gt;</pre></div></td></tr></table></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh domblklist sles15sp5
  Target   Source
---------------------------------------------
  vda      /mnt/images/sles15sp5/disk0.qcow2
  sda      /dev/mapper/storage1</pre></div></li><li class="step"><p>
              In the VM Host Server, start the VM Guest. <code class="systemitem">libvirt</code> launches a
              qemu-pr-helper instance as the server role for the VM Guest
              sles15sp5, then launches the VM Guest sles15sp5 as the client
              role.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh start sles15sp5
    Domain 'sles15sp5' started</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh list
     Id   Name        State
    ---------------------------
     4    sles15sp5   running</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  ps -eo pid,args | grep -v grep | grep qemu-pr-helper
    37063 /usr/bin/qemu-pr-helper -k /var/lib/libvirt/qemu/domain-4-sles15sp5/pr-helper0.sock</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  virsh dumpxml sles15sp5 | grep -A11 "&lt;disk type='block' device='lun'&gt;
  &lt;disk type='block' device='lun'&gt;
    &lt;driver name='qemu' type='raw'/&gt;
    &lt;source dev='/dev/mapper/storage1' index='1'&gt;
      &lt;reservations managed='yes'&gt;
        &lt;source type='unix' path='/var/lib/libvirt/qemu/domain-4-sles15sp5/pr-helper0.sock' mode='client'/&gt;
      &lt;/reservations&gt;
    &lt;/source&gt;
    &lt;backingStore/&gt;
    &lt;target dev='sda' bus='scsi'/&gt;
    &lt;alias name='scsi0-0-0-0'/&gt;
    &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;
  &lt;/disk&gt;</pre></div></li><li class="step"><p>
              In the VM Guest, reserve the scsi disk, for example,
              <code class="literal">sda</code>, with the key <code class="literal">123abc</code>.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda      8:0    0   50G  0 disk
vda    253:0    0   20G  0 disk
├─vda1 253:1    0    8M  0 part
├─vda2 253:2    0    2G  0 part [SWAP]
└─vda3 253:3    0   18G  0 part /</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --out --register --param-sark=123abc /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation out cdb: [5f 00 00 00 00 00 00 00 18 00]
PR out: command (Register) successful</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --in -k /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation in cdb: [5e 00 00 00 00 00 00 20 00 00]
  PR generation=0x5, 2 registered reservation keys follow:
    0x123abc
    0x123abc</pre></div></li><li class="step"><p>
              In the VM Guest, release the <code class="literal">sda</code> disk with
              the key <code class="literal">123abc</code>.
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --out --clear --param-rk=123abc /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation out cdb: [5f 03 00 00 00 00 00 00 18 00]
PR out: command (Clear) successful</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code>  sg_persist --verbose --in -k /dev/sda
    inquiry cdb: [12 00 00 00 24 00]
  TrueNAS   iSCSI Disk        0123
  Peripheral device type: disk
    Persistent reservation in cdb: [5e 00 00 00 00 00 00 20 00 00]
  PR generation=0x6, there are NO registered reservation keys</pre></div></li></ol></div></div></div></div></section></section><section class="sect1" id="sec-vt-best-refs" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-vt-best-refs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/art_virtualization-best-practices.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <a class="link" href="https://kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf" target="_blank">Increasing
          memory density using KSM</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.linux-kvm.org/page/KSM" target="_blank">linux-kvm.org
          KSM</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/Documentation/vm/ksm.txt" target="_blank">KSM's
          kernel documentation</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://lwn.net/Articles/329123/" target="_blank">ksm - dynamic
          page sharing driver for Linux v4</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.espenbraastad.no/post/memory-ballooning/" target="_blank">Memory
          Ballooning</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://wiki.libvirt.org/page/Virtio" target="_blank">libvirt
          virtio</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/html/latest/block/bfq-iosched.html" target="_blank">BFQ
          (Budget Fair Queueing)</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt" target="_blank">Documentation
          for sysctl</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://lwn.net/Articles/525459/" target="_blank">LWN Random
          Number</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/Documentation/admin-guide/kernel-parameters.txt" target="_blank">Kernel
          Parameters</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://lwn.net/Articles/374424/" target="_blank">Huge pages
          Administration (Mel Gorman)</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_blank">kernel
          hugetlbpage</a>
        </p></li></ul></div></section><section class="sect1" id="id-1.13.4.11" data-id-title="Legal notice"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Legal notice</span></span> <a title="Permalink" class="permalink" href="#id-1.13.4.11">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_copyright_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copyright© 2006–
2024

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither SUSE LLC,
  its affiliates, the authors, nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </p></section><div class="legal-section"><section class="sect1" id="id-1.13.4.12" data-id-title="GNU Free Documentation License"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">GNU Free Documentation License</span></span> <a title="Permalink" class="permalink" href="#id-1.13.4.12">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_license_gfdl1.2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.4"><span class="name">
    0. PREAMBLE
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.4">#</a></h5></div><p>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </p><p>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </p><p>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.8"><span class="name">
    1. APPLICABILITY AND DEFINITIONS
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.8">#</a></h5></div><p>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </p><p>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </p><p>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </p><p>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </p><p>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </p><p>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </p><p>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </p><p>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </p><p>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </p><p>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.19"><span class="name">
    2. VERBATIM COPYING
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.19">#</a></h5></div><p>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </p><p>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.22"><span class="name">
    3. COPYING IN QUANTITY
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.22">#</a></h5></div><p>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </p><p>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </p><p>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </p><p>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.27"><span class="name">
    4. MODIFICATIONS
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.27">#</a></h5></div><p>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </p></li><li class="listitem"><p>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </p></li><li class="listitem"><p>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </p></li><li class="listitem"><p>
    Preserve all the copyright notices of the Document.
   </p></li><li class="listitem"><p>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </p></li><li class="listitem"><p>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </p></li><li class="listitem"><p>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </p></li><li class="listitem"><p>
    Include an unaltered copy of this License.
   </p></li><li class="listitem"><p>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </p></li><li class="listitem"><p>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </p></li><li class="listitem"><p>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </p></li><li class="listitem"><p>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </p></li><li class="listitem"><p>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </p></li><li class="listitem"><p>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </p></li><li class="listitem"><p>
    Preserve any Warranty Disclaimers.
   </p></li></ol></div><p>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </p><p>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </p><p>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </p><p>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.34"><span class="name">
    5. COMBINING DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.34">#</a></h5></div><p>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </p><p>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </p><p>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.38"><span class="name">
    6. COLLECTIONS OF DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.38">#</a></h5></div><p>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </p><p>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.41"><span class="name">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.41">#</a></h5></div><p>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </p><p>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.44"><span class="name">
    8. TRANSLATION
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.44">#</a></h5></div><p>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </p><p>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.47"><span class="name">
    9. TERMINATION
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.47">#</a></h5></div><p>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.49"><span class="name">
    10. FUTURE REVISIONS OF THIS LICENSE
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.49">#</a></h5></div><p>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <a class="link" href="https://www.gnu.org/copyleft/" target="_blank">https://www.gnu.org/copyleft/</a>.
 </p><p>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.13.4.12.52"><span class="name">
    ADDENDUM: How to use this License for your documents
  </span><a title="Permalink" class="permalink" href="#id-1.13.4.12.52">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</pre></div><p>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the “with...Texts.” line with this:
 </p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </p><p>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
 </p></section></div></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="#sec-vt-best-scenario"><span class="title-number">1 </span><span class="title-name">Virtualization scenarios</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-intro"><span class="title-number">2 </span><span class="title-name">Before you apply modifications</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-reco"><span class="title-number">3 </span><span class="title-name">Recommendations</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-hostlevel"><span class="title-number">4 </span><span class="title-name">VM Host Server configuration and resource allocation</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-img"><span class="title-number">5 </span><span class="title-name">VM Guest images</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-vmguests"><span class="title-number">6 </span><span class="title-name">VM Guest configuration</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-vm-setup-config"><span class="title-number">7 </span><span class="title-name">VM Guest-specific configurations and settings</span></a></span></li><li><span class="sect1"><a href="#sec-vt-best-refs"><span class="title-number">8 </span><span class="title-name">More information</span></a></span></li><li><span class="sect1"><a href="#id-1.13.4.11"><span class="title-number">9 </span><span class="title-name">Legal notice</span></a></span></li><li><span class="sect1"><a href="#id-1.13.4.12"><span class="title-number">10 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>