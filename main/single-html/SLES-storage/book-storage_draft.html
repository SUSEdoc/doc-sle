<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><title>SLES 15 SP7 | Storage Administration Guide</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Storage Administration Guide | SLES 15 SP7"/>
<meta name="description" content="How to manage storage devices on SUSE Linux Enterprise Server"/>
<meta name="product-name" content="SUSE Linux Enterprise Server"/>
<meta name="product-number" content="15 SP7"/>
<meta name="book-title" content="Storage Administration Guide"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="fs@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise Server 15 SP7"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Storage Administration Guide"/>
<meta property="og:description" content="Administer storage devices on SLES"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Storage Administration Guide"/>
<meta name="twitter:description" content="Administer storage devices on SLES"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    
    "inLanguage": "en",
    

    "headline": "Storage Administration Guide",
  
    "description": "How to manage storage devices on SUSE Linux Enterprise Server",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2024-06-26T00:00+02:00",
      
    "datePublished": "2024-06-26T00:00+02:00",
      

    "about": [
      {
        "@type": "Thing",
        "name": "Administration"
      },
      {
        "@type": "Thing",
        "name": "Configuration"
      },
      {
        "@type": "Thing",
        "name": "Storage"
      }
    ],
  
    "mentions": [
      
      { "@type": "SoftwareApplication",
        "name": "SUSE Linux Enterprise Server",
        "softwareVersion": "15 SP7",
        "applicationCategory": "Operating System",
        "operatingSystem": "Linux"
      }
      
    ],
    
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sle/edit/main/xml/book_storage.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage">Storage Administration Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage" data-id-title="Storage Administration Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">15 SP7</span></span></span></div><div class="title-container"><h1 class="title">Storage Administration Guide</h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/book_storage.xml" title="Edit source document"> </a></div></div><div class="abstract"><p>
    This guide provides information about how to manage storage
    devices on a SUSE Linux Enterprise Server.
   </p></div><div><div class="titlepage-revhistory"><a aria-label="Revision History" hreflang="en" href="rh-book-storage.html" target="_blank">Revision History: Storage Administration Guide</a></div></div><div class="date"><span class="imprint-label">Publication Date: </span>October 08, 2024
</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#storage-preface"><span class="title-name">Preface</span></a></span><ul><li><span class="sect1"><a href="#id-1.11.2.3"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.11.2.4"><span class="title-name">Improving the documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.11.2.5"><span class="title-name">Documentation conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.11.2.6"><span class="title-name">Support</span></a></span></li></ul></li><li><span class="part"><a href="#part-filesystems"><span class="title-number">I </span><span class="title-name">File systems and mounting</span></a></span><ul><li><span class="chapter"><a href="#cha-filesystems"><span class="title-number">1 </span><span class="title-name">Overview of file systems in Linux</span></a></span><ul><li><span class="sect1"><a href="#sec-filesystems-glossary"><span class="title-number">1.1 </span><span class="title-name">Terminology</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-major-btrfs"><span class="title-number">1.2 </span><span class="title-name">Btrfs</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-major-xfs"><span class="title-number">1.3 </span><span class="title-name">XFS</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-major-ext2"><span class="title-number">1.4 </span><span class="title-name">Ext2</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-major-ext3"><span class="title-number">1.5 </span><span class="title-name">Ext3</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-major-ext4"><span class="title-number">1.6 </span><span class="title-name">Ext4</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-major-reiser"><span class="title-number">1.7 </span><span class="title-name">ReiserFS</span></a></span></li><li><span class="sect1"><a href="#sec-filessytems-zfs"><span class="title-number">1.8 </span><span class="title-name">OpenZFS and ZFS</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-tmpfs"><span class="title-number">1.9 </span><span class="title-name">tmpfs</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-other"><span class="title-number">1.10 </span><span class="title-name">Other supported file systems</span></a></span></li><li><span class="sect1"><a href="#sec-blacklist-filsystem"><span class="title-number">1.11 </span><span class="title-name">Blocked file systems</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-lfs"><span class="title-number">1.12 </span><span class="title-name">Large file support in Linux</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-stor-limits"><span class="title-number">1.13 </span><span class="title-name">Linux kernel storage limitations</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-trouble-trim"><span class="title-number">1.14 </span><span class="title-name">Freeing unused file system blocks</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-trouble"><span class="title-number">1.15 </span><span class="title-name">Troubleshooting file systems</span></a></span></li><li><span class="sect1"><a href="#sec-filesystems-info"><span class="title-number">1.16 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-resize-fs"><span class="title-number">2 </span><span class="title-name">Resizing file systems</span></a></span><ul><li><span class="sect1"><a href="#sec-resize-fs-usecases"><span class="title-number">2.1 </span><span class="title-name">Use cases</span></a></span></li><li><span class="sect1"><a href="#sec-resize-fs-guidelines"><span class="title-number">2.2 </span><span class="title-name">Guidelines for resizing</span></a></span></li><li><span class="sect1"><a href="#sec-resize-fs-btrfs"><span class="title-number">2.3 </span><span class="title-name">Changing the size of a Btrfs file system</span></a></span></li><li><span class="sect1"><a href="#sec-resize-fs-xfs"><span class="title-number">2.4 </span><span class="title-name">Changing the size of an XFS file system</span></a></span></li><li><span class="sect1"><a href="#sec-resize-fs-ext"><span class="title-number">2.5 </span><span class="title-name">Changing the size of an ext2, ext3, or ext4 file system</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-uuid"><span class="title-number">3 </span><span class="title-name">Mounting storage devices</span></a></span><ul><li><span class="sect1"><a href="#sec-uuid-understanduuid"><span class="title-number">3.1 </span><span class="title-name">Understanding UUIDs</span></a></span></li><li><span class="sect1"><a href="#sec-uuid-udev"><span class="title-number">3.2 </span><span class="title-name">Persistent device names with udev</span></a></span></li><li><span class="sect1"><a href="#sec-netdev-option"><span class="title-number">3.3 </span><span class="title-name">Mounting network storage devices</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-multitiercache"><span class="title-number">4 </span><span class="title-name">Multi-tier caching for block device operations</span></a></span><ul><li><span class="sect1"><a href="#sec-multitiercache-terminology"><span class="title-number">4.1 </span><span class="title-name">General terminology</span></a></span></li><li><span class="sect1"><a href="#sec-multitiercache-caching-modes"><span class="title-number">4.2 </span><span class="title-name">Caching modes</span></a></span></li><li><span class="sect1"><a href="#sec-multitiercache-bcache"><span class="title-number">4.3 </span><span class="title-name"><code class="systemitem">bcache</code></span></a></span></li><li><span class="sect1"><a href="#sec-multitiercache-lvmcache"><span class="title-number">4.4 </span><span class="title-name"><code class="systemitem">lvmcache</code></span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-lvm"><span class="title-number">II </span><span class="title-name">Logical volumes (LVM)</span></a></span><ul><li><span class="chapter"><a href="#cha-lvm"><span class="title-number">5 </span><span class="title-name">LVM configuration</span></a></span><ul><li><span class="sect1"><a href="#sec-lvm-explained"><span class="title-number">5.1 </span><span class="title-name">Understanding the logical volume manager</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-vg"><span class="title-number">5.2 </span><span class="title-name">Creating volume groups</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-lv"><span class="title-number">5.3 </span><span class="title-name">Creating logical volumes</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-activate-vgs"><span class="title-number">5.4 </span><span class="title-name">Automatically activating non-root LVM volume groups</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-vg-resize"><span class="title-number">5.5 </span><span class="title-name">Resizing an existing volume group</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-lv-resize"><span class="title-number">5.6 </span><span class="title-name">Resizing a logical volume</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-delete"><span class="title-number">5.7 </span><span class="title-name">Deleting a volume group or a logical volume</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-onboot"><span class="title-number">5.8 </span><span class="title-name">Disabling LVM on boot</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-cli"><span class="title-number">5.9 </span><span class="title-name">Using LVM commands</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-tagging"><span class="title-number">5.10 </span><span class="title-name">Tagging LVM2 storage objects</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-lvm-snapshots"><span class="title-number">6 </span><span class="title-name">LVM volume snapshots</span></a></span><ul><li><span class="sect1"><a href="#sec-lvm-snapshots-intro"><span class="title-number">6.1 </span><span class="title-name">Understanding volume snapshots</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-snapshots-create"><span class="title-number">6.2 </span><span class="title-name">Creating Linux snapshots with LVM</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-snapshots-monitor"><span class="title-number">6.3 </span><span class="title-name">Monitoring a snapshot</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-snapshots-delete"><span class="title-number">6.4 </span><span class="title-name">Deleting Linux snapshots</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-snapshots-xen-host"><span class="title-number">6.5 </span><span class="title-name">Using snapshots for virtual machines on a virtual host</span></a></span></li><li><span class="sect1"><a href="#sec-lvm-snapshots-rollback"><span class="title-number">6.6 </span><span class="title-name">Merging a snapshot with the source logical volume to revert changes or roll back to a previous state</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-software-raid"><span class="title-number">III </span><span class="title-name">Software RAID</span></a></span><ul><li><span class="chapter"><a href="#cha-raid"><span class="title-number">7 </span><span class="title-name">Software RAID configuration</span></a></span><ul><li><span class="sect1"><a href="#sec-raid-intro"><span class="title-number">7.1 </span><span class="title-name">Understanding RAID levels</span></a></span></li><li><span class="sect1"><a href="#sec-raid-yast"><span class="title-number">7.2 </span><span class="title-name">Soft RAID configuration with YaST</span></a></span></li><li><span class="sect1"><a href="#sec-arm-raid"><span class="title-number">7.3 </span><span class="title-name">Configuring stripe size on RAID 5 on AArch64</span></a></span></li><li><span class="sect1"><a href="#sec-raid-counters"><span class="title-number">7.4 </span><span class="title-name">Monitoring software RAIDs</span></a></span></li><li><span class="sect1"><a href="#sec-raid-more"><span class="title-number">7.5 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-raidroot"><span class="title-number">8 </span><span class="title-name">Configuring software RAID for the root partition</span></a></span><ul><li><span class="sect1"><a href="#sec-raidroot-require"><span class="title-number">8.1 </span><span class="title-name">Prerequisites for using a software RAID device for the root partition</span></a></span></li><li><span class="sect1"><a href="#sec-raidroot-setup"><span class="title-number">8.2 </span><span class="title-name">Setting up the system with a software RAID device for the root (<code class="filename">/</code>) partition</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-raid10"><span class="title-number">9 </span><span class="title-name">Creating software RAID 10 devices</span></a></span><ul><li><span class="sect1"><a href="#sec-raid10-nest"><span class="title-number">9.1 </span><span class="title-name">Creating nested RAID 10 devices with <code class="command">mdadm</code></span></a></span></li><li><span class="sect1"><a href="#sec-raid10-complex"><span class="title-number">9.2 </span><span class="title-name">Creating a complex RAID 10</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-raid-degraded"><span class="title-number">10 </span><span class="title-name">Creating a degraded RAID array</span></a></span></li><li><span class="chapter"><a href="#cha-raid-resize"><span class="title-number">11 </span><span class="title-name">Resizing software RAID arrays with mdadm</span></a></span><ul><li><span class="sect1"><a href="#sec-raid-resize-incr"><span class="title-number">11.1 </span><span class="title-name">Increasing the size of a software RAID</span></a></span></li><li><span class="sect1"><a href="#sec-raid-resize-decr"><span class="title-number">11.2 </span><span class="title-name">Decreasing the size of a software RAID</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-raid-leds"><span class="title-number">12 </span><span class="title-name">Storage enclosure LED utilities for MD software RAIDs</span></a></span><ul><li><span class="sect1"><a href="#sec-raid-leds-ledmon"><span class="title-number">12.1 </span><span class="title-name">The storage enclosure LED monitor service</span></a></span></li><li><span class="sect1"><a href="#sec-raid-leds-ledctl"><span class="title-number">12.2 </span><span class="title-name">The storage enclosure LED control application</span></a></span></li><li><span class="sect1"><a href="#sec-raid-leds-info"><span class="title-number">12.3 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-raidtroubleshooting"><span class="title-number">13 </span><span class="title-name">Troubleshooting software RAIDs</span></a></span><ul><li><span class="sect1"><a href="#sec-raid-trouble-autorecovery"><span class="title-number">13.1 </span><span class="title-name">Recovery after failing disk is back again</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-net-storage"><span class="title-number">IV </span><span class="title-name">Network storage</span></a></span><ul><li><span class="chapter"><a href="#cha-isns"><span class="title-number">14 </span><span class="title-name">iSNS for Linux</span></a></span><ul><li><span class="sect1"><a href="#sec-isns-overview"><span class="title-number">14.1 </span><span class="title-name">How iSNS works</span></a></span></li><li><span class="sect1"><a href="#sec-isns-install"><span class="title-number">14.2 </span><span class="title-name">Installing iSNS server for Linux</span></a></span></li><li><span class="sect1"><a href="#sec-isns-domains"><span class="title-number">14.3 </span><span class="title-name">Configuring iSNS discovery domains</span></a></span></li><li><span class="sect1"><a href="#sec-isns-start"><span class="title-number">14.4 </span><span class="title-name">Starting the iSNS service</span></a></span></li><li><span class="sect1"><a href="#sec-isns-info"><span class="title-number">14.5 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-iscsi"><span class="title-number">15 </span><span class="title-name">Mass storage over IP networks: iSCSI</span></a></span><ul><li><span class="sect1"><a href="#sec-iscsi-install"><span class="title-number">15.1 </span><span class="title-name">Installing the iSCSI LIO target server and iSCSI initiator</span></a></span></li><li><span class="sect1"><a href="#sec-iscsi-target"><span class="title-number">15.2 </span><span class="title-name">Setting up an iSCSI LIO target server</span></a></span></li><li><span class="sect1"><a href="#sec-iscsi-initiator"><span class="title-number">15.3 </span><span class="title-name">Configuring iSCSI initiator</span></a></span></li><li><span class="sect1"><a href="#sec-iscsi-targetcli"><span class="title-number">15.4 </span><span class="title-name">Setting up software targets using targetcli-fb</span></a></span></li><li><span class="sect1"><a href="#sec-iscsi-installation"><span class="title-number">15.5 </span><span class="title-name">Using iSCSI disks when installing</span></a></span></li><li><span class="sect1"><a href="#sec-iscsi-trouble"><span class="title-number">15.6 </span><span class="title-name">Troubleshooting iSCSI</span></a></span></li><li><span class="sect1"><a href="#sec-iscsi-terminology"><span class="title-number">15.7 </span><span class="title-name">iSCSI LIO target terminology</span></a></span></li><li><span class="sect1"><a href="#sec-iscsi-info"><span class="title-number">15.8 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-fcoe"><span class="title-number">16 </span><span class="title-name">Fibre Channel storage over Ethernet networks: FCoE</span></a></span><ul><li><span class="sect1"><a href="#sec-fcoe-installation"><span class="title-number">16.1 </span><span class="title-name">Configuring FCoE interfaces during the installation</span></a></span></li><li><span class="sect1"><a href="#sec-fcoe-install"><span class="title-number">16.2 </span><span class="title-name">Installing FCoE and the YaST FCoE client</span></a></span></li><li><span class="sect1"><a href="#sec-fcoe-yast"><span class="title-number">16.3 </span><span class="title-name">Managing FCoE services with YaST</span></a></span></li><li><span class="sect1"><a href="#sec-fcoe-cli"><span class="title-number">16.4 </span><span class="title-name">Configuring FCoE with commands</span></a></span></li><li><span class="sect1"><a href="#sec-fcoe-admin"><span class="title-number">16.5 </span><span class="title-name">Managing FCoE instances with the FCoE administration tool</span></a></span></li><li><span class="sect1"><a href="#sec-fcoe-info"><span class="title-number">16.6 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-nvmeof"><span class="title-number">17 </span><span class="title-name">NVMe-oF</span></a></span><ul><li><span class="sect1"><a href="#sec-nvmeof-overview"><span class="title-number">17.1 </span><span class="title-name">Overview</span></a></span></li><li><span class="sect1"><a href="#sec-nvmeof-host-configuration"><span class="title-number">17.2 </span><span class="title-name">Setting up an NVMe-oF host</span></a></span></li><li><span class="sect1"><a href="#sec-nvmeof-target-configuration"><span class="title-number">17.3 </span><span class="title-name">Setting up an NVMe-oF target</span></a></span></li><li><span class="sect1"><a href="#sec-nvmeof-hardware"><span class="title-number">17.4 </span><span class="title-name">Special hardware configuration</span></a></span></li><li><span class="sect1"><a href="#sec-nvmeof-boot"><span class="title-number">17.5 </span><span class="title-name">Booting from NVMe-oF over TCP</span></a></span></li><li><span class="sect1"><a href="#sec-nvmeof-more-information"><span class="title-number">17.6 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-multipath"><span class="title-number">18 </span><span class="title-name">Managing multipath I/O for devices</span></a></span><ul><li><span class="sect1"><a href="#sec-multipath-intro"><span class="title-number">18.1 </span><span class="title-name">Understanding multipath I/O</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-hardware"><span class="title-number">18.2 </span><span class="title-name">Hardware support</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-planning"><span class="title-number">18.3 </span><span class="title-name">Planning for multipathing</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-installing"><span class="title-number">18.4 </span><span class="title-name">Installing <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> on multipath systems</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-offline-update"><span class="title-number">18.5 </span><span class="title-name">Updating SLE on multipath systems</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-mpiotools"><span class="title-number">18.6 </span><span class="title-name">Multipath management tools</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-config"><span class="title-number">18.7 </span><span class="title-name">Configuring the system for multipathing</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-conf-file"><span class="title-number">18.8 </span><span class="title-name">Multipath configuration</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-policies-default"><span class="title-number">18.9 </span><span class="title-name">Configuring policies for failover, queuing, and failback</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-grouping"><span class="title-number">18.10 </span><span class="title-name">Configuring path grouping and priorities</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-select-devices"><span class="title-number">18.11 </span><span class="title-name">Selecting devices for multipathing</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-names"><span class="title-number">18.12 </span><span class="title-name">Multipath device names and WWIDs</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-conf-misc"><span class="title-number">18.13 </span><span class="title-name">Miscellaneous options</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-best-practice"><span class="title-number">18.14 </span><span class="title-name">Best practice</span></a></span></li><li><span class="sect1"><a href="#sec-multipath-trouble"><span class="title-number">18.15 </span><span class="title-name">Troubleshooting MPIO</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-nfs"><span class="title-number">19 </span><span class="title-name">Sharing file systems with NFS</span></a></span><ul><li><span class="sect1"><a href="#sec-nfs-overview"><span class="title-number">19.1 </span><span class="title-name">Overview</span></a></span></li><li><span class="sect1"><a href="#sec-nfs-installation"><span class="title-number">19.2 </span><span class="title-name">Installing NFS server</span></a></span></li><li><span class="sect1"><a href="#sec-nfs-configuring-nfs-server"><span class="title-number">19.3 </span><span class="title-name">Configuring NFS server</span></a></span></li><li><span class="sect1"><a href="#sec-nfs-configuring-nfs-clients"><span class="title-number">19.4 </span><span class="title-name">Configuring clients</span></a></span></li><li><span class="sect1"><a href="#sec-nfs-firewall"><span class="title-number">19.5 </span><span class="title-name">Operating an NFS server and clients behind a firewall</span></a></span></li><li><span class="sect1"><a href="#nfs4-acls"><span class="title-number">19.6 </span><span class="title-name">Managing Access Control Lists over NFSv4</span></a></span></li><li><span class="sect1"><a href="#sec-nfs-info"><span class="title-number">19.7 </span><span class="title-name">More information</span></a></span></li><li><span class="sect1"><a href="#sec-nfs-troubleshooting"><span class="title-number">19.8 </span><span class="title-name">Gathering information for NFS troubleshooting</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-samba"><span class="title-number">20 </span><span class="title-name">Samba</span></a></span><ul><li><span class="sect1"><a href="#sec-samba-term"><span class="title-number">20.1 </span><span class="title-name">Terminology</span></a></span></li><li><span class="sect1"><a href="#sec-samba-install"><span class="title-number">20.2 </span><span class="title-name">Installing a Samba server</span></a></span></li><li><span class="sect1"><a href="#sec-samba-serv-start"><span class="title-number">20.3 </span><span class="title-name">Starting and stopping Samba</span></a></span></li><li><span class="sect1"><a href="#sec-samba-serv-inst"><span class="title-number">20.4 </span><span class="title-name">Configuring a Samba server</span></a></span></li><li><span class="sect1"><a href="#sec-samba-client-inst"><span class="title-number">20.5 </span><span class="title-name">Configuring clients</span></a></span></li><li><span class="sect1"><a href="#sec-samba-anmeld-serv"><span class="title-number">20.6 </span><span class="title-name">Samba as login server</span></a></span></li><li><span class="sect1"><a href="#sec-samba-adnet"><span class="title-number">20.7 </span><span class="title-name">Samba server in the network with Active Directory</span></a></span></li><li><span class="sect1"><a href="#sec-samba-advanced"><span class="title-number">20.8 </span><span class="title-name">Advanced topics</span></a></span></li><li><span class="sect1"><a href="#sec-samba-info"><span class="title-number">20.9 </span><span class="title-name">More information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-autofs"><span class="title-number">21 </span><span class="title-name">On-demand mounting with autofs</span></a></span><ul><li><span class="sect1"><a href="#sec-autofs-installation"><span class="title-number">21.1 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="#sec-autofs-configuration"><span class="title-number">21.2 </span><span class="title-name">Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-autofs-debugging"><span class="title-number">21.3 </span><span class="title-name">Operation and debugging</span></a></span></li><li><span class="sect1"><a href="#sec-autofs-nfs"><span class="title-number">21.4 </span><span class="title-name">Auto-mounting an NFS share</span></a></span></li><li><span class="sect1"><a href="#sec-autofs-advanced"><span class="title-number">21.5 </span><span class="title-name">Advanced topics</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#id-1.11.7"><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></span><ul><li><span class="sect1"><a href="#id-1.11.7.4"><span class="title-number">A.1 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.11.3.2.9.7.3.3.4.2"><span class="number">1.1 </span><span class="name">Enabling Btrfs quotas</span></a></span></li><li><span class="figure"><a href="#id-1.11.3.2.9.7.3.3.6.2"><span class="number">1.2 </span><span class="name">Setting quota for a subvolume</span></a></span></li><li><span class="figure"><a href="#id-1.11.3.2.9.7.3.3.6.4"><span class="number">1.3 </span><span class="name">List of subvolumes for a device</span></a></span></li><li><span class="figure"><a href="#fig-lvm-explain"><span class="number">5.1 </span><span class="name">Physical partitioning versus LVM</span></a></span></li><li><span class="figure"><a href="#fig-yast2-lvm-physical-volumes"><span class="number">5.2 </span><span class="name">Physical volumes in the volume group named DATA</span></a></span></li><li><span class="figure"><a href="#fig-yast2-raid3"><span class="number">7.1 </span><span class="name">Example RAID 5 configuration</span></a></span></li><li><span class="figure"><a href="#id-1.11.6.2.7.4"><span class="number">14.1 </span><span class="name">iSNS discovery domains</span></a></span></li><li><span class="figure"><a href="#id-1.11.6.3.4"><span class="number">15.1 </span><span class="name">iSCSI SAN with an iSNS server</span></a></span></li><li><span class="figure"><a href="#id-1.11.6.4.4"><span class="number">16.1 </span><span class="name">Open Fibre channel over Ethernet SAN</span></a></span></li><li><span class="figure"><a href="#fig-inst-nfsserver1"><span class="number">19.1 </span><span class="name">NFS server configuration tool</span></a></span></li><li><span class="figure"><a href="#id-1.11.6.8.10.4.3.2"><span class="number">20.1 </span><span class="name">Determining Windows domain membership</span></a></span></li><li><span class="figure"><a href="#id-1.11.6.8.11.4.3"><span class="number">20.2 </span><span class="name">Windows Explorer <span class="guimenu">Advanced Attributes</span> dialog</span></a></span></li><li><span class="figure"><a href="#id-1.11.6.8.11.4.6"><span class="number">20.3 </span><span class="name">Windows Explorer directory listing with compressed files</span></a></span></li><li><span class="figure"><a href="#fig-yast2-samba-snapshot"><span class="number">20.4 </span><span class="name">Adding a new Samba share with snapshots enabled</span></a></span></li><li><span class="figure"><a href="#fig-samba-win-explorer"><span class="number">20.5 </span><span class="name">The <span class="guimenu">Previous versions</span> tab in Windows explorer</span></a></span></li></ul></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><ul><li><span class="table"><a href="#tab-filesystems-other"><span class="number">1.1 </span><span class="name">File system types in Linux</span></a></span></li><li><span class="table"><a href="#tab-filesystems-maxsize"><span class="number">1.2 </span><span class="name">Maximum sizes of files and file systems (on-disk format, 4 KiB block size)</span></a></span></li><li><span class="table"><a href="#tab-filesystems-stor-limits"><span class="number">1.3 </span><span class="name">Storage limitations</span></a></span></li><li><span class="table"><a href="#id-1.11.3.3.5.5.3"><span class="number">2.1 </span><span class="name">File system support for resizing</span></a></span></li><li><span class="table"><a href="#id-1.11.5.2.6.8.5"><span class="number">7.1 </span><span class="name">Comparison of RAID 5 and RAID 6</span></a></span></li><li><span class="table"><a href="#id-1.11.5.4.4.6"><span class="number">9.1 </span><span class="name">Nested RAID levels</span></a></span></li><li><span class="table"><a href="#id-1.11.5.4.4.7.5"><span class="number">9.2 </span><span class="name">Scenario for creating a RAID 10 (1+0) by nesting</span></a></span></li><li><span class="table"><a href="#id-1.11.5.4.4.8.6"><span class="number">9.3 </span><span class="name">Scenario for creating a RAID 10 (0+1) by nesting</span></a></span></li><li><span class="table"><a href="#id-1.11.5.4.5.4"><span class="number">9.4 </span><span class="name">Complex RAID 10 compared to nested RAID 10</span></a></span></li><li><span class="table"><a href="#id-1.11.5.4.5.8.3"><span class="number">9.5 </span><span class="name">Scenario for creating a RAID 10 using mdadm</span></a></span></li><li><span class="table"><a href="#id-1.11.5.6.7"><span class="number">11.1 </span><span class="name">Tasks involved in resizing a RAID</span></a></span></li><li><span class="table"><a href="#id-1.11.5.6.9"><span class="number">11.2 </span><span class="name">Scenario for increasing the size of component partitions</span></a></span></li><li><span class="table"><a href="#id-1.11.5.7.9.9.5"><span class="number">12.1 </span><span class="name">Translation between non-SES-2 patterns and SES-2 patterns</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#dat-cd-rom"><span class="number">20.1 </span><span class="name">A CD-ROM share</span></a></span></li><li><span class="example"><a href="#dat-homes-frei"><span class="number">20.2 </span><span class="name">[homes] share</span></a></span></li><li><span class="example"><a href="#dat-samba-smb-conf-dom"><span class="number">20.3 </span><span class="name">Global section in smb.conf</span></a></span></li><li><span class="example"><a href="#id-1.11.6.8.11.5.5.3"><span class="number">20.4 </span><span class="name">Using <code class="command">rpcclient</code> to request a Windows server 2012 share snapshot</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.11.1.6"><p>
  Copyright © 2006–2024

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="storage-preface" data-id-title="Preface"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">Preface</span></span> <a title="Permalink" class="permalink" href="#storage-preface">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_intro.xml" title="Edit source document"> </a></div></div></div></div></div><section xml:lang="en" class="sect1" id="id-1.11.2.3" data-id-title="Available documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Available documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.11.2.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_intro_available_doc.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.2.3.3.1"><span class="term">Online documentation</span></dt><dd><p>
     Our documentation is available online at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
     Browse or download the documentation in various formats.
    </p><div id="id-1.11.2.3.3.1.2.2" data-id-title="Latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Latest updates</div><p>
      The latest updates are usually available in the English-language version of this documentation.
     </p></div></dd><dt id="id-1.11.2.3.3.2"><span class="term">SUSE Knowledgebase</span></dt><dd><p>
     If you run into an issue, check out the Technical Information
     Documents (TIDs) that are available online at <a class="link" href="https://www.suse.com/support/kb/" target="_blank">https://www.suse.com/support/kb/</a>.
     Search the SUSE Knowledgebase for known solutions driven by customer need.
     </p></dd><dt id="id-1.11.2.3.3.3"><span class="term">Release notes</span></dt><dd><p>
     For release notes, see
     <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
    </p></dd><dt id="id-1.11.2.3.3.4"><span class="term">In your system</span></dt><dd><p>
      For offline use, the release notes are also available under
      <code class="filename">/usr/share/doc/release-notes</code> on your system.
      The documentation for individual packages is available at
      <code class="filename">/usr/share/doc/packages</code>.
    </p><p>
      Many commands are also described in their <span class="emphasis"><em>manual
      pages</em></span>. To view them, run <code class="command">man</code>, followed
      by a specific command name. If the <code class="command">man</code> command is
      not installed on your system, install it with <code class="command">sudo zypper
      install man</code>.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.11.2.4" data-id-title="Improving the documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Improving the documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.11.2.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_intro_feedback.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Your feedback and contributions to this documentation are welcome.
  The following channels for giving feedback are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.2.4.4.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.11.2.4.4.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
    </p><p>
     To simplify this process, click the <span class="guimenu">Report
     an issue</span> icon next to a headline in the HTML
     version of this document. This preselects the right product and
     category in Bugzilla and adds a link to the current section.
     You can start typing your bug report right away.
    </p><p>
     A Bugzilla account is required.
    </p></dd><dt id="id-1.11.2.4.4.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, click the <span class="guimenu">Edit source
     document</span> icon next to a headline in the HTML version of
     this document. This will take you to the source code on GitHub, where you
     can open a pull request.</p><p>
     A GitHub account is required.
    </p><div id="id-1.11.2.4.4.3.2.3" data-id-title="Edit source document only available for English" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <span class="guimenu">Edit source document</span> only available for English</div><p>
      The <span class="guimenu">Edit source document</span> icons are only available for the
      English version of each document. For all other languages, use the
      <span class="guimenu">Report an issue</span> icons instead.
     </p></div><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README.
    </p></dd><dt id="id-1.11.2.4.4.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.11.2.5" data-id-title="Documentation conventions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Documentation conventions</span></span> <a title="Permalink" class="permalink" href="#id-1.11.2.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_intro_convention.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of a user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. You can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as a non-privileged user:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands can be split into two or multiple lines by a backslash character
    (<code class="literal">\</code>) at the end of a line. The backslash informs the shell that
    the command invocation will continue after the end of the line:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">echo</code> a b \
c d</pre></div></li><li class="listitem"><p>
     A code block that shows both the command (preceded by a prompt)
     and the respective output returned by the shell:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code>
output</pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.11.2.5.4.15.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Warning notice</div><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.11.2.5.4.15.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Important notice</div><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.11.2.5.4.15.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Note notice</div><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.11.2.5.4.15.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Tip notice</div><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.11.2.5.4.16.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.11.2.5.4.16.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div></section><section xml:lang="en" class="sect1" id="id-1.11.2.6" data-id-title="Support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Support</span></span> <a title="Permalink" class="permalink" href="#id-1.11.2.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Find the support statement for <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
  
  
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.11.2.6.5" data-id-title="Support statement for SUSE Linux Enterprise Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Support statement for <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.11.2.6.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offers available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.2.6.5.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.11.2.6.5.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate a problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.11.2.6.5.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.11.2.6.6" data-id-title="Technology previews"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span></span> <a title="Permalink" class="permalink" href="#id-1.11.2.6.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback.
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or otherwise
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes" target="_blank">https://www.suse.com/releasenotes</a>.
  </p></section></section></section><div class="part" id="part-filesystems" data-id-title="File systems and mounting"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part I </span><span class="title-name">File systems and mounting </span></span><a title="Permalink" class="permalink" href="#part-filesystems">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/book_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-filesystems"><span class="title-number">1 </span><span class="title-name">Overview of file systems in Linux</span></a></span></li><dd class="toc-abstract"><p>
    <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> ships with different file systems from which to choose,
    including Btrfs, Ext4, Ext3, Ext2 and XFS. Each file system has its own
    advantages and disadvantages. For a side-by-side feature comparison of the
    major file systems in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, see
    <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP3/#file-system-comparison" target="_blank">https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP3/#file-system-comparison</a>
    (<em class="citetitle">Comparison of supported file systems</em>). This chapter
    contains an overview of how these file systems work and what advantages
    they offer.
   </p></dd><li><span class="chapter"><a href="#cha-resize-fs"><span class="title-number">2 </span><span class="title-name">Resizing file systems</span></a></span></li><dd class="toc-abstract"><p>
  Resizing file systems—not to be confused with resizing partitions or
  volumes—can be used to make space available on physical volumes or to
  use additional space available on a physical volume.
 </p></dd><li><span class="chapter"><a href="#cha-uuid"><span class="title-number">3 </span><span class="title-name">Mounting storage devices</span></a></span></li><dd class="toc-abstract"><p>
  This section gives an overview of which device identificators are used during
  mounting of devices, and provides details about mounting network storages.
 </p></dd><li><span class="chapter"><a href="#cha-multitiercache"><span class="title-number">4 </span><span class="title-name">Multi-tier caching for block device operations</span></a></span></li><dd class="toc-abstract"><p>
  A multi-tier cache is a replicated/distributed cache that consists of at
  least two tiers: one is represented by slower but cheaper rotational block
  devices (hard disks), while the other is more expensive but performs faster
  data operations (for example SSD flash disks).
 </p></dd></ul></div><section xml:lang="en" class="chapter" id="cha-filesystems" data-id-title="Overview of file systems in Linux"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Overview of file systems in Linux</span></span> <a title="Permalink" class="permalink" href="#cha-filesystems">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> ships with different file systems from which to choose,
    including Btrfs, Ext4, Ext3, Ext2 and XFS. Each file system has its own
    advantages and disadvantages. For a side-by-side feature comparison of the
    major file systems in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, see
    <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP3/#file-system-comparison" target="_blank">https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP3/#file-system-comparison</a>
    (<em class="citetitle">Comparison of supported file systems</em>). This chapter
    contains an overview of how these file systems work and what advantages
    they offer.
   </p></div></div></div></div><p>
  Btrfs is the default file system for the operating system and
  XFS is the default for all other use cases. SUSE also continues to support
  the Ext family of file systems and OCFS2. By default, the Btrfs file system
  will be set up with subvolumes. Snapshots will be automatically enabled for
  the root file system using the snapper infrastructure. For more information
  about snapper, refer to <span class="intraxref">Book “Administration Guide”, Chapter 10 “System recovery and snapshot management with Snapper”</span>.
 </p><p>
  Professional high-performance setups might require a highly available storage
  system. To meet the requirements of high-performance clustering scenarios,
  <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> includes OCFS2 (Oracle Cluster File System 2) and the
  Distributed Replicated Block Device (DRBD) in the High Availability add-on. These
  advanced storage systems are not covered in this guide. For information, see
  the
  <a class="link" href="https://documentation.suse.com/sle-ha-15/html/SLE-HA-all/book-administration.html" target="_blank">
  <em class="citetitle">Administration Guide for SUSE Linux Enterprise High Availability</em></a>.
 </p><p>
  It is very important to remember that no file system best suits all kinds of
  applications. Each file system has its particular strengths and weaknesses,
  which must be taken into account. In addition, even the most sophisticated
  file system cannot replace a reasonable backup strategy.
 </p><p>
  The terms <span class="emphasis"><em>data integrity</em></span> and <span class="emphasis"><em>data
  consistency</em></span>, when used in this section, do not refer to the
  consistency of the user space data (the data your application writes to its
  files). Whether this data is consistent must be controlled by the application
  itself.
 </p><p>
  Unless stated otherwise in this section, all the steps required to set up or
  change partitions and file systems can be performed by using the YaST
  Partitioner (which is also strongly recommended). For information, see
  <span class="intraxref">Book “Deployment Guide”, Chapter 11 “<span class="guimenu">Expert Partitioner</span>”</span>.
 </p><section class="sect1" id="sec-filesystems-glossary" data-id-title="Terminology"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.1 </span><span class="title-name">Terminology</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-glossary">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.2.8.2.1"><span class="term">metadata</span></dt><dd><p>
      A data structure that is internal to the file system. It ensures that all
      of the on-disk data is properly organized and accessible. Almost every
      file system has its own structure of metadata, which is one reason the
      file systems show different performance characteristics. It is extremely
      important to maintain metadata intact, because otherwise all data on the
      file system could become inaccessible.
     </p></dd><dt id="id-1.11.3.2.8.2.2"><span class="term">inode</span></dt><dd><p>
      A data structure on a file system that contains a variety of information
      about a file, including size, number of links, pointers to the disk
      blocks where the file contents are actually stored, and date and time of
      creation, modification, and access.
     </p></dd><dt id="id-1.11.3.2.8.2.3"><span class="term">journal</span></dt><dd><p>
      In the context of a file system, a journal is an on-disk structure
      containing a type of log in which the file system stores what it is about
      to change in the file system’s metadata. Journaling greatly reduces the
      recovery time of a file system because it has no need for the lengthy
      search process that checks the entire file system at system start-up.
      Instead, only the journal is replayed.
     </p></dd></dl></div></section><section class="sect1" id="sec-filesystems-major-btrfs" data-id-title="Btrfs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.2 </span><span class="title-name">Btrfs</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Btrfs is a copy-on-write (COW) file system developed by Chris Mason. It is
   based on COW-friendly B-trees developed by Ohad Rodeh. Btrfs is a
   logging-style file system. Instead of journaling the block changes, it
   writes them in a new location, then links the change in. Until the last
   write, the new changes are not committed.
  </p><section class="sect2" id="sec-filesystems-major-btrfs-features" data-id-title="Key features"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.1 </span><span class="title-name">Key features</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-features">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Btrfs provides fault tolerance, repair, and easy management features, such
    as the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Writable snapshots that allow you to easily roll back your system if
      needed after applying updates, or to back up files.
     </p></li><li class="listitem"><p>
      Subvolume support: Btrfs creates a default subvolume in its assigned pool
      of space. It allows you to create additional subvolumes that act as
      individual file systems within the same pool of space. The number of
      subvolumes is limited only by the space allocated to the pool.
     </p></li><li class="listitem"><p>
      The online check and repair functionality <code class="command">scrub</code> is
      available as part of the Btrfs command line tools. It verifies the
      integrity of data and metadata, assuming the tree structure is fine. You
      can run scrub periodically on a mounted file system; it runs as a
      background process during normal operation.
     </p></li><li class="listitem"><p>
      Different RAID levels for metadata and user data.
     </p></li><li class="listitem"><p>
      Different checksums for metadata and user data to improve error
      detection.
     </p></li><li class="listitem"><p>
      Integration with Linux Logical Volume Manager (LVM) storage objects.
     </p></li><li class="listitem"><p>
      Integration with the YaST Partitioner and AutoYaST on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>. This
      also includes creating a Btrfs file system on Multiple Devices (MD) and
      Device Mapper (DM) storage configurations.
     </p></li><li class="listitem"><p>
      Offline migration from existing Ext2, Ext3, and Ext4 file systems.
     </p></li><li class="listitem"><p>
      Boot loader support for <code class="filename">/boot</code>, allowing to boot from
      a Btrfs partition.
     </p></li><li class="listitem"><p>
      Multivolume Btrfs is supported in RAID0, RAID1, and RAID10 profiles in
      <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> <span class="productnumber"><span class="phrase">15 SP7</span></span>. Higher RAID levels are not supported yet,
      but might be enabled with a future service pack.
     </p></li><li class="listitem"><p>
      Use Btrfs commands to set up transparent compression.
     </p></li></ul></div></section><section class="sect2" id="sec-filesystems-major-btrfs-suse" data-id-title="The root file system setup on SUSE Linux Enterprise Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.2 </span><span class="title-name">The root file system setup on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-suse">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    By default, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> is set up using Btrfs and snapshots for the root
    partition. Snapshots allow you to easily roll back your system if needed
    after applying updates, or to back up files. Snapshots can easily be
    managed with the SUSE Snapper infrastructure as explained in
    <span class="intraxref">Book “Administration Guide”, Chapter 10 “System recovery and snapshot management with Snapper”</span>. For general information about the SUSE
    Snapper project, see the Snapper Portal wiki at OpenSUSE.org
    (<a class="link" href="http://snapper.io" target="_blank">http://snapper.io</a>).
   </p><p>
    When using a snapshot to roll back the system, it must be ensured that data
    such as user's home directories, Web and FTP server contents or log files
    do not get lost or overwritten during a roll back. This is achieved by
    using Btrfs subvolumes on the root file system. Subvolumes can be excluded
    from snapshots. The default root file system setup on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> as
    proposed by YaST during the installation contains the following
    subvolumes. They are excluded from snapshots for the reasons given below.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.2.9.4.4.2"><span class="term"><code class="filename">/boot/grub2/i386-pc</code>,
   <code class="filename">/boot/grub2/x86_64-efi</code>,
   <code class="filename">/boot/grub2/powerpc-ieee1275</code>,
   <code class="filename">/boot/grub2/s390x-emu</code>
  </span></dt><dd><p>
    A rollback of the boot loader configuration is not supported. The
    directories listed above are architecture-specific. The first two
    directories are present on AMD64/Intel 64 machines, the latter two on IBM
    POWER and on IBM Z, respectively.
   </p></dd><dt id="id-1.11.3.2.9.4.4.3"><span class="term"><code class="filename">/home</code>
  </span></dt><dd><p>
    If <code class="filename">/home</code> does not reside on a separate partition, it
    is excluded to avoid data loss on rollbacks.
   </p></dd><dt id="id-1.11.3.2.9.4.4.4"><span class="term"><code class="filename">/opt</code>
  </span></dt><dd><p>
    Third-party products usually get installed to <code class="filename">/opt</code>. It
    is excluded to avoid uninstalling these applications on rollbacks.
   </p></dd><dt id="id-1.11.3.2.9.4.4.5"><span class="term"><code class="filename">/srv</code>
  </span></dt><dd><p>
    Contains data for Web and FTP servers. It is excluded to avoid data loss on
    rollbacks.
   </p></dd><dt id="id-1.11.3.2.9.4.4.6"><span class="term"><code class="filename">/tmp</code>
  </span></dt><dd><p>
    All directories containing temporary files and caches are excluded from
    snapshots.
   </p></dd><dt id="id-1.11.3.2.9.4.4.7"><span class="term"><code class="filename">/usr/local</code>
  </span></dt><dd><p>
    This directory is used when manually installing software. It is excluded to
    avoid uninstalling these installations on rollbacks.
   </p></dd><dt id="id-1.11.3.2.9.4.4.8"><span class="term"><code class="filename">/var</code>
  </span></dt><dd><p>
    This directory contains many variable files, including logs, temporary
    caches, third party products in <code class="filename">/var/opt</code>, and is the
    default location for virtual machine images and databases. Therefore this
    subvolume is created to exclude all of this variable data from snapshots
    and has Copy-On-Write disabled.
   </p></dd></dl></div><div id="id-1.11.3.2.9.4.5" data-id-title="Support for rollbacks" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Support for rollbacks</div><p>
     Rollbacks are only supported by SUSE if you do not remove any of the
     preconfigured subvolumes. You may, however, add subvolumes using the
     YaST Partitioner.
    </p></div><section class="sect3" id="sec-filesystems-major-btrfs-compress" data-id-title="Mounting compressed Btrfs file systems"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.2.1 </span><span class="title-name">Mounting compressed Btrfs file systems</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-compress">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The Btrfs file system supports transparent compression. While enabled,
     Btrfs compresses file data when written and uncompresses file data when
     read.
    </p><p>
     Use the <code class="option">compress</code> or <code class="option">compress-force</code> mount
     option and select the compression algorithm, <code class="literal">zstd</code>,
     <code class="literal">lzo</code>, or <code class="literal">zlib</code> (the default). zlib
     compression has a higher compression ratio while lzo is faster and takes
     less CPU load. The zstd algorithm offers a modern compromise, with
     performance close to lzo and compression ratios similar to zlib.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -o compress=zstd /dev/sdx /mnt</pre></div><p>
     In case you create a file, write to it, and the compressed result is
     greater or equal to the uncompressed size, Btrfs will skip compression for
     future write operations forever for this file. If you do not like this
     behavior, use the <code class="option">compress-force</code> option. This can be
     useful for files that have some initial non-compressible data.
    </p><p>
     Note, compression takes effect for new files only. Files that were written
     without compression are not compressed when the file system is mounted
     with the <code class="option">compress</code> or <code class="option">compress-force</code>
     option. Furthermore, files with the <code class="option">nodatacow</code> attribute
     never get their extents compressed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">chattr</code> +C <em class="replaceable">FILE</em>
<code class="prompt root"># </code><code class="command">mount</code> -o nodatacow  /dev/sdx /mnt</pre></div><p>
     In regard to encryption, this is independent from any compression. After
     you have written some data to this partition, print the details:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>btrfs filesystem show /mnt
btrfs filesystem show /mnt
Label: 'Test-Btrfs'  uuid: 62f0c378-e93e-4aa1-9532-93c6b780749d
        Total devices 1 FS bytes used 3.22MiB
      devid    1 size 2.00GiB used 240.62MiB path /dev/sdb1</pre></div><p>
     If you want this to be permanent, add the <code class="option">compress</code> or
     <code class="option">compress-force</code> option into the
     <code class="filename">/etc/fstab</code> configuration file. For example:
    </p><div class="verbatim-wrap"><pre class="screen">UUID=1a2b3c4d /home btrfs subvol=@/home,<span class="strong"><strong>compress</strong></span> 0 0</pre></div></section><section class="sect3" id="sec-filesystems-major-btrfs-suse-mount" data-id-title="Mounting subvolumes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.2.2 </span><span class="title-name">Mounting subvolumes</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-suse-mount">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     A system rollback from a snapshot on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> is performed by booting
     from the snapshot first. This allows you to check the snapshot while
     running before doing the rollback. Being able to boot from snapshots is
     achieved by mounting the subvolumes (which would normally not be
     necessary).
    </p><p>
     In addition to the subvolumes listed in
     <a class="xref" href="#sec-filesystems-major-btrfs-suse" title="1.2.2. The root file system setup on SUSE Linux Enterprise Server">Section 1.2.2, “The root file system setup on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>”</a> a volume named
     <code class="literal">@</code> exists. This is the default subvolume that will be
     mounted as the root partition (<code class="filename">/</code>). The other
     subvolumes will be mounted into this volume.
    </p><p>
     When booting from a snapshot, not the <code class="literal">@</code> subvolume will
     be used, but rather the snapshot. The parts of the file system included in
     the snapshot will be mounted read-only as <code class="filename">/</code>. The
     other subvolumes will be mounted writable into the snapshot. This state is
     temporary by default: the previous configuration will be restored with the
     next reboot. To make it permanent, execute the <code class="command">snapper
     rollback</code> command. This will make the snapshot that is currently
     booted the new <span class="emphasis"><em>default</em></span> subvolume, which will be used
     after a reboot.
    </p></section><section class="sect3" id="sec-filesystems-major-btrfs-suse-space" data-id-title="Checking for free space"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.2.3 </span><span class="title-name">Checking for free space</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-suse-space">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     File system usage is usually checked by running the <code class="command">df</code>
     command. On a Btrfs file system, the output of <code class="command">df</code> can
     be misleading, because in addition to the space the raw data allocates, a
     Btrfs file system also allocates and uses space for metadata.
    </p><p>
     Consequently a Btrfs file system may report being out of space even though
     it seems that plenty of space is still available. In that case, all space
     allocated for the metadata is used up. Use the following commands to check
     for used and available space on a Btrfs file system:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.2.9.4.8.4.1"><span class="term"><code class="command">btrfs filesystem show</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem show /
Label: 'ROOT'  uuid: 52011c5e-5711-42d8-8c50-718a005ec4b3
        Total devices 1 FS bytes used 10.02GiB
        devid    1 size 20.02GiB used 13.78GiB path /dev/sda3</pre></div><p>
        Shows the total size of the file system and its usage. If these two
        values in the last line match, all space on the file system has been
        allocated.
       </p></dd><dt id="id-1.11.3.2.9.4.8.4.2"><span class="term"><code class="command">btrfs filesystem df</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem df /
Data, single: total=13.00GiB, used=9.61GiB
System, single: total=32.00MiB, used=16.00KiB
Metadata, single: total=768.00MiB, used=421.36MiB
GlobalReserve, single: total=144.00MiB, used=0.00B</pre></div><p>
        Shows values for allocated (<code class="literal">total</code>) and used space of
        the file system. If the values for <code class="literal">total</code> and
        <code class="literal">used</code> for the metadata are almost equal, all space
        for metadata has been allocated.
       </p></dd><dt id="id-1.11.3.2.9.4.8.4.3"><span class="term"><code class="command">btrfs filesystem usage</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem usage /
Overall:
    Device size:                  20.02GiB
    Device allocated:             13.78GiB
    Device unallocated:            6.24GiB
    Device missing:                  0.00B
    Used:                         10.02GiB
    Free (estimated):              9.63GiB      (min: 9.63GiB)
    Data ratio:                       1.00
    Metadata ratio:                   1.00
    Global reserve:              144.00MiB      (used: 0.00B)

             Data     Metadata  System
Id Path      single   single    single   Unallocated
-- --------- -------- --------- -------- -----------
 1 /dev/sda3 13.00GiB 768.00MiB 32.00MiB     6.24GiB
-- --------- -------- --------- -------- -----------
   Total     13.00GiB 768.00MiB 32.00MiB     6.24GiB
   Used       9.61GiB 421.36MiB 16.00KiB</pre></div><p>
        Shows data similar to that of the two previous commands combined.
       </p></dd></dl></div><p>
     For more information refer to <code class="command">man 8 btrfs-filesystem</code>
     and <a class="link" href="https://btrfs.wiki.kernel.org/index.php/FAQ" target="_blank">https://btrfs.wiki.kernel.org/index.php/FAQ</a>.
    </p></section></section><section class="sect2" id="sec-filesystems-major-btrfs-migrate" data-id-title="Migration from ReiserFS and ext file systems to Btrfs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.3 </span><span class="title-name">Migration from ReiserFS and ext file systems to Btrfs</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-migrate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can migrate data volumes from existing ReiserFS or Ext (Ext2, Ext3, or
    Ext4) to the Btrfs file system using the <code class="command">btrfs-convert</code>
    tool. This allows you to do an in-place conversion of unmounted (offline)
    file systems, which may require a bootable install media with the
    <code class="command">btrfs-convert</code> tool. The tool constructs a Btrfs file
    system within the free space of the original file system, directly linking
    to the data contained in it. There must be enough free space on the device
    to create the metadata or the conversion will fail. The original file
    system will be intact and no free space will be occupied by the Btrfs file
    system. The amount of space required is dependent on the content of the
    file system and can vary based on the number of file system objects (such
    as files, directories, extended attributes) contained in it. Since the data
    is directly referenced, the amount of data on the file system does not
    impact the space required for conversion, except for files that use tail
    packing and are larger than about 2 KiB in size.
   </p><div id="id-1.11.3.2.9.5.3" data-id-title="Root file system conversion not supported" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Root file system conversion not supported</div><p>
     Converting the root file system to Btrfs is not supported and not
     recommended. Automating such a conversion is not possible due to various
     steps that need to be tailored to your specific setup—the process
     requires a complex configuration to provide a correct rollback,
     <code class="filename">/boot</code> must be on the root file system, and the system
     must have specific subvolumes, etc. Either keep the existing file system
     or re-install the whole system from scratch.
    </p></div><p>
    To convert the original file system to the Btrfs file system, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>btrfs-convert /path/to/device</pre></div><div id="id-1.11.3.2.9.5.6" data-id-title="Check /etc/fstab" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Check <code class="filename">/etc/fstab</code></div><p>
     After the conversion, you need to ensure that any references to the
     original file system in <code class="filename">/etc/fstab</code> have been adjusted
     to indicate that the device contains a Btrfs file system.
    </p></div><p>
    When converted, the contents of the Btrfs file system will reflect the
    contents of the source file system. The source file system will be
    preserved until you remove the related read-only image created at
    <code class="filename"><em class="replaceable">fs_root</em>/reiserfs_saved/image</code>.
    The image file is effectively a 'snapshot' of the ReiserFS file system
    prior to conversion and will not be modified as the Btrfs file system is
    modified. To remove the image file, remove the
    <code class="filename">reiserfs_saved</code> subvolume:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>btrfs subvolume delete <em class="replaceable">fs_root</em>/reiserfs_saved</pre></div><p>
    To revert the file system back to the original one, use the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>btrfs-convert -r /path/to/device</pre></div><div id="id-1.11.3.2.9.5.11" data-id-title="Lost changes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Lost changes</div><p>
     Any changes you made to the file system while it was mounted as a Btrfs
     file system will be lost. A balance operation must not have been performed
     in the interim, or the file system will not be restored correctly.
    </p></div></section><section class="sect2" id="sec-filesystems-major-btrfs-admin" data-id-title="Btrfs administration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.4 </span><span class="title-name">Btrfs administration</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-admin">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Btrfs is integrated in the YaST Partitioner and AutoYaST. It is available
    during the installation to allow you to set up a solution for the root file
    system. You can use the YaST Partitioner after the installation to view
    and manage Btrfs volumes.
   </p><p>
    Btrfs administration tools are provided in the
    <code class="filename">btrfsprogs</code> package. For information about using Btrfs
    commands, see the <code class="command">man 8 btrfs</code>, <code class="command">man 8
    btrfsck</code>, and <code class="command">man 8 mkfs.btrfs</code> commands. For
    information about Btrfs features, see the <em class="citetitle">Btrfs wiki</em>
    at <a class="link" href="https://btrfs.wiki.kernel.org" target="_blank">https://btrfs.wiki.kernel.org</a>.
   </p></section><section class="sect2" id="sec-filesystems-major-btrfs-quota" data-id-title="Btrfs quota support for subvolumes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.5 </span><span class="title-name">Btrfs quota support for subvolumes</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-quota">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The Btrfs root file system subvolumes (for example,
    <code class="filename">/var/log</code>, <code class="filename">/var/crash</code>, or
    <code class="filename">/var/cache</code>) can use all the available disk space
    during normal operation, and cause a system malfunction. To help avoid this
    situation, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers quota support for Btrfs subvolumes. If you
    set up the root file system from a YaST proposal, you are ready to enable
    and set subvolume quotas.
   </p><section class="sect3" id="setting-btrfs-quotas-using-yast" data-id-title="Setting Btrfs quotas using YaST"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.5.1 </span><span class="title-name">Setting Btrfs quotas using YaST</span></span> <a title="Permalink" class="permalink" href="#setting-btrfs-quotas-using-yast">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To set a quota for a subvolume of the root file system by using YaST,
     proceed as follows:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Start YaST and select
       <span class="guimenu">System</span> › <span class="guimenu">Partitioner</span>
       and confirm the warning with <span class="guimenu">Yes</span>.
      </p></li><li class="step"><p>
       In the left pane, click <span class="guimenu">Btrfs</span>.
      </p></li><li class="step"><p>
       In the main window, select the device for which you want to enable
       subvolume quotas and click <span class="guimenu">Edit</span> at the bottom.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Edit Btrfs</span> window, activate the
       <span class="guimenu">Enable Subvolume Quotas</span> check box and confirm with
       <span class="guimenu">Next</span>.
      </p><div class="figure" id="id-1.11.3.2.9.7.3.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_btrfs_quotas_enable.png"><img src="images/yast2_btrfs_quotas_enable.png" width="75%" alt="Enabling Btrfs quotas" title="Enabling Btrfs quotas"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.1: </span><span class="title-name">Enabling Btrfs quotas </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.2.9.7.3.3.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
       From the list of existing subvolumes, click the subvolume whose size you
       intend to limit by quota and click <span class="guimenu">Edit</span> at the
       bottom.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Edit subvolume of Btrfs</span> window, activate
       <span class="guimenu">Limit size</span> and specify the maximum referenced size.
       Confirm with <span class="guimenu">Accept</span>.
      </p><div class="figure" id="id-1.11.3.2.9.7.3.3.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_btrfs_quotas_set.png"><img src="images/yast2_btrfs_quotas_set.png" width="75%" alt="Setting quota for a subvolume" title="Setting quota for a subvolume"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.2: </span><span class="title-name">Setting quota for a subvolume </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.2.9.7.3.3.6.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div><p>
       The new size limit will be displayed next to the subvolume name:
      </p><div class="figure" id="id-1.11.3.2.9.7.3.3.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_btrfs_quotas_edit.png"><img src="images/yast2_btrfs_quotas_edit.png" width="75%" alt="List of subvolumes for a device" title="List of subvolumes for a device"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.3: </span><span class="title-name">List of subvolumes for a device </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.2.9.7.3.3.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
       Apply changes with <span class="guimenu">Next</span>.
      </p></li></ol></div></div></section><section class="sect3" id="setting-btrfs-quotas-using-cmdline" data-id-title="Setting Btrfs quotas on the command line"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.5.2 </span><span class="title-name">Setting Btrfs quotas on the command line</span></span> <a title="Permalink" class="permalink" href="#setting-btrfs-quotas-using-cmdline">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To set a quota for a subvolume of the root file system on the command
     line, proceed as follows:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Enable quota support:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs quota enable /</pre></div></li><li class="step"><p>
       Get a list of subvolumes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs subvolume list /</pre></div><p>
       Quotas can only be set for existing subvolumes.
      </p></li><li class="step"><p>
       Set a quota for one of the subvolumes that was listed in the previous
       step. A subvolume can either be identified by path (for example
       <code class="filename">/var/tmp</code>) or by <code class="literal">0/<em class="replaceable">SUBVOLUME
       ID</em></code> (for example <code class="literal">0/272</code>). The
       following example sets a quota of 5 GB for
       <code class="filename">/var/tmp</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs qgroup limit 5G /var/tmp</pre></div><p>
       The size can either be specified in bytes (5000000000), kilobytes
       (5000000K), megabytes (5000M), or gigabytes (5G). The resulting values
       in bytes slightly differ, since 1024 Bytes = 1 KiB, 1024 KiB = 1 MiB,
       etc.
      </p></li><li class="step"><p>
       To list the existing quotas, use the following command. The column
       <code class="literal">max_rfer</code> shows the quota in bytes.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs qgroup show -r /</pre></div></li></ol></div></div><div id="id-1.11.3.2.9.7.4.4" data-id-title="Nullifying a quota" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Nullifying a quota</div><p>
      In case you want to nullify an existing quota, set a quota size of
      <code class="literal">none</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs qgroup limit none /var/tmp</pre></div><p>
      To disable quota support for a partition and all its subvolumes, use
      <code class="command">btrfs quota disable</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs quota disable /</pre></div></div></section><section class="sect3" id="setting-btrfs-quotas-for-more-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.5.3 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#setting-btrfs-quotas-for-more-info">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     See the <code class="command">man 8 btrfs-qgroup</code> and <code class="command">man 8
     btrfs-quota</code> for more details. The
     <em class="citetitle">UseCases</em> page on the Btrfs wiki
     (<a class="link" href="https://btrfs.wiki.kernel.org/index.php/UseCases" target="_blank">https://btrfs.wiki.kernel.org/index.php/UseCases</a>)
     also provides more information.
    </p></section></section><section class="sect2" id="sec-filesystems-major-btrfs-swapping" data-id-title="Swapping on Btrfs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.6 </span><span class="title-name">Swapping on Btrfs</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-swapping">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.11.3.2.9.8.2" data-id-title="Snapshots with swapping" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Snapshots with swapping</div><p>
     You will not be able to create a snapshot if the source subvolume contains
     any enabled swap files.
    </p></div><p>
    SLES supports swapping to a file on the Btrfs file system if the
    following criteria relating to the resulting swap file are fulfilled:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The swap file must have the <code class="option">NODATACOW</code> and
      <code class="option">NODATASUM</code> mount options.
     </p></li><li class="listitem"><p>
      The swap file can not be compressed—you can ensure this by setting
      the <code class="option">NODATACOW</code> and <code class="option">NODATASUM</code> mount
      options. Both options disable swap file compression.
     </p></li><li class="listitem"><p>
      The swap file cannot be activated while exclusive operations are
      running—such as device resizing, adding, removing, or replacing, or
      when a balancing operation is running.
     </p></li><li class="listitem"><p>
      The swap file cannot be sparse.
     </p></li><li class="listitem"><p>
      The swap file can not be an inline file.
     </p></li><li class="listitem"><p>
      The swap file must be on a <code class="literal">single</code> allocation profile
      file system.
     </p></li></ul></div></section><section class="sect2" id="sec-filesystems-major-btrfs-s-r" data-id-title="Btrfs send/receive"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.7 </span><span class="title-name">Btrfs send/receive</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-s-r">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Btrfs allows to make snapshots to capture the state of the file system.
    Snapper, for example, uses this feature to create snapshots before and
    after system changes, allowing a rollback. However, together with the
    send/receive feature, snapshots can also be used to create and maintain
    copies of a file system in a remote location. This feature can, for
    example, be used to do incremental backups.
   </p><p>
    A <code class="command">btrfs send</code> operation calculates the difference between
    two read-only snapshots from the same subvolume and sends it to a file or
    to STDOUT. A <code class="command">btrfs receive</code> operation takes the result of
    the send command and applies it to a snapshot.
   </p><section class="sect3" id="sec-filesystems-major-btrfs-s-r-requires" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.7.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-s-r-requires">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To use the send/receive feature, the following requirements need to be
     met:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       A Btrfs file system is required on the source side
       (<code class="literal">send</code>) and on the target side
       (<code class="literal">receive</code>).
      </p></li><li class="listitem"><p>
       Btrfs send/receive operates on snapshots, therefore the respective data
       needs to reside in a Btrfs subvolume.
      </p></li><li class="listitem"><p>
       Snapshots on the source side need to be read-only.
      </p></li><li class="listitem"><p>
       SUSE Linux Enterprise 12 SP2 or better. Earlier versions of SUSE Linux Enterprise do not support
       send/receive.
      </p></li></ul></div></section><section class="sect3" id="sec-filesystems-major-btrfs-s-r-backup" data-id-title="Incremental backups"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.2.7.2 </span><span class="title-name">Incremental backups</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-s-r-backup">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The following procedure shows the basic usage of Btrfs send/receive using
     the example of creating incremental backups of <code class="filename">/data</code>
     (source side) in <code class="filename">/backup/data</code> (target side).
     <code class="filename">/data</code> needs to be a subvolume.
    </p><div class="procedure" id="id-1.11.3.2.9.9.5.3" data-id-title="Initial setup"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 1.1: </span><span class="title-name">Initial setup </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.2.9.9.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create the initial snapshot (called <code class="literal">snapshot_0</code> in
       this example) on the source side and make sure it is written to the
       disk:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs subvolume snapshot -r /data /data/bkp_data
sync</pre></div><p>
       A new subvolume <code class="filename">/data/bkp_data</code> is created. It will
       be used as the basis for the next incremental backup and should be kept
       as a reference.
      </p></li><li class="step"><p>
       Send the initial snapshot to the target side. Since this is the initial
       send/receive operation, the complete snapshot needs to be sent:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> bash -c 'btrfs send /data/bkp_data | btrfs receive /backup'</pre></div><p>
       A new subvolume <code class="filename">/backup/bkp_data</code> is created on the
       target side.
      </p></li></ol></div></div><p>
     When the initial setup has been finished, you can create incremental
     backups and send the differences between the current and previous
     snapshots to the target side. The procedure is always the same:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Create a new snapshot on the source side.
      </p></li><li class="listitem"><p>
       Send the differences to the target side.
      </p></li><li class="listitem"><p>
       Optional: Rename and/or clean up snapshots on both sides.
      </p></li></ol></div><div class="procedure" id="id-1.11.3.2.9.9.5.6" data-id-title="Performing an incremental backup"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 1.2: </span><span class="title-name">Performing an incremental backup </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.2.9.9.5.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a new snapshot on the source side and make sure it is written to
       the disk. In the following example the snapshot is named
       bkp_data_<em class="replaceable">CURRENT_DATE</em>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs subvolume snapshot -r /data /data/bkp_data_$(date +%F)
sync</pre></div><p>
       A new subvolume, for example
       <code class="filename">/data/bkp_data_2016-07-07</code>, is created.
      </p></li><li class="step"><p>
       Send the difference between the previous snapshot and the one you have
       created to the target side. This is achieved by specifying the previous
       snapshot with the option <code class="option">-p
       <em class="replaceable">SNAPSHOT</em></code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> bash -c 'btrfs send -p /data/bkp_data /data/bkp_data_2016-07-07 \
| btrfs receive /backup'</pre></div><p>
       A new subvolume <code class="filename">/backup/bkp_data_2016-07-07</code> is
       created.
      </p></li><li class="step"><p>
       As a result four snapshots, two on each side, exist:
      </p><table style="border: 0; " class="simplelist"><tr><td><code class="filename">/data/bkp_data</code></td></tr><tr><td><code class="filename">/data/bkp_data_2016-07-07</code></td></tr><tr><td><code class="filename">/backup/bkp_data</code></td></tr><tr><td><code class="filename">/backup/bkp_data_2016-07-07</code></td></tr></table><p>
       Now you have three options for how to proceed:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Keep all snapshots on both sides. With this option you can roll back
         to any snapshot on both sides while having all data duplicated at the
         same time. No further action is required. When doing the next
         incremental backup, keep in mind to use the next-to-last snapshot as
         parent for the send operation.
        </p></li><li class="listitem"><p>
         Only keep the last snapshot on the source side and all snapshots on
         the target side. Also allows to roll back to any snapshot on both
         sides—to do a rollback to a specific snapshot on the source
         side, perform a send/receive operation of a complete snapshot from the
         target side to the source side. Do a delete/move operation on the
         source side.
        </p></li><li class="listitem"><p>
         Only keep the last snapshot on both sides. This way you have a backup
         on the target side that represents the state of the last snapshot made
         on the source side. It is not possible to roll back to other
         snapshots. Do a delete/move operation on the source and the target
         side.
        </p></li></ul></div><ol type="a" class="substeps"><li class="step"><p>
         To only keep the last snapshot on the source side, perform the
         following commands:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs subvolume delete /data/bkp_data
<code class="prompt user">&gt; </code><code class="command">sudo</code> mv /data/bkp_data_2016-07-07 /data/bkp_data</pre></div><p>
         The first command will delete the previous snapshot, the second
         command renames the current snapshot to
         <code class="filename">/data/bkp_data</code>. This ensures that the last
         snapshot that was backed up is always named
         <code class="filename">/data/bkp_data</code>. As a consequence, you can also
         always use this subvolume name as a parent for the incremental send
         operation.
        </p></li><li class="step"><p>
         To only keep the last snapshot on the target side, perform the
         following commands:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs subvolume delete /backup/bkp_data
<code class="prompt user">&gt; </code><code class="command">sudo</code> mv /backup/bkp_data_2016-07-07 /backup/bkp_data</pre></div><p>
         The first command will delete the previous backup snapshot, the second
         command renames the current backup snapshot to
         <code class="filename">/backup/bkp_data</code>. This ensures that the latest
         backup snapshot is always named <code class="filename">/backup/bkp_data</code>.
        </p></li></ol></li></ol></div></div><div id="id-1.11.3.2.9.9.5.7" data-id-title="Sending to a remote target side" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Sending to a remote target side</div><p>
      To send the snapshots to a remote machine, use SSH:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>btrfs send /data/bkp_data | ssh root@jupiter.example.com 'btrfs receive /backup'</pre></div></div></section></section><section class="sect2" id="sec-filesystems-major-btrfs-deduplication" data-id-title="Data deduplication support"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.8 </span><span class="title-name">Data deduplication support</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-btrfs-deduplication">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Btrfs supports data deduplication by replacing identical blocks in the file
    system with logical links to a single copy of the block in a common storage
    location. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> provides the tool <code class="command">duperemove</code> for
    scanning the file system for identical blocks. When used on a Btrfs file
    system, it can also be used to deduplicate these blocks and thus save space
    on the file system. <code class="command">duperemove</code> is not installed by
    default. To make it available, install the package
    <span class="package">duperemove</span> .
   </p><div id="id-1.11.3.2.9.10.3" data-id-title="Deduplicating large datasets" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Deduplicating large datasets</div><p>
     If you intend to deduplicate a large amount of files, use the
     <code class="option">--hashfile</code> option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> duperemove <code class="option">--hashfile <em class="replaceable">HASH_FILE</em></code> file1 file2 file3</pre></div><p>
     The <code class="option">--hashfile</code> option stores hashes of all specified
     files into the <em class="replaceable">HASH_FILE</em> instead of RAM and
     prevents it from being exhausted. <em class="replaceable">HASH_FILE</em> is
     reusable—you can deduplicate changes to large datasets very quickly
     after an initial run that generated a baseline hash file.
    </p></div><p>
    <code class="command">duperemove</code> can either operate on a list of files or
    recursively scan a directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> duperemove <em class="replaceable">OPTIONS</em> file1 file2 file3
<code class="prompt user">&gt; </code><code class="command">sudo</code> duperemove -r <em class="replaceable">OPTIONS</em> directory</pre></div><p>
    It operates in two modes: read-only and de-duping. When run in read-only
    mode (that is without the <code class="option">-d</code> switch), it scans the given
    files or directories for duplicated blocks and prints them. This works on
    any file system.
   </p><p>
    Running <code class="command">duperemove</code> in de-duping mode is only supported
    on Btrfs file systems. After having scanned the given files or directories,
    the duplicated blocks will be submitted for deduplication.
   </p><p>
    For more information see <code class="command">man 8 duperemove</code>.
   </p></section><section class="sect2" id="btrfs-delete-subvolumes" data-id-title="Deleting subvolumes from the root file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.9 </span><span class="title-name">Deleting subvolumes from the root file system</span></span> <a title="Permalink" class="permalink" href="#btrfs-delete-subvolumes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You may need to delete one of the default Btrfs subvolumes from the root
    file system for specific purposes. One of them is transforming a
    subvolume—for example <code class="filename">@/home</code> or
    <code class="filename">@/srv</code>—into a file system on a separate device.
    The following procedure illustrates how to delete a Btrfs subvolume:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Identify the subvolume you need to delete (for example
      <code class="filename">@/opt</code>). Notice that the root path has always
      subvolume ID '5'.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs subvolume list /
ID 256 gen 30 top level 5 path @
ID 258 gen 887 top level 256 path @/var
ID 259 gen 872 top level 256 path @/usr/local
ID 260 gen 886 top level 256 path @/tmp
ID 261 gen 60 top level 256 path @/srv
ID 262 gen 886 top level 256 path @/root
ID 263 gen 39 top level 256 path @/opt
[...]</pre></div></li><li class="step"><p>
      Find the device name that hosts the root partition:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs device usage /
/dev/sda1, ID: 1
  Device size:            23.00GiB
  Device slack:              0.00B
  Data,single:             7.01GiB
  Metadata,DUP:            1.00GiB
  System,DUP:             16.00MiB
  Unallocated:            14.98GiB</pre></div></li><li class="step"><p>
      Mount the root file system (subvolume with ID 5) on a separate mount
      point (for example <code class="filename">/mnt</code>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount -o subvolid=5 /dev/sda1 /mnt</pre></div></li><li class="step"><p>
      Delete the <code class="filename">@/opt</code> partition from the mounted root
      file system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs subvolume delete /mnt/@/opt</pre></div></li><li class="step"><p>
      Unmount the previously mounted root file system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> umount /mnt</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-filesystems-major-xfs" data-id-title="XFS"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.3 </span><span class="title-name">XFS</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-xfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Originally intended as the file system for their IRIX OS, SGI started XFS
   development in the early 1990s. The idea behind XFS was to create a
   high-performance 64-bit journaling file system to meet extreme computing
   challenges. XFS is very good at manipulating large files and performs well
   on high-end hardware.
   
   XFS is the default file system for data partitions in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>.
  </p><p>
   A quick review of XFS’s key features explains why it might prove to be a
   strong competitor for other journaling file systems in high-end computing.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.2.10.4.1"><span class="term">High scalability</span></dt><dd><p>
      XFS offers high scalability by using allocation groups
     </p><p>
      At the creation time of an XFS file system, the block device underlying
      the file system is divided into eight or more linear regions of equal
      size. Those are called <span class="emphasis"><em>allocation groups</em></span>. Each
      allocation group manages its own inodes and free disk space. Practically,
      allocation groups can be seen as file systems in a file system. Because
      allocation groups are rather independent of each other, more than one of
      them can be addressed by the kernel simultaneously. This feature is the
      key to XFS’s great scalability. Naturally, the concept of independent
      allocation groups suits the needs of multiprocessor systems.
     </p></dd><dt id="id-1.11.3.2.10.4.2"><span class="term">High performance</span></dt><dd><p>
      XFS offers high performance through efficient management of disk space
     </p><p>
      Free space and inodes are handled by B<sup>+</sup> trees
      inside the allocation groups. The use of B<sup>+</sup>
      trees greatly contributes to XFS’s performance and scalability. XFS uses
      <span class="emphasis"><em>delayed allocation</em></span>, which handles allocation by
      breaking the process into two pieces. A pending transaction is stored in
      RAM and the appropriate amount of space is reserved. XFS still does not
      decide where exactly (in file system blocks) the data should be stored.
      This decision is delayed until the last possible moment. Some short-lived
      temporary data might never make its way to disk, because it is obsolete
      by the time XFS decides where to save it. In this way, XFS
      increases write performance and reduces file system fragmentation.
      Because delayed allocation results in less frequent write events than in
      other file systems, it is likely that data loss after a crash during a
      write is more severe.
     </p></dd><dt id="id-1.11.3.2.10.4.3"><span class="term">Preallocation to avoid file system fragmentation</span></dt><dd><p>
      Before writing the data to the file system, XFS
      <span class="emphasis"><em>reserves</em></span> (preallocates) the free space needed for a
      file. Thus, file system fragmentation is greatly reduced. Performance is
      increased because the contents of a file are not distributed all over the
      file system.
     </p></dd></dl></div><section class="sect2" id="sec-filesystems-major-prealloc" data-id-title="XFS formats"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.3.1 </span><span class="title-name">XFS formats</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-prealloc">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> supports the <span class="quote">“<span class="quote">on-disk format</span>”</span> (v5) of the XFS
    file system. The main advantages of this format are automatic checksums of
    all XFS metadata, file type support, and support for a larger number of
    access control lists for a file.
   </p><p>
    Note that this format is not supported by SUSE Linux Enterprise kernels older than version
    3.12, by <code class="literal">xfsprogs</code> older than version 3.2.0, and GRUB 2
    versions released before SUSE Linux Enterprise 12.
   </p><div id="id-1.11.3.2.10.5.4" data-id-title="V4 is deprecated" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: V4 is deprecated</div><p>
     XFS is deprecating file systems with the V4 format. This file system
     format was created by the command:
    </p><div class="verbatim-wrap"><pre class="screen">mkfs.xfs -m crc=0 <em class="replaceable">DEVICE</em></pre></div><p>
     The format was used in SLE 11 and older releases and currently it creates
     a warning message by <code class="command">dmesg</code>:
    </p><div class="verbatim-wrap"><pre class="screen">Deprecated V4 format (crc=0) will not be supported after September 2030</pre></div><p>
     If you see the message above in the output of the <code class="command">dmesg</code>
     command, it is recommended that you update your file system to the V5
     format:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Back up your data to another device.
      </p></li><li class="step"><p>
       Create the file system on the device.
      </p><div class="verbatim-wrap"><pre class="screen">mkfs.xfs -m crc=1 <em class="replaceable">DEVICE</em></pre></div></li><li class="step"><p>
       Restore the data from the backup on the updated device.
      </p></li></ol></div></div></div></section></section><section class="sect1" id="sec-filesystems-major-ext2" data-id-title="Ext2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.4 </span><span class="title-name">Ext2</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-ext2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The origins of Ext2 go back to the early days of Linux history. Its
   predecessor, the Extended File System, was implemented in April 1992 and
   integrated in Linux 0.96c. The Extended File System underwent several
   modifications and, as Ext2, became the most popular Linux file system for
   years. With the creation of journaling file systems and their short recovery
   times, Ext2 became less important.
  </p><p>
   A brief summary of Ext2’s strengths might help understand why it
   was—and in some areas still is—the favorite Linux file system of
   many Linux users.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.2.11.4.1"><span class="term">Solidity and speed</span></dt><dd><p>
      Being an <span class="quote">“<span class="quote">old-timer</span>”</span>, Ext2 underwent many improvements and
      was heavily tested. This might be the reason people often refer to it as
      rock-solid. After a system outage when the file system could not be
      cleanly unmounted, e2fsck starts to analyze the file system data.
      Metadata is brought into a consistent state and pending files or data
      blocks are written to a designated directory (called
      <code class="filename">lost+found</code>). In contrast to journaling file systems,
      e2fsck analyzes the entire file system and not only the recently modified
      bits of metadata. This takes significantly longer than checking the log
      data of a journaling file system. Depending on file system size, this
      procedure can take half an hour or more. Therefore, it is not desirable
      to choose Ext2 for any server that needs high availability. However,
      because Ext2 does not maintain a journal and uses less memory, it is
      sometimes faster than other file systems.
     </p></dd><dt id="id-1.11.3.2.11.4.2"><span class="term">Easy upgradability</span></dt><dd><p>
      Because Ext3 is based on the Ext2 code and shares its on-disk format and
      its metadata format, upgrades from Ext2 to Ext3 are very easy.
     </p></dd></dl></div></section><section class="sect1" id="sec-filesystems-major-ext3" data-id-title="Ext3"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.5 </span><span class="title-name">Ext3</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-ext3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Ext3 was designed by Stephen Tweedie. Unlike all other next-generation file
   systems, Ext3 does not follow a completely new design principle. It is based
   on Ext2. These two file systems are very closely related to each other. An
   Ext3 file system can be easily built on top of an Ext2 file system. The most
   important difference between Ext2 and Ext3 is that Ext3 supports journaling.
   In summary, Ext3 has three major advantages to offer:
  </p><section class="sect2" id="sec-filesystems-major-ext3-upgrade" data-id-title="Easy and highly reliable upgrades from ext2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.5.1 </span><span class="title-name">Easy and highly reliable upgrades from ext2</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-ext3-upgrade">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The code for Ext2 is the strong foundation on which Ext3 could become a
    highly acclaimed next-generation file system. Its reliability and solidity
    are elegantly combined in Ext3 with the advantages of a journaling file
    system. Unlike transitions to other journaling file systems, such as XFS,
    which can be quite tedious (making backups of the entire file system and
    re-creating it from scratch), a transition to Ext3 is a matter of minutes.
    It is also very safe, because re-creating an entire file system from
    scratch might not work flawlessly. Considering the number of existing Ext2
    systems that await an upgrade to a journaling file system, you can easily
    see why Ext3 might be of some importance to many system administrators.
    Downgrading from Ext3 to Ext2 is as easy as the upgrade. Perform a clean
    unmount of the Ext3 file system and remount it as an Ext2 file system.
   </p></section><section class="sect2" id="sec-filesystems-major-ext3-ext22ext3a" data-id-title="Converting an ext2 file system into ext3"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.5.2 </span><span class="title-name">Converting an ext2 file system into ext3</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-ext3-ext22ext3a">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To convert an Ext2 file system to Ext3:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create an Ext3 journal by running <code class="command">tune2fs -j</code> as the
      <code class="systemitem">root</code> user.
     </p><p>
      This creates an Ext3 journal with the default parameters.
     </p><p>
      To specify how large the journal should be and on which device it should
      reside, run <code class="command">tune2fs</code> <code class="option">-J</code> instead
      together with the desired journal options <code class="option">size=</code> and
      <code class="option">device=</code>. More information about the
      <code class="command">tune2fs</code> program is available in the
      <code class="command">tune2fs</code> man page.
     </p></li><li class="step"><p>
      Edit the file <code class="filename">/etc/fstab</code> as the <code class="systemitem">root</code> user to
      change the file system type specified for the corresponding partition
      from <code class="literal">ext2</code> to <code class="literal">ext3</code>, then save the
      changes.
     </p><p>
      This ensures that the Ext3 file system is recognized as such. The change
      takes effect after the next reboot.
     </p></li><li class="step"><p>
      To boot a root file system that is set up as an Ext3 partition, add the
      modules <code class="literal">ext3</code> and <code class="literal">jbd</code> in the
      <code class="filename">initrd</code>. Do so by
     </p><ol type="a" class="substeps"><li class="step"><p>
        opening or creating
        <code class="filename">/etc/dracut.conf.d/filesystem.conf</code> and adding the
        following line (mind the leading blank space):
       </p><div class="verbatim-wrap"><pre class="screen">force_drivers+=" ext3 jbd"</pre></div></li><li class="step"><p>
        and running the <code class="command">dracut</code> <code class="option">-f</code> command.
       </p></li></ol></li><li class="step"><p>
      Reboot the system.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-filesystems-major-ext4" data-id-title="Ext4"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.6 </span><span class="title-name">Ext4</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-ext4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In 2006, Ext4 started as a fork from Ext3. It is the latest file system in
   the extended file system version. Ext4 was originally designed to increase
   storage size by supporting volumes with a size of up to 1 exbibyte, files
   with a size of up to 16 tebibytes and an unlimited number of subdirectories.
   Ext4 uses extents, instead of the traditional direct and indirect block
   pointers, to map the file contents. Usage of extents improves both storage
   and retrieval of data from disks.
  </p><p>
   Ext4 also introduces several performance enhancements such as delayed block
   allocation and a much faster file system checking routine. Ext4 is also more
   reliable by supporting journal checksums and by providing time stamps
   measured in nanoseconds. Ext4 is fully backward compatible to Ext2 and
   Ext3—both file systems can be mounted as Ext4.
  </p><div id="id-1.11.3.2.13.4" data-id-title="Ext3 functionality on Ext4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Ext3 functionality on Ext4</div><p>
    The Ext3 functionality is fully supported by the Ext4 driver in the Ext4
    kernel module.
   </p></div><section class="sect2" id="sec-filesystems-major-ext4-performance" data-id-title="Reliability and performance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.6.1 </span><span class="title-name">Reliability and performance</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-ext4-performance">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Some other journaling file systems follow the <span class="quote">“<span class="quote">metadata-only</span>”</span>
    journaling approach. This means your metadata is always kept in a
    consistent state, but this cannot be automatically guaranteed for the file
    system data itself. Ext4 is designed to take care of both metadata and
    data. The degree of <span class="quote">“<span class="quote">care</span>”</span> can be customized. Mounting Ext4 in
    the <code class="option">data=journal</code> mode offers maximum security (data
    integrity), but can slow down the system because both metadata and data are
    journaled. Another approach is to use the <code class="option">data=ordered</code>
    mode, which ensures both data and metadata integrity, but uses journaling
    only for metadata. The file system driver collects all data blocks that
    correspond to one metadata update. These data blocks are written to disk
    before the metadata is updated. As a result, consistency is achieved for
    metadata and data without sacrificing performance. A third mount option to
    use is <code class="option">data=writeback</code>, which allows data to be written to
    the main file system after its metadata has been committed to the journal.
    This option is often considered the best in performance. It can, however,
    allow old data to reappear in files after crash and recovery while internal
    file system integrity is maintained. Ext4 uses the
    <code class="option">data=ordered</code> option as the default.
   </p></section><section class="sect2" id="sec-filesystems-major-ext4-inodesize" data-id-title="Ext4 file system inode size and number of inodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.6.2 </span><span class="title-name">Ext4 file system inode size and number of inodes</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-ext4-inodesize">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    An inode stores information about the file and its block location in the
    file system. To allow space in the inode for extended attributes and ACLs,
    the default inode size was increased to 256 bytes.
   </p><p>
    When you create a new Ext4 file system, the space in the inode table is
    preallocated for the total number of inodes that can be created. The
    bytes-per-inode ratio and the size of the file system determine how many
    inodes are possible. When the file system is made, an inode is created for
    every bytes-per-inode bytes of space:
   </p><div class="verbatim-wrap"><pre class="screen">number of inodes = total size of the file system divided by the number of bytes per inode</pre></div><p>
    The number of inodes controls the number of files you can have in the file
    system: one inode for each file.
   </p><div id="id-1.11.3.2.13.6.6" data-id-title="Changing the inode size of an existing Ext4 file system not possible" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Changing the inode size of an existing Ext4 file system not possible</div><p>
     After the inodes are allocated, you cannot change the settings for the
     inode size or bytes-per-inode ratio. No new inodes are possible without
     re-creating the file system with different settings, or unless the file
     system gets extended. When you exceed the maximum number of inodes, no new
     files can be created on the file system until some files are deleted.
    </p></div><p>
    When you make a new Ext4 file system, you can specify the inode size and
    bytes-per-inode ratio to control inode space usage and the number of files
    possible on the file system. If the blocks size, inode size, and
    bytes-per-inode ratio values are not specified, the default values in the
    <code class="filename">/etc/mked2fs.conf</code> file are applied. For information,
    see the <code class="filename">mke2fs.conf(5)</code> man page.
   </p><p>
    Use the following guidelines:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Inode size:</span>
       The default inode size is 256 bytes. Specify a value in bytes that is a
       power of 2 and equal to 128 or larger in bytes and up to the block size,
       such as 128, 256, 512, and so on. Use 128 bytes only if you do not use
       extended attributes or ACLs on your Ext4 file systems.
      </p></li><li class="listitem"><p><span class="formalpara-title">Bytes-per-inode ratio:</span>
       The default bytes-per-inode ratio is 16384 bytes. Valid bytes-per-inode
       ratio values must be a power of 2 equal to 1024 or greater in bytes,
       such as 1024, 2048, 4096, 8192, 16384, 32768, and so on. This value
       should not be smaller than the block size of the file system, because
       the block size is the smallest chunk of space used to store data. The
       default block size for the Ext4 file system is 4 KiB.
      </p><p>
      In addition, consider the number of files and the size of files you need
      to store. For example, if your file system will have many small files,
      you can specify a smaller bytes-per-inode ratio, which increases the
      number of inodes. If your file system will have very large files, you can
      specify a larger bytes-per-inode ratio, which reduces the number of
      possible inodes.
     </p><p>
      Generally, it is better to have too many inodes than to run out of them.
      If you have too few inodes and very small files, you could reach the
      maximum number of files on a disk that is practically empty. If you have
      too many inodes and very large files, you might have free space reported
      but be unable to use it because you cannot create new files in space
      reserved for inodes.
     </p></li></ul></div><p>
    Use any of the following methods to set the inode size and bytes-per-inode
    ratio:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Modifying the default settings for all new Ext4 file systems:</span>
       In a text editor, modify the <code class="literal">defaults</code> section of the
       <code class="filename">/etc/mke2fs.conf</code> file to set the
       <code class="literal">inode_size</code> and <code class="literal">inode_ratio</code> to the
       desired default values. The values apply to all new Ext4 file systems.
       For example:
      </p><div class="verbatim-wrap"><pre class="screen">blocksize = 4096
inode_size = 128
inode_ratio = 8192</pre></div></li><li class="listitem"><p><span class="formalpara-title">At the command line:</span>
       Pass the inode size (<code class="literal">-I 128</code>) and the bytes-per-inode
       ratio (<code class="literal">-i 8192</code>) to the
       <code class="command">mkfs.ext4(8)</code> command or the
       <code class="command">mke2fs(8)</code> command when you create a new Ext4 file
       system. For example, use either of the following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mkfs.ext4 -b 4096 -i 8092 -I 128 /dev/sda2
<code class="prompt user">&gt; </code><code class="command">sudo</code> mke2fs -t ext4 -b 4096 -i 8192 -I 128 /dev/sda2</pre></div></li><li class="listitem"><p><span class="formalpara-title">During installation with YaST:</span>
       Pass the inode size and bytes-per-inode ratio values when you create a
       new Ext4 file system during the installation. In the <span class="guimenu">Expert
       Partitioner</span>, select the partition, click
       <span class="guimenu">Edit</span>. Under <span class="guimenu">Formatting Options</span>,
       select <span class="guimenu">Format device</span><span class="guimenu">Ext4</span>, then
       click <span class="guimenu">Options</span>. In the <span class="guimenu">Format
       options</span> dialog, select the desired values from the
       <span class="guimenu">Block Size in Bytes</span>,
       <span class="guimenu">Bytes-per-inode</span>, and <span class="guimenu">Inode Size</span>
       drop-down box.
      </p><p>
      For example, select 4096 for the <span class="guimenu">Block Size in Bytes</span>
      drop-down box, select 8192 from the <span class="guimenu">Bytes per inode</span>
      drop-down box, select 128 from the <span class="guimenu">Inode Size</span>
      drop-down box, then click <span class="guimenu">OK</span>.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/ext4_inode_yast_a.png"><img src="images/ext4_inode_yast_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li></ul></div></section><section class="sect2" id="sec-upgrade-to-ext4" data-id-title="Upgrading to Ext4"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.6.3 </span><span class="title-name">Upgrading to Ext4</span></span> <a title="Permalink" class="permalink" href="#sec-upgrade-to-ext4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.11.3.2.13.7.2" data-id-title="Backup of data" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Backup of data</div><p>
     Back up all data on the file system before performing any update of your
     file system.
    </p></div><div class="procedure" id="id-1.11.3.2.13.7.3" data-id-title="Upgrading to ext4"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 1.3: </span><span class="title-name">Upgrading to ext4 </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.2.13.7.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      To upgrade from Ext2 or Ext3, you must enable the following:
     </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Features required by Ext4 </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.2.13.7.3.2.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.11.3.2.13.7.3.2.2.2"><span class="term">extents</span></dt><dd><p>
         contiguous blocks on the hard disk that are used to keep files close
         together and prevent fragmentation
        </p></dd><dt id="id-1.11.3.2.13.7.3.2.2.3"><span class="term">unint_bg</span></dt><dd><p>
         lazy inode table initialization
        </p></dd><dt id="id-1.11.3.2.13.7.3.2.2.4"><span class="term">dir_index</span></dt><dd><p>
         hashed b-tree lookups for large directories
        </p></dd><dt id="id-1.11.3.2.13.7.3.2.2.5"><span class="term">on Ext2: <code class="literal">as_journal</code></span></dt><dd><p>
         enable journaling on your Ext2 file system.
        </p></dd></dl></div><p>
      To enable the features, run:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        on Ext3:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>tune2fs -O extents,uninit_bg,dir_index <em class="replaceable">DEVICE_NAME</em></pre></div></li><li class="listitem"><p>
        on Ext2:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>tune2fs -O extents,uninit_bg,dir_index,has_journal <em class="replaceable">DEVICE_NAME</em></pre></div></li></ul></div></li><li class="step"><p>
      As <code class="systemitem">root</code> edit the <code class="filename">/etc/fstab</code> file: change the
      <code class="literal">ext3</code> or <code class="literal">ext2</code> record to
      <code class="literal">ext4</code>. The change takes effect after the next reboot.
     </p></li><li class="step"><p>
      To boot a file system that is set up on an ext4 partition, add the modules:
      <code class="literal">ext4</code> and <code class="literal">jbd</code> in the
      <code class="literal">initramfs</code>. Open or create
      <code class="filename">/etc/dracut.conf.d/filesystem.conf</code> and add the
      following line:
     </p><div class="verbatim-wrap"><pre class="screen">force_drivers+=" ext4 jbd"</pre></div><p>
      You need to overwrite the existing dracut <code class="filename">initramfs</code>
      by running:
     </p><div class="verbatim-wrap"><pre class="screen">dracut -f</pre></div></li><li class="step"><p>
      Reboot your system.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-filesystems-major-reiser" data-id-title="ReiserFS"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.7 </span><span class="title-name">ReiserFS</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-major-reiser">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   ReiserFS support was completely removed with <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 15. To migrate
   your existing partitions to Btrfs, refer to
   <a class="xref" href="#sec-filesystems-major-btrfs-migrate" title="1.2.3. Migration from ReiserFS and ext file systems to Btrfs">Section 1.2.3, “Migration from ReiserFS and ext file systems to Btrfs”</a>.
  </p></section><section class="sect1" id="sec-filessytems-zfs" data-id-title="OpenZFS and ZFS"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.8 </span><span class="title-name">OpenZFS and ZFS</span></span> <a title="Permalink" class="permalink" href="#sec-filessytems-zfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Neither the OpenZFS nor ZFS file systems are supported by
SUSE. Although ZFS was originally released by Sun under an open source
license, the current Oracle Solaris ZFS is now closed source, and
therefore cannot be used by SUSE. OpenZFS (based on the original ZFS)
is under the CDDL license that is incompatible with the GPL license
and therefore cannot be included in our kernels. However, Btrfs
provides an excellent alternative to OpenZFS with a similar design
philosophy and is fully supported by SUSE.
  </p></section><section class="sect1" id="sec-filesystems-tmpfs" data-id-title="tmpfs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.9 </span><span class="title-name">tmpfs</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-tmpfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    tmpfs is a RAM-based virtual memory file system. The file system is temporary, which means no files are
    stored on the hard disk, and when the file system is unmounted, all data is discarded.```
  </p><p>
    Data in this file system is stored in the kernel internal cache. The needed kernel cache
    space can grow or shrink. 
  </p><p>
The file system has the following characteristics:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
  Very fast access to files.
</p></li><li class="listitem"><p>
        When swap is enabled for the tmpfs mount, unused data is swapped.
      </p></li><li class="listitem"><p>
        You can change the file system size during the <code class="command">mount -o remount</code>
        operation without losing data. However, you cannot resize to the value lower than its
        current usage.
        </p></li><li class="listitem"><p>
  tmpfs supports Transparent HugePage Support (THP).
</p></li></ul></div><p>
    For more information, you can refer to:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        the <a class="link" href="https://www.kernel.org/doc/html/latest/filesystems/tmpfs.html" target="_blank">kernel
        documentation</a>
      </p></li><li class="listitem"><p>
        <code class="command">man tmpfs</code>
      </p></li></ul></div></section><section class="sect1" id="sec-filesystems-other" data-id-title="Other supported file systems"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.10 </span><span class="title-name">Other supported file systems</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-other">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p class="intro">
   <a class="xref" href="#tab-filesystems-other" title="File system types in Linux">Table 1.1, “File system types in Linux”</a> summarizes
   some other file systems supported by Linux. They are supported mainly to
   ensure compatibility and interchange of data with different kinds of media
   or foreign operating systems.
  </p><div class="table" id="tab-filesystems-other" data-id-title="File system types in Linux"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 1.1: </span><span class="title-name">File system types in Linux </span></span><a title="Permalink" class="permalink" href="#tab-filesystems-other">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        File System Type
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">iso9660</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Standard file system on CD-ROMs.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">msdos</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="filename">fat</code>, the file system originally used by DOS, is
        today used by various operating systems.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">nfs</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Network File System: Here, data can be stored on any machine in a
        network and access might be granted via a network.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">ntfs</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Windows NT file system supported only with read-only access. The FUSE client
        <code class="literal">ntfs-3g</code> provides the access.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">OverlayFS</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        File system combines multiple different underlying mount points into one. It is used
        mainly on transactional-update systems and with containers.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">exfat</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        File system optimized for use with flash memory, such as USB flash
        drives and SD cards.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">Squashfs</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A compressed read-only file system. The file system compresses files, inodes and directories, and supports block sizes from 4 KiB up to 1 MiB for greater compression. 
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">smbfs</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Server Message Block is used by products such as Windows to enable file
        access over a network. Includes support for  <code class="literal">cifs</code>— a network file system client for mounting SMB/CIFS shares.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
  <p>
      <code class="systemitem">ufs</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">    
       <p>
        Used by BSD, SunOS and NextStep. Only supported in read-only mode.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        <code class="systemitem">vfat</code>
       </p>
      </td><td>
       <p>
        Virtual FAT: Extension of the <code class="literal">fat</code> file system
        (supports long file names).
       </p>
      </td></tr></tbody></table></div></div></section><section class="sect1" id="sec-blacklist-filsystem" data-id-title="Blocked file systems"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.11 </span><span class="title-name">Blocked file systems</span></span> <a title="Permalink" class="permalink" href="#sec-blacklist-filsystem">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Due to security reasons, some file systems have been blocked from automatic
   mounting. These file systems are usually not maintained anymore and are not
   in common use. However, the kernel module for this file system can be
   loaded, because the in-kernel API is still compatible. A combination of
   user-mountable file systems and the automatic mounting of file systems on
   removable devices could result in the situation where unprivileged users
   might trigger the automatic loading of kernel modules, and the removable
   devices could store potentially malicious data.
  </p><p>
   To get a list of file systems that are not allowed to be mounted
   automatically, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> rpm -ql suse-module-tools  | sed -nE 's/.*blacklist_fs-(.*)\.conf/\1/p'</pre></div><p>
   If you try to mount a device with a blocked file system using the
   <code class="command">mount</code>command, the command outputs an error message, for
   example:
  </p><div class="verbatim-wrap"><pre class="screen">mount: /mnt/mx: unknown filesystem type 'minix' (hint: possibly blacklisted, see mount(8)).</pre></div><p>
   Even though a file system cannot be mounted automatically, you can load the
   corresponding kernel module for the file system directly using
   <code class="command">modprobe</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> modprobe <em class="replaceable">FILESYSTEM</em></pre></div><p>
   For example, for the <code class="systemitem">cramfs</code> file system, the output
   looks as follows:
  </p><div class="verbatim-wrap"><pre class="screen">unblacklist: loading cramfs file system module
unblacklist: Do you want to un-blacklist cramfs permanently (&lt;y&gt;es/&lt;n&gt;o/n&lt;e&gt;ver)? y
unblacklist: cramfs un-blacklisted by creating /etc/modprobe.d/60-blacklist_fs-cramfs.conf</pre></div><p>
  Here you have the following options:
</p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.2.18.12.1"><span class="term"><span class="keycap">y</span>es</span></dt><dd><p>
        The module will be loaded and removed from the blacklist. Therefore, the module will be
        loaded automatically in future without any other prompt.
      </p><p>
        The configuration file <code class="filename">/etc/modprobe.d/60-blacklist_fs-${MODULE}.conf</code> is
        created. Remove the file to undo the changes you performed.
      </p></dd><dt id="id-1.11.3.2.18.12.2"><span class="term"><span class="keycap">n</span>o</span></dt><dd><p>
      The module will be loaded, but it is not removed from the blacklist. Therefore, on a next
      module loading
      you will see the prompt above again.
    </p></dd><dt id="id-1.11.3.2.18.12.3"><span class="term"><span class="keycap">e</span>ver</span></dt><dd><p>
      The module will be loaded, but autoloading is disabled even for future use, and the prompt 
      will no longer be displayed.
    </p><p>
      The configuration file <code class="filename">/etc/modprobe.d/60-blacklist_fs-${MODULE}.conf</code> is
      created. Remove the file to undo the changes you performed.
    </p></dd><dt id="id-1.11.3.2.18.12.4"><span class="term"><span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">c</span></span></dt><dd><p>
      This option is used to interrupt the module loading.
    </p></dd></dl></div></section><section class="sect1" id="sec-filesystems-lfs" data-id-title="Large file support in Linux"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.12 </span><span class="title-name">Large file support in Linux</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-lfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Originally, Linux supported a maximum file size of 2 GiB
   (2<sup>31</sup> bytes). Unless a file system comes with
   large file support, the maximum file size on a 32-bit system is 2 GiB.
  </p><p>
   Currently, all our standard file systems have LFS (large file support),
   which gives a maximum file size of 2<sup>63</sup> bytes in
   theory. <a class="xref" href="#tab-filesystems-maxsize" title="Maximum sizes of files and file systems (on-disk format, 4 KiB block size)">Table 1.2, “Maximum sizes of files and file systems (on-disk format, 4 KiB block size)”</a>
   offers an overview of the current on-disk format limitations of Linux files
   and file systems. The numbers in the table assume that the file systems are
   using 4 KiB block size, which is a common standard. When using different
   block sizes, the results are different. The maximum file sizes in
   <a class="xref" href="#tab-filesystems-maxsize" title="Maximum sizes of files and file systems (on-disk format, 4 KiB block size)">Table 1.2, “Maximum sizes of files and file systems (on-disk format, 4 KiB block size)”</a> can be
   larger than the file system's actual size when using sparse blocks.
  </p><div id="id-1.11.3.2.19.4" data-id-title="Binary multiples" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Binary multiples</div><p>
    In this document: 1024 Bytes = 1 KiB; 1024 KiB = 1 MiB; 1024 MiB = 1 GiB;
    1024 GiB = 1 TiB; 1024 TiB = 1 PiB; 1024 PiB = 1 EiB (see also
    <a class="link" href="https://physics.nist.gov/cuu/Units/binary.html" target="_blank"><em class="citetitle">NIST:
    Prefixes for Binary Multiples</em></a>.
   </p></div><div class="table" id="tab-filesystems-maxsize" data-id-title="Maximum sizes of files and file systems (on-disk format, 4 KiB block size)"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 1.2: </span><span class="title-name">Maximum sizes of files and file systems (on-disk format, 4 KiB block size) </span></span><a title="Permalink" class="permalink" href="#tab-filesystems-maxsize">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        File System (4 KiB Block Size)
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Maximum File System Size
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Maximum File Size
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Btrfs
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        16 EiB
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        16 EiB
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Ext3
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        16 TiB
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        2 TiB
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Ext4
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        1 EiB
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        16 TiB
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        OCFS2 (a cluster-aware file system available in SLE HA)
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        16 TiB
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        1 EiB
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        XFS
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        16 EiB
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        8 EiB
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        NFSv2 (client side)
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        8 EiB
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        2 GiB
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        NFSv3/NFSv4 (client side)
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        8 EiB
       </p>
      </td><td>
       <p>
        8 EiB
       </p>
      </td></tr></tbody></table></div></div><div id="id-1.11.3.2.19.6" data-id-title="Limitations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Limitations</div><p>
    <a class="xref" href="#tab-filesystems-maxsize" title="Maximum sizes of files and file systems (on-disk format, 4 KiB block size)">Table 1.2, “Maximum sizes of files and file systems (on-disk format, 4 KiB block size)”</a> describes
    the limitations regarding the on-disk format. The Linux kernel imposes its
    own limits on the size of files and file systems handled by it. These are
    as follows:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.2.19.6.3.1"><span class="term">File size</span></dt><dd><p>
       On 32-bit systems, files cannot exceed 2 TiB
       (2<sup>41</sup> bytes).
      </p></dd><dt id="id-1.11.3.2.19.6.3.2"><span class="term">File system size</span></dt><dd><p>
       File systems can be up to 2<sup>73</sup> bytes in size.
       However, this limit is still out of reach for the currently available
       hardware.
      </p></dd></dl></div></div></section><section class="sect1" id="sec-filesystems-stor-limits" data-id-title="Linux kernel storage limitations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.13 </span><span class="title-name">Linux kernel storage limitations</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-stor-limits">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <a class="xref" href="#tab-filesystems-stor-limits" title="Storage limitations">Table 1.3, “Storage limitations”</a>
   summarizes the kernel limits for storage associated with <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>.
  </p><div class="table" id="tab-filesystems-stor-limits" data-id-title="Storage limitations"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 1.3: </span><span class="title-name">Storage limitations </span></span><a title="Permalink" class="permalink" href="#tab-filesystems-stor-limits">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Storage Feature
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Limitation
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Maximum number of LUNs supported
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        16384 LUNs per target.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Maximum number of paths per single LUN
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        No limit by default. Each path is treated as a normal LUN.
       </p>
       <p>
        The actual limit is given by the number of LUNs per target and the
        number of targets per HBA (16777215 for a Fibre Channel HBA).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Maximum number of HBAs
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Unlimited. The actual limit is determined by the amount of PCI slots of
        the system.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Maximum number of paths with device-mapper-multipath (in total) per
        operating system
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Approximately 1024. The actual number depends on the length of the
        device number strings for each multipath device. It is a compile-time
        variable within multipath-tools, which can be raised if this limit
        poses a problem.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Maximum size per block device
       </p>
      </td><td>
       <p>
        Up to 8 EiB.
       </p>
      </td></tr></tbody></table></div></div></section><section class="sect1" id="sec-filesystems-trouble-trim" data-id-title="Freeing unused file system blocks"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.14 </span><span class="title-name">Freeing unused file system blocks</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-trouble-trim">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   On solid-state drives (SSDs) and thinly provisioned volumes, it is useful to
   trim blocks that are not in use by the file system. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> fully
   supports <code class="literal">unmap</code> and <code class="literal">TRIM</code> operations on
   all file systems supporting them.
  </p><p>
   There are two types of commonly used TRIM—online
   <code class="literal">TRIM</code> and periodic <code class="literal">TRIM</code>. The most
   suitable way of trimming devices depends on your use case. In general, it is
   recommended to use periodic TRIM, especially if the device has enough free
   blocks. If the device is often near its full capacity, online TRIM is
   preferable.
  </p><div id="id-1.11.3.2.21.4" data-id-title="TRIM support on devices" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="literal">TRIM</code> support on devices</div><p>
    Always verify that your device supports the <code class="literal">TRIM</code>
    operation before you attempt to use it. Otherwise, you might lose your data
    on that device. To verify the <code class="literal">TRIM</code> support, run the
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lsblk --discard</pre></div><p>
    The command outputs information about all available block devices. If the
    values of the columns <code class="literal">DISC-GRAN</code> and
    <code class="literal">DISC-MAX</code> are non-zero, the device supports the
    <code class="literal">TRIM</code> operation.
   </p></div><section class="sect2" id="sec-filesystems-periodic-trim" data-id-title="Periodic TRIM"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.14.1 </span><span class="title-name">Periodic TRIM</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-periodic-trim">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Periodic TRIM is handled by the <code class="command">fstrim</code> command invoked
    by <code class="systemitem">systemd</code> on a regular basis. You can also run the command manually.
   </p><p>
    To schedule periodic TRIM, enable the <code class="literal">fstrim.timer</code> as
    follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl enable fstrim.timer</pre></div><p>
    <code class="systemitem">systemd</code> creates a unit file in
    <code class="filename">/usr/lib/systemd/system</code>. By default, the service runs
    once a week, which is usually sufficient. However, you can change the
    frequency by configuring the <code class="literal">OnCalendar</code> option to a
    required value.
   </p><p>
    The default behaviour of <code class="command">fstrim</code> is to discard all blocks
    in the file system. You can use options when invoking the command to modify
    this behaviour. For example, you can pass the <code class="literal">offset</code>
    option to define the place where to start the trimming procedure. For
    details, see <code class="command">man fstrim</code>.
   </p><p>
    The <code class="command">fstrim</code> command can perform trimming on all devices
    stored in the <code class="filename">/etc/fstab</code> file, which support the
    <code class="literal">TRIM</code> operation—use the <code class="literal">-A</code>
    option when invoking the command for this purpose.
   </p><p>
    To disable the trimming of a particular device, add the option
    <code class="literal">X-fstrim.notrim</code> to the <code class="filename">/etc/fstab</code>
    file as follows:
   </p><div class="verbatim-wrap"><pre class="screen">UID=83df497d-bd6d-48a3-9275-37c0e3c8dc74  /  btrfs  defaults,X-fstrim.notrim                      0  0</pre></div></section><section class="sect2" id="sec-filesystem-online-trim" data-id-title="Online TRIM"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.14.2 </span><span class="title-name">Online TRIM</span></span> <a title="Permalink" class="permalink" href="#sec-filesystem-online-trim">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Online TRIM of a device is performed each time data is written to the
    device.
   </p><p>
    To enable online TRIM on a device, add the <code class="literal">discard</code>
    option to the <code class="filename">/etc/fstab</code> file as follows:
   </p><div class="verbatim-wrap"><pre class="screen">UID=83df497d-bd6d-48a3-9275-37c0e3c8dc74  /  btrfs  defaults,discard</pre></div><p>
    Alternatively, on the Ext4 file system you can use the
    <code class="command">tune2fs</code> command to set the <code class="literal">discard</code>
    option in <code class="filename">/etc/fstab</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> tune2fs -o discard <em class="replaceable">DEVICE</em></pre></div><p>
    The <code class="literal">discard</code> option is also added to
    <code class="filename">/etc/fstab</code> in case the device was mounted by
    <code class="command">mount</code> with the <code class="literal">discard</code> option:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount -o discard <em class="replaceable">DEVICE</em></pre></div><div id="id-1.11.3.2.21.6.9" data-id-title="Drawbacks of online TRIM" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Drawbacks of online TRIM</div><p>
     Using the <code class="literal">discard</code> option may decrease the lifetime of some
     lower-quality SSD devices. Online TRIM can also impact the performance of
     the device, for example, if a larger amount of data is deleted. In this
     situation, an erase block might be reallocated, and shortly afterwards,
     the same erase block might be marked as unused again.
    </p></div></section></section><section class="sect1" id="sec-filesystems-trouble" data-id-title="Troubleshooting file systems"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.15 </span><span class="title-name">Troubleshooting file systems</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-trouble">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section describes some known issues and possible solutions for file
   systems.
  </p><section class="sect2" id="sec-filesystems-trouble-btrfs-volfull" data-id-title="Btrfs error: no space is left on device"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.15.1 </span><span class="title-name">Btrfs error: no space is left on device</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-trouble-btrfs-volfull">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The root (<code class="filename">/</code>) partition using the Btrfs file system
    stops accepting data. You receive the error <span class="quote">“<span class="quote"><code class="literal">No space left
    on device</code></span>”</span>.
   </p><p>
    See the following sections for information about possible causes and
    prevention of this issue.
   </p><section class="sect3" id="sec-filesystems-trouble-btrfs-volfull-snapshots" data-id-title="Disk space consumed by Snapper snapshots"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.15.1.1 </span><span class="title-name">Disk space consumed by Snapper snapshots</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-trouble-btrfs-volfull-snapshots">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     If Snapper is running for the Btrfs file system, the <span class="quote">“<span class="quote"><code class="literal">No
     space left on device</code></span>”</span> problem is typically caused by
     having too much data stored as snapshots on your system.
    </p><p>
     You can remove some snapshots from Snapper, however, the snapshots are not
     deleted immediately and might not free up as much space as you need.
    </p><p>
     To delete files from Snapper:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Open a terminal.
      </p></li><li class="step"><p>
       At the command prompt, enter <code class="command">btrfs filesystem show</code>,
       for example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem show
Label: none uuid: 40123456-cb2c-4678-8b3d-d014d1c78c78
 Total devices 1 FS bytes used 20.00GB
 devid 1 size 20.00GB used 20.00GB path /dev/sda3</pre></div></li><li class="step"><p>
       Enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs fi balance start <em class="replaceable">MOUNTPOINT</em> -dusage=5</pre></div><p>
       This command attempts to relocate data in empty or near-empty data
       chunks, allowing the space to be reclaimed and reassigned to metadata.
       This can take a while (many hours for 1 TB) although the system is
       otherwise usable during this time.
      </p></li><li class="step"><p>
       List the snapshots in Snapper. Enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> snapper -c root list</pre></div></li><li class="step"><p>
       Delete one or more snapshots from Snapper. Enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> snapper -c root delete <em class="replaceable">SNAPSHOT_NUMBER(S)</em></pre></div><p>
       Ensure that you delete the oldest snapshots first. The older a snapshot
       is, the more disk space it occupies.
      </p></li></ol></div></div><p>
     To help prevent this problem, you can change the Snapper cleanup
     algorithms. See <span class="intraxref">Book “Administration Guide”, Chapter 10 “System recovery and snapshot management with Snapper”, Section 10.6.1.2 “Cleanup algorithms”</span> for
     details. The configuration values controlling snapshot cleanup are
     <code class="envar">EMPTY_*</code>, <code class="envar">NUMBER_*</code>, and
     <code class="envar">TIMELINE_*</code>.
    </p><p>
     If you use Snapper with Btrfs on the file system disk, it is advisable to
     reserve twice the amount of disk space than the standard storage proposal.
     The YaST Partitioner automatically proposes twice the standard disk
     space in the Btrfs storage proposal for the root file system.
    </p></section><section class="sect3" id="sec-filesystems-trouble-btrfs-volfull-var" data-id-title="Disk space consumed by log, crash, and cache files"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.15.1.2 </span><span class="title-name">Disk space consumed by log, crash, and cache files</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-trouble-btrfs-volfull-var">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     If the system disk is filling up with data, you can try deleting files
     from <code class="filename">/var/log</code>, <code class="filename">/var/crash</code>,
     <code class="filename">/var/lib/systemd/coredump</code> and
     <code class="filename">/var/cache</code>.
    </p><p>
     The Btrfs <code class="systemitem">root</code> file system subvolumes <code class="filename">/var/log</code>,
     <code class="filename">/var/crash</code> and <code class="filename">/var/cache</code> can
     use all of the available disk space during normal operation, and cause a
     system malfunction. To help avoid this situation, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers
     Btrfs quota support for subvolumes. See
     <a class="xref" href="#sec-filesystems-major-btrfs-quota" title="1.2.5. Btrfs quota support for subvolumes">Section 1.2.5, “Btrfs quota support for subvolumes”</a> for details.
    </p><p>
     On test and development machines, especially if you have frequent crashes
     of applications, you may also want to have a look at
     <code class="filename">/var/lib/systemd/coredump</code> where the core dumps are
     stored.
    </p></section></section><section class="sect2" id="sec-filesystems-trouble-balance" data-id-title="Btrfs: balancing data across devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.15.2 </span><span class="title-name">Btrfs: balancing data across devices</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-trouble-balance">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="command">btrfs balance</code> command is part of the
    <span class="package">btrfs-progs</span> package. It balances block groups on Btrfs
    file systems in the following example situations:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Assume you have a 1 TB drive with 600 GB used by data and you
      add another 1 TB drive. The balancing will theoretically result in
      having 300 GB of used space on each drive.
     </p></li><li class="listitem"><p>
      You have a lot of near-empty chunks on a device. Their space will not be
      available until the balancing has cleared those chunks.
     </p></li><li class="listitem"><p>
      You need to compact half-empty block group based on the percentage of
      their usage. The following command will balance block groups whose usage
      is 5% or less:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs balance start -dusage=5 /</pre></div><div id="id-1.11.3.2.22.4.3.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
       The <code class="filename">/usr/lib/systemd/system/btrfs-balance.timer</code>
       timer takes care of cleaning up unused block groups on a monthly basis.
      </p></div></li><li class="listitem"><p>
      You need to clear out non-full portions of block devices and spread data
      more evenly.
     </p></li><li class="listitem"><p>
      You need to migrate data between different RAID types. For example, to
      convert data on a set of disks from RAID1 to RAID5, run the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs balance start -dprofiles=raid1,convert=raid5 /</pre></div></li></ul></div><div id="id-1.11.3.2.22.4.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
     To fine-tune the default behavior of balancing data on Btrfs file
     systems—for example, how frequently or which mount points to
     balance— inspect and customize
     <code class="filename">/etc/sysconfig/btrfsmaintenance</code>. The relevant options
     start with <code class="option">BTRFS_BALANCE_</code>.
    </p></div><p>
    For details about the <code class="command">btrfs balance</code> command usage, see
    its manual pages (<code class="command">man 8 btrfs-balance</code>).
   </p></section><section class="sect2" id="sec-filesystems-trouble-defrag" data-id-title="No defragmentation on SSDs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.15.3 </span><span class="title-name">No defragmentation on SSDs</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-trouble-defrag">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Linux file systems contain mechanisms to avoid data fragmentation and
    usually it is not necessary to defragment. However, there are use cases,
    where data fragmentation cannot be avoided and where defragmenting the hard
    disk significantly improves the performance.
   </p><p>
    This only applies to conventional hard disks. On solid state disks (SSDs)
    which use flash memory to store data, the firmware provides an algorithm
    that determines to which chips the data is written. Data is usually spread
    all over the device. Therefore defragmenting an SSD does not have the
    desired effect and will reduce the lifespan of an SSD by writing
    unnecessary data.
   </p><p>
    For the reasons mentioned above, SUSE explicitly recommends
    <span class="emphasis"><em>not</em></span> to defragment SSDs. Some vendors also warn about
    defragmenting solid state disks. This includes, but it is not limited to
    the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      HPE 3PAR StoreServ All-Flash
     </p></li><li class="listitem"><p>
      HPE 3PAR StoreServ Converged Flash
     </p></li></ul></div></section></section><section class="sect1" id="sec-filesystems-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.16 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-filesystems-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_filesystems.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Each of the file system projects described above maintains its own home page
   on which to find mailing list information, further documentation, and FAQs:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The Btrfs Wiki on Kernel.org:
     <a class="link" href="https://btrfs.wiki.kernel.org/" target="_blank">https://btrfs.wiki.kernel.org/</a>
    </p></li><li class="listitem"><p>
     E2fsprogs: Ext2/3/4 File System Utilities:
     <a class="link" href="https://e2fsprogs.sourceforge.net/" target="_blank">https://e2fsprogs.sourceforge.net/</a>
    </p></li><li class="listitem"><p>
     The OCFS2 Project:
     <a class="link" href="https://oss.oracle.com/projects/ocfs2/" target="_blank">https://oss.oracle.com/projects/ocfs2/</a>
    </p></li></ul></div><p>
   An in-depth comparison of file systems (not only Linux file systems) is
   available from the Wikipedia project in Comparison of File Systems
   (<a class="link" href="https://en.wikipedia.org/wiki/Comparison_of_file_systems#Comparison" target="_blank">https://en.wikipedia.org/wiki/Comparison_of_file_systems#Comparison</a>).
  </p></section></section><section xml:lang="en" class="chapter" id="cha-resize-fs" data-id-title="Resizing file systems"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Resizing file systems</span></span> <a title="Permalink" class="permalink" href="#cha-resize-fs">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Resizing file systems—not to be confused with resizing partitions or
  volumes—can be used to make space available on physical volumes or to
  use additional space available on a physical volume.
 </p><section class="sect1" id="sec-resize-fs-usecases" data-id-title="Use cases"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Use cases</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-usecases">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   It is strongly recommended to use the YaST Partitioner to resize
   partitions or logical volumes. When doing so, the file system will
   automatically be adjusted to the new size of the partition or volume.
   However, there are some cases where you need to resize the file system
   manually, because they are not supported by YaST:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     After having resized a virtual disk of a VM Guest.
    </p></li><li class="listitem"><p>
     After having resized a volume from a network-attached storage.
    </p></li><li class="listitem"><p>
     After having manually resized partitions (for example by using
     <code class="command">fdisk</code> or <code class="command">parted</code>) or logical volumes
     (for example by using <code class="command">lvresize</code>).
    </p></li><li class="listitem"><p>
     When wanting to shrink Btrfs file systems (as of <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12, YaST
     only supports growing Btrfs file systems).
    </p></li></ul></div></section><section class="sect1" id="sec-resize-fs-guidelines" data-id-title="Guidelines for resizing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Guidelines for resizing</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-guidelines">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Resizing any file system involves some risks that can potentially result in
   losing data.
  </p><div id="id-1.11.3.3.5.3" data-id-title="Back up your data" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Back up your data</div><p>
    To avoid data loss, ensure that you back up your data before you begin any
    resizing task.
   </p></div><p>
   Consider the following guidelines when planning to resize a file system.
  </p><section class="sect2" id="sec-resize-fs-guidelines-support" data-id-title="File systems that support resizing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2.1 </span><span class="title-name">File systems that support resizing</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-guidelines-support">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The file system must support resizing to take advantage of increases in
    available space for the volume. In <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, file system resizing
    utilities are available for file systems Ext2, Ext3, and Ext4.
    The utilities support increasing and decreasing the size as follows:
   </p><div class="table" id="id-1.11.3.3.5.5.3" data-id-title="File system support for resizing"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.1: </span><span class="title-name">File system support for resizing </span></span><a title="Permalink" class="permalink" href="#id-1.11.3.3.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         File System
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Utility
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Increase Size (Grow)
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Decrease Size (Shrink)
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Btrfs
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="command">btrfs filesystem resize</code>
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Online
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Online
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         XFS
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="command">xfs_growfs</code>
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Online
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Not supported
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Ext2
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="command">resize2fs</code>
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Online or offline
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Offline only
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Ext3
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="command">resize2fs</code>
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Online or offline
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Offline only
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Ext4
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="command">resize2fs</code>
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         Online or offline
        </p>
       </td><td>
        <p>
         Offline only
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-resize-fs-guidelines-increase" data-id-title="Increasing the size of a file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2.2 </span><span class="title-name">Increasing the size of a file system</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-guidelines-increase">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can grow a file system to the maximum space available on the device, or
    specify an exact size. Ensure that you grow the size of the device or
    logical volume before you attempt to increase the size of the file system.
   </p><p>
    When specifying an exact size for the file system, ensure that the new size
    satisfies the following conditions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The new size must be greater than the size of the existing data;
      otherwise, data loss occurs.
     </p></li><li class="listitem"><p>
      The new size must be equal to or less than the current device size
      because the file system size cannot extend beyond the space available.
     </p></li></ul></div></section><section class="sect2" id="sec-resize-fs-guidelines-decrease" data-id-title="Decreasing the size of a file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2.3 </span><span class="title-name">Decreasing the size of a file system</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-guidelines-decrease">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When decreasing the size of the file system on a device, ensure that the
    new size satisfies the following conditions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The new size must be greater than the size of the existing data;
      otherwise, data loss occurs.
     </p></li><li class="listitem"><p>
      The new size must be equal to or less than the current device size
      because the file system size cannot extend beyond the space available.
     </p></li></ul></div><p>
    If you plan to also decrease the size of the logical volume that holds the
    file system, ensure that you decrease the size of the file system before
    you attempt to decrease the size of the device or logical volume.
   </p><div id="id-1.11.3.3.5.7.5" data-id-title="XFS" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: XFS</div><p>
     Decreasing the size of a file system formatted with XFS is not possible,
     since such a feature is not supported by XFS.
    </p></div></section></section><section class="sect1" id="sec-resize-fs-btrfs" data-id-title="Changing the size of a Btrfs file system"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.3 </span><span class="title-name">Changing the size of a Btrfs file system</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-btrfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The size of a Btrfs file system can be changed by using the <code class="command">btrfs
   filesystem resize</code> command when the file system is mounted.
   Increasing and decreasing the size are both supported while the file system
   is mounted.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal.
    </p></li><li class="step"><p>
     Make sure the file system you want to change is mounted.
    </p></li><li class="step"><p>
     Change the size of the file system using the <code class="command">btrfs filesystem
     resize</code> command with one of the following methods:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       To extend the file system size to the maximum available size of the
       device, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem resize max /mnt</pre></div></li><li class="listitem"><p>
       To extend the file system to a specific size, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem resize <em class="replaceable">SIZE</em> /mnt</pre></div><p>
       Replace <em class="replaceable">SIZE</em> with the desired size in bytes.
       You can also specify units on the value, such as 50000K (kilobytes),
       250M (megabytes), or 2G (gigabytes). Alternatively, you can specify an
       increase or decrease to the current size by prefixing the value with a
       plus (<code class="literal">+</code>) or a minus (<code class="literal">-</code>) sign,
       respectively:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem resize +<em class="replaceable">SIZE</em> /mnt
sudo btrfs filesystem resize -<em class="replaceable">SIZE</em> /mnt</pre></div></li></ul></div></li><li class="step"><p>
     Check the effect of the resize on the mounted file system by entering
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>df -h</pre></div><p>
     The Disk Free (<code class="command">df</code>) command shows the total size of the
     disk, the number of blocks used, and the number of blocks available on the
     file system. The -h option prints sizes in human-readable format, such as
     1K, 234M, or 2G.
    </p></li></ol></div></div></section><section class="sect1" id="sec-resize-fs-xfs" data-id-title="Changing the size of an XFS file system"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.4 </span><span class="title-name">Changing the size of an XFS file system</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-xfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The size of an XFS file system can be increased by using the
   <code class="command">xfs_growfs</code> command when the file system is mounted.
   Reducing the size of an XFS file system is not possible.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal.
    </p></li><li class="step"><p>
     Make sure the file system you want to change is mounted.
    </p></li><li class="step"><p>
     Increase the size of the file system using the
     <code class="command">xfs_growfs</code> command. The following example expands the
     size of the file system to the maximum value available. See <code class="command">man 8
     xfs_growfs</code> for more options.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> xfs_growfs -d /mnt</pre></div></li><li class="step"><p>
     Check the effect of the resize on the mounted file system by entering
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>df -h</pre></div><p>
     The Disk Free (<code class="command">df</code>) command shows the total size of the
     disk, the number of blocks used, and the number of blocks available on the
     file system. The -h option prints sizes in human-readable format, such as
     1K, 234M, or 2G.
    </p></li></ol></div></div></section><section class="sect1" id="sec-resize-fs-ext" data-id-title="Changing the size of an ext2, ext3, or ext4 file system"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.5 </span><span class="title-name">Changing the size of an ext2, ext3, or ext4 file system</span></span> <a title="Permalink" class="permalink" href="#sec-resize-fs-ext">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fs-resizing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The size of Ext2, Ext3, and Ext4 file systems can be increased by using the
   <code class="command">resize2fs</code> command, regardless of whether the respective
   partition is mounted or not. To decrease the size of an Ext file system it
   needs to be unmounted.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal.
    </p></li><li class="step"><p>
     If the file system size should be decreased, unmount it.
    </p></li><li class="step"><p>
     Change the size of the file system using one of the following methods:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       To extend the file system size to the maximum available size of the
       device called <code class="filename">/dev/sda1</code>, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> resize2fs /dev/sda1</pre></div><p>
       If a size parameter is not specified, the size defaults to the size of
       the partition.
      </p></li><li class="listitem"><p>
       To change the file system to a specific size, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> resize2fs /dev/sda1 <em class="replaceable">SIZE</em></pre></div><p>
       The <em class="replaceable">SIZE</em> parameter specifies the requested
       new size of the file system. If no units are specified, the unit of the
       size parameter is the block size of the file system. Optionally, the
       size parameter can be suffixed by one of the following unit designations:
       <code class="literal">s</code> for 512 byte sectors; <code class="literal">K</code> for
       kilobytes (1 kilobyte is 1024 bytes); <code class="literal">M</code> for
       megabytes; or <code class="literal">G</code> for gigabytes.
      </p></li></ul></div><p>
     Wait until the resizing is completed before continuing.
    </p></li><li class="step"><p>
     If the file system is not mounted, mount it now.
    </p></li><li class="step"><p>
     Check the effect of the resize on the mounted file system by entering
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>df -h</pre></div><p>
     The Disk Free (<code class="command">df</code>) command shows the total size of the
     disk, the number of blocks used, and the number of blocks available on the
     file system. The -h option prints sizes in human-readable format, such as
     1K, 234M, or 2G.
    </p></li></ol></div></div></section></section><section xml:lang="en" class="chapter" id="cha-uuid" data-id-title="Mounting storage devices"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Mounting storage devices</span></span> <a title="Permalink" class="permalink" href="#cha-uuid">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_uuids.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section gives an overview of which device identificators are used during
  mounting of devices, and provides details about mounting network storages.
 </p><section class="sect1" id="sec-uuid-understanduuid" data-id-title="Understanding UUIDs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Understanding UUIDs</span></span> <a title="Permalink" class="permalink" href="#sec-uuid-understanduuid">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_uuids.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A UUID (Universally Unique Identifier) is a 128-bit number for a file system
   that is unique on both the local system and across other systems. It is
   randomly generated with system hardware information and time stamps as part
   of its seed. UUIDs are commonly used to uniquely tag devices.
  </p><p>
   Using non-persistent <span class="quote">“<span class="quote">traditional</span>”</span> device names such as
   <code class="filename">/dev/sda1</code> may render the system unbootable when adding
   storage. For example, if root (<code class="filename">/</code>) is assigned to
   <code class="filename">/dev/sda1</code>, it might be reassigned to
   <code class="filename">/dev/sdg1</code> after a SAN has been attached or additional
   hard disks have been applied to the system. In this case the boot loader
   configuration and the <code class="filename">/etc/fstab</code> file need to be
   adjusted, otherwise the system will no longer boot.
  </p><p>
   By default, UUIDs are used in the boot loader and
   <code class="filename">/etc/fstab</code> files for the boot device. The UUID is a
   property of the file system and can change if you reformat the drive. Other
   alternatives to using UUIDs of device names would be to identify devices by
   ID or label.
  </p><p>
   You can also use the UUID as criterion for assembling and activating
   software RAID devices. When a RAID is created, the <code class="command">md</code>
   driver generates a UUID for the device, and stores the value in the
   <code class="filename">md</code> superblock.
  </p><p>
   You can find the UUID for any block device in the
   <code class="filename">/dev/disk/by-uuid</code> directory. For example, a UUID entry
   looks like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>ls -og /dev/disk/by-uuid/
lrwxrwxrwx 1 10 Dec  5 07:48 e014e482-1c2d-4d09-84ec-61b3aefde77a -&gt; ../../sda1</pre></div></section><section class="sect1" id="sec-uuid-udev" data-id-title="Persistent device names with udev"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">Persistent device names with udev</span></span> <a title="Permalink" class="permalink" href="#sec-uuid-udev">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_uuids.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Starting with Linux kernel 2.6, <code class="command">udev</code> provides a user
   space solution for the dynamic <code class="filename">/dev</code> directory, with
   persistent device naming. As part of the hotplug system,
   <code class="command">udev</code> is executed if a device is added to or removed from
   the system.
  </p><p>
   A list of rules is used to match against specific device attributes. The
   <code class="command">udev</code> rules infrastructure (defined in the
   <code class="filename">/etc/udev/rules.d</code> directory) provides stable names for
   all disk devices, regardless of their order of recognition or the connection
   used for the device. The <code class="command">udev</code> tools examine every
   appropriate block device that the kernel creates to apply naming rules based
   on certain buses, drive types, or file systems. For information about how to
   define your own rules for <code class="command">udev</code>, see
   <a class="link" href="https://reactivated.net/writing_udev_rules.html" target="_blank"><em class="citetitle">Writing
   udev Rules</em></a>.
  </p><p>
   Along with the dynamic kernel-provided device node name,
   <code class="command">udev</code> maintains classes of persistent symbolic links
   pointing to the device in the <code class="filename">/dev/disk</code> directory,
   which is further categorized by the <code class="filename">by-id</code>,
   <code class="filename">by-label</code>, <code class="filename">by-path</code>, and
   <code class="filename">by-uuid</code> subdirectories.
  </p><div id="id-1.11.3.4.5.5" data-id-title="UUID generators" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: UUID generators</div><p>
    Other programs besides <code class="command">udev</code>, such as LVM or
    <code class="command">md</code>, might also generate UUIDs, but they are not listed
    in <code class="filename">/dev/disk</code>.
   </p></div><p>
   For more information about using <code class="systemitem">udev</code> for managing
   devices, see <span class="intraxref">Book “Administration Guide”, Chapter 29 “Dynamic kernel device management with <code class="systemitem">udev</code>”</span>.
  </p><p>
   For more information about <code class="systemitem">udev</code> commands, see
   <code class="command">man 7 udev</code>.
  </p></section><section class="sect1" id="sec-netdev-option" data-id-title="Mounting network storage devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.3 </span><span class="title-name">Mounting network storage devices</span></span> <a title="Permalink" class="permalink" href="#sec-netdev-option">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_uuids.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Some types of storage devices require the network to be configured and available
   before <code class="command">systemd.mount</code> starts to mount the devices. To
   postpone mounting of these types of devices, add the
   <code class="literal">_netdev</code> and <code class="literal">x-systemd.requires=iscsi.service</code> options to the <code class="filename">/etc/fstab</code>
   file for each particular network storage device. An example follows:
  </p><div class="verbatim-wrap"><pre class="screen">mars.example.org:/nfsexport  /shared   nfs  defaults,_netdev,x-systemd.requires=iscsi.service    0  0</pre></div><p>
Do not use the <code class="literal">nofail</code> option as booting of the machine continues without waiting
for the particular storage device to be successfully mounted.
  </p></section></section><section xml:lang="en" class="chapter" id="cha-multitiercache" data-id-title="Multi-tier caching for block device operations"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Multi-tier caching for block device operations</span></span> <a title="Permalink" class="permalink" href="#cha-multitiercache">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  A multi-tier cache is a replicated/distributed cache that consists of at
  least two tiers: one is represented by slower but cheaper rotational block
  devices (hard disks), while the other is more expensive but performs faster
  data operations (for example SSD flash disks).
 </p><p>
  SUSE Linux Enterprise Server implements two different solutions for caching between flash and
  rotational devices: <code class="systemitem">bcache</code> and <code class="systemitem">lvmcache</code>.
 </p><section class="sect1" id="sec-multitiercache-terminology" data-id-title="General terminology"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">General terminology</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-terminology">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section explains several terms often used when describing cache related
   features:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.5.5.3.1"><span class="term">Migration</span></dt><dd><p>
      Movement of the primary copy of a logical block from one device to the
      other.
     </p></dd><dt id="id-1.11.3.5.5.3.2"><span class="term">Promotion</span></dt><dd><p>
      Migration from the slow device to the fast device.
     </p></dd><dt id="id-1.11.3.5.5.3.3"><span class="term">Demotion</span></dt><dd><p>
      Migration from the fast device to the slow device.
     </p></dd><dt id="id-1.11.3.5.5.3.4"><span class="term">Origin device</span></dt><dd><p>
      The big and slower block device. It always contains a copy of the logical
      block, which may be out of date or kept in synchronization with the copy
      on the cache device (depending on policy).
     </p></dd><dt id="id-1.11.3.5.5.3.5"><span class="term">Cache device</span></dt><dd><p>
      The small and faster block device.
     </p></dd><dt id="id-1.11.3.5.5.3.6"><span class="term">Metadata device</span></dt><dd><p>
      A small device that records which blocks are in the cache, which are
      dirty, and extra hints for use by the policy object. This information
      could be put on the cache device as well, but having it separate allows
      the volume manager to configure it differently, for example as a mirror
      for extra robustness. The metadata device may only be used by a single
      cache device.
     </p></dd><dt id="id-1.11.3.5.5.3.7"><span class="term">Dirty block</span></dt><dd><p>
      If some process writes to a block of data which is placed in the cache,
      the cached block is marked as <span class="emphasis"><em>dirty</em></span> because it was
      overwritten in the cache and needs to be written back to the original
      device.
     </p></dd><dt id="id-1.11.3.5.5.3.8"><span class="term">Cache miss</span></dt><dd><p>
      A request for I/O operations is pointed to the cached device's cache
      first. If it cannot find the requested values, it looks in the device
      itself, which is slow. This is called a <span class="emphasis"><em>cache miss</em></span>.
     </p></dd><dt id="id-1.11.3.5.5.3.9"><span class="term">Cache hit</span></dt><dd><p>
      When a requested value is found in the cached device's cache, it is
      served fast. This is called a <span class="emphasis"><em>cache hit</em></span>.
     </p></dd><dt id="id-1.11.3.5.5.3.10"><span class="term">Cold cache</span></dt><dd><p>
      Cache that holds no values (is empty) and causes <span class="emphasis"><em>cache
      misses</em></span>. As the cached block device operations progress, it
      gets filled with data and becomes <span class="emphasis"><em>warm</em></span>.
     </p></dd><dt id="id-1.11.3.5.5.3.11"><span class="term">Warm cache</span></dt><dd><p>
      Cache that already holds some values and is likely to result in
      <span class="emphasis"><em>cache hits</em></span>.
     </p></dd></dl></div></section><section class="sect1" id="sec-multitiercache-caching-modes" data-id-title="Caching modes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Caching modes</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-caching-modes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Following are the basic caching modes that multi-tier caches use:
   <span class="emphasis"><em>write-back</em></span>, <span class="emphasis"><em>write-through</em></span>,
   <span class="emphasis"><em>write-around</em></span> and <span class="emphasis"><em>pass-through</em></span>.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.3.5.6.3.1"><span class="term">write-back</span></dt><dd><p>
      Data written to a block that is cached go to the cache only, and the
      block is marked dirty. This is the default caching mode.
     </p></dd><dt id="id-1.11.3.5.6.3.2"><span class="term">write-through</span></dt><dd><p>
      Writing to a cached block will not complete until it has hit both the
      origin and cache devices. Clean blocks remain clean with
      <span class="emphasis"><em>write-through</em></span> cache.
     </p></dd><dt id="id-1.11.3.5.6.3.3"><span class="term">write-around</span></dt><dd><p>
      A similar technique to write-through cache, but write I/O is written
      directly to a permanent storage, bypassing the cache. This can prevent
      the cache being flooded with write I/O that will not subsequently be
      re-read, but the disadvantage is that a read request for recently written
      data will create a 'cache miss' and needs to be read from slower bulk
      storage and experience higher latency.
     </p></dd><dt id="id-1.11.3.5.6.3.4"><span class="term">pass-through</span></dt><dd><p>
      To enable the <span class="emphasis"><em>pass-through</em></span> mode, the cache needs to
      be clean. Reading is served from the origin device bypassing the cache.
      Writing is forwarded to the origin device and 'invalidates' the cache
      block. <span class="emphasis"><em>Pass-through</em></span> allows a cache device activation
      without having to care about data coherency, which is maintained. The
      cache will gradually become cold as writing takes place. If you can
      verify the coherency of the cache later, or establish it by using the
      <code class="literal">invalidate_cblocks</code> message, you can switch the cache
      device to <span class="emphasis"><em>write-through</em></span> or
      <span class="emphasis"><em>write-back</em></span> mode while it is still warm. Otherwise,
      you can discard the cache contents before switching to the desired
      caching mode.
     </p></dd></dl></div></section><section class="sect1" id="sec-multitiercache-bcache" data-id-title="bcache"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name"><code class="systemitem">bcache</code></span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-bcache">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="systemitem">bcache</code> is a Linux kernel block layer cache. It allows one or more fast
   disk drives (such as SSDs) to act as a cache for one or more slower hard
   disks. <code class="systemitem">bcache</code> supports write-through and write-back, and is independent of
   the file system used. By default it caches random reads and writes only,
   which SSDs excel at. It is suitable for desktops, servers, and high end
   storage arrays as well.
  </p><section class="sect2" id="sec-multitiercache-bcache-features" data-id-title="Main features"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.1 </span><span class="title-name">Main features</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-bcache-features">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      A single cache device can be used to cache an arbitrary number of backing
      devices. Backing devices can be attached and detached at runtime, while
      mounted and in use.
     </p></li><li class="listitem"><p>
      Recovers from unclean shutdowns—writes are not completed until the
      cache is consistent with regard to the backing device.
     </p></li><li class="listitem"><p>
      Throttles traffic to the SSD if it becomes congested.
     </p></li><li class="listitem"><p>
      Highly efficient write-back implementation. Dirty data is always written
      out in sorted order.
     </p></li><li class="listitem"><p>
      Stable and reliable—in production use.
     </p></li></ul></div></section><section class="sect2" id="sec-multitiercache-bcache-setting-bcache-device" data-id-title="Setting up a bcache device"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.2 </span><span class="title-name">Setting up a <code class="systemitem">bcache</code> device</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-bcache-setting-bcache-device">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section describes steps to set up and manage a <code class="systemitem">bcache</code> device.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Install the <code class="systemitem">bcache-tools</code> package:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper in bcache-tools</pre></div></li><li class="step"><p>
      Create a backing device (typically a mechanical drive). The backing
      device can be a whole device, a partition, or any other standard block
      device.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> make-bcache -B /dev/sdb</pre></div></li><li class="step"><p>
      Create a cache device (typically an SSD disk).
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> make-bcache -C /dev/sdc</pre></div><p>
      In this example, the default block and bucket sizes of 512 B and 128 KB
      are used. The block size should match the backing device's sector size
      which will usually be either 512 or 4k. The bucket size should match the
      erase block size of the caching device with the intention of reducing
      write amplification. For example, using a hard disk with 4k sectors and
      an SSD with an erase block size of 2 MB this command would look as
      follows:
     </p><div class="verbatim-wrap"><pre class="screen">sudo make-bcache --block 4k --bucket 2M -C /dev/sdc</pre></div><div id="id-1.11.3.5.7.4.3.3.5" data-id-title="Multi-device support" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Multi-device support</div><p>
       <code class="command">make-bcache</code> can prepare and register multiple backing
       devices and a cache device at the same time. In this case you do not
       need to manually attach the cache device to the backing device
       afterward:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> make-bcache -B /dev/sda /dev/sdb -C /dev/sdc</pre></div></div></li><li class="step"><p>
      <code class="systemitem">bcache</code> devices show up as
     </p><div class="verbatim-wrap"><pre class="screen">/dev/bcache<em class="replaceable">N</em></pre></div><p>
      and as
     </p><div class="verbatim-wrap"><pre class="screen">/dev/bcache/by-uuid/<em class="replaceable">UUID</em>
/dev/bcache/by-label/<em class="replaceable">LABEL</em></pre></div><p>
      You can normally format and mount <code class="systemitem">bcache</code> devices as usual:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mkfs.ext4 /dev/bcache0
<code class="prompt user">&gt; </code><code class="command">sudo</code> mount /dev/bcache0 /mnt</pre></div><p>
      You can control <code class="systemitem">bcache</code> devices through <code class="systemitem">sysfs</code>
      at
      <code class="filename">/sys/block/bcache<em class="replaceable">N</em>/bcache</code>.
     </p></li><li class="step"><p>
      After both the cache and backing devices are registered, you need to
      attach the backing device to the related cache set to enable caching:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>echo <em class="replaceable">CACHE_SET_UUID</em> &gt; /sys/block/bcache0/bcache/attach</pre></div><p>
      where <em class="replaceable">CACHE_SET_UUID</em> is found in
      <code class="filename">/sys/fs/bcache</code>.
     </p></li><li class="step"><p>
      By default <code class="systemitem">bcache</code> uses a pass-through caching mode. To change it to for
      example write-back, run
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>echo writeback &gt; /sys/block/bcache0/bcache/cache_mode</pre></div></li></ol></div></div></section><section class="sect2" id="sec-multitiercache-bcache-sysfs" data-id-title="bcache configuration using sysfs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.3 </span><span class="title-name"><code class="systemitem">bcache</code> configuration using <code class="systemitem">sysfs</code></span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-bcache-sysfs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <code class="systemitem">bcache</code> devices use the <code class="systemitem">sysfs</code> interface to store
    their runtime configuration values. This way you can change <code class="systemitem">bcache</code>
    backing and cache disks' behavior or see their usage statistics.
   </p><p>
    For the complete list of <code class="systemitem">bcache</code> <code class="systemitem">sysfs</code>
    parameters, see the contents of the
    <code class="filename">/usr/src/linux/Documentation/bcache.txt</code> file, mainly
    the <code class="literal">SYSFS - BACKING DEVICE</code>, <code class="literal">SYSFS - BACKING
    DEVICE STATS</code>, and <code class="literal">SYSFS - CACHE DEVICE</code>
    sections.
   </p></section></section><section class="sect1" id="sec-multitiercache-lvmcache" data-id-title="lvmcache"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name"><code class="systemitem">lvmcache</code></span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-lvmcache">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="systemitem">lvmcache</code> is a caching mechanism consisting of logical volumes (LVs). It
   uses the <code class="literal">dm-cache</code> kernel driver and supports
   write-through (default) and write-back caching modes. <code class="systemitem">lvmcache</code> improves
   performance of a large and slow LV by dynamically migrating some of its data
   to a faster and smaller LV. For more information on LVM, see
   <a class="xref" href="#part-lvm" title="Part II. Logical volumes (LVM)">Part II, “Logical volumes (LVM)”</a>.
  </p><p>
   LVM refers to the small, fast LV as a <span class="emphasis"><em>cache pool LV</em></span>.
   The large, slow LV is called the <span class="emphasis"><em>origin LV</em></span>. Because of
   requirements from dm-cache, LVM further splits the cache pool LV into two
   devices: the <span class="emphasis"><em>cache data LV</em></span> and <span class="emphasis"><em>cache
   metadata LV</em></span>. The cache data LV is where copies of data blocks
   are kept from the origin LV to increase speed. The cache metadata LV holds
   the accounting information that specifies where data blocks are stored.
  </p><section class="sect2" id="sec-multitiercache-lvmcache-configure" data-id-title="Configuring lvmcache"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.4.1 </span><span class="title-name">Configuring <code class="systemitem">lvmcache</code></span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-lvmcache-configure">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section describes steps to create and configure LVM based caching.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <span class="emphasis"><em>Create the origin LV.</em></span> Create a new LV or use an
      existing LV to become the origin LV:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -n <em class="replaceable">ORIGIN_LV</em> -L 100G vg <em class="replaceable">/dev/SLOW_DEV</em></pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Create the cache data LV.</em></span> This LV will hold data
      blocks from the origin LV. The size of this LV is the size of the cache
      and will be reported as the size of the cache pool LV.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -n <em class="replaceable">CACHE_DATA_LV</em> -L 10G vg <em class="replaceable">/dev/FAST</em></pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Create the cache metadata LV.</em></span> This LV will hold
      cache pool metadata. The size of this LV should be approximately 1000
      times smaller than the cache data LV, with a minimum size of 8MB.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -n <em class="replaceable">CACHE_METADATA_LV</em> -L 12M vg <em class="replaceable">/dev/FAST</em></pre></div><p>
      List the volumes you have created so far:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvs -a vg
LV                VG   Attr        LSize   Pool Origin
cache_data_lv     vg   -wi-a-----  10.00g
cache_metadata_lv vg   -wi-a-----  12.00m
origin_lv         vg   -wi-a----- 100.00g</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Create a cache pool LV.</em></span> Combine the data and
      metadata LVs into a cache pool LV. You can set the cache pool LV's
      behavior at the same time.
     </p><p>
      <em class="replaceable">CACHE_POOL_LV</em> takes the name of
      <em class="replaceable">CACHE_DATA_LV</em>.
     </p><p>
      <em class="replaceable">CACHE_DATA_LV</em> is renamed to
      <em class="replaceable">CACHE_DATA_LV</em>_cdata and becomes hidden.
     </p><p>
      <em class="replaceable">CACHE_META_LV</em> is renamed to
      <em class="replaceable">CACHE_DATA_LV</em>_cmeta and becomes hidden.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --type cache-pool \
 --poolmetadata vg/cache_metadata_lv vg/cache_data_lv</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvs -a vg
LV                     VG   Attr       LSize   Pool Origin
cache_data_lv          vg   Cwi---C---  10.00g
[cache_data_lv_cdata]  vg   Cwi-------  10.00g
[cache_data_lv_cmeta]  vg   ewi-------  12.00m
origin_lv              vg   -wi-a----- 100.00g</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Create a cache LV.</em></span> Create a cache LV by linking the
      cache pool LV to the origin LV.
     </p><p>
      The user accessible cache LV takes the name of the origin LV, while the
      origin LV becomes a hidden LV renamed to
      <em class="replaceable">ORIGIN_LV</em>_corig.
     </p><p>
      CacheLV takes the name of <em class="replaceable">ORIGIN_LV</em>.
     </p><p>
      <em class="replaceable">ORIGIN_LV</em> is renamed to
      <em class="replaceable">ORIGIN_LV</em>_corig and becomes hidden.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --type cache --cachepool vg/cache_data_lv vg/origin_lv</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvs -a vg
LV              VG   Attr       LSize   Pool   Origin
cache_data_lv          vg   Cwi---C---  10.00g
[cache_data_lv_cdata]  vg   Cwi-ao----  10.00g
[cache_data_lv_cmeta]  vg   ewi-ao----  12.00m
origin_lv              vg   Cwi-a-C--- 100.00g cache_data_lv [origin_lv_corig]
[origin_lv_corig]      vg   -wi-ao---- 100.00g</pre></div></li></ol></div></div></section><section class="sect2" id="sec-multitiercache-lvmcache-remove" data-id-title="Removing a cache pool"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.4.2 </span><span class="title-name">Removing a cache pool</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-lvmcache-remove">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are several ways to turn off the LV cache.
   </p><section class="sect3" id="sec-multitiercache-lvmcache-remove-detach" data-id-title="Detach a cache pool LV from a cache LV"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.2.1 </span><span class="title-name">Detach a cache pool LV from a cache LV</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-lvmcache-remove-detach">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You can disconnect a cache pool LV from a cache LV, leaving an unused
     cache pool LV and an uncached origin LV. Data are written back from the
     cache pool to the origin LV when necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --splitcache vg/origin_lv</pre></div></section><section class="sect3" id="sec-multitiercache-lvmcache-remove-wo-origin" data-id-title="Removing a cache pool LV without removing its origin LV"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.2.2 </span><span class="title-name">Removing a cache pool LV without removing its origin LV</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-lvmcache-remove-wo-origin">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This writes back data from the cache pool to the origin LV when necessary,
     then removes the cache pool LV, leaving the uncached origin LV.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvremove vg/cache_data_lv</pre></div><p>
     An alternative command that also disconnects the cache pool from the cache
     LV, and deletes the cache pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --uncache vg/origin_lv</pre></div></section><section class="sect3" id="sec-multitiercache-lvmcache-remove-both" data-id-title="Removing both the origin LV and the cache pool LV"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.2.3 </span><span class="title-name">Removing both the origin LV and the cache pool LV</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-lvmcache-remove-both">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Removing a cache LV removes both the origin LV and the linked cache pool
     LV.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvremove vg/origin_lv</pre></div></section><section class="sect3" id="sec-multitiercache-lvmcache-remove-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">4.4.2.4 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-multitiercache-lvmcache-remove-info">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multitier-caching.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You can find more <code class="systemitem">lvmcache</code> related topics, such as supported cache
     modes, redundant sub-logical volumes, cache policy, or converting existing
     LVs to cache types, in the <code class="systemitem">lvmcache</code> manual page (<code class="command">man 7
     lvmcache</code>).
    </p></section></section></section></section></div><div class="part" id="part-lvm" data-id-title="Logical volumes (LVM)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part II </span><span class="title-name">Logical volumes (LVM) </span></span><a title="Permalink" class="permalink" href="#part-lvm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/book_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-lvm"><span class="title-number">5 </span><span class="title-name">LVM configuration</span></a></span></li><dd class="toc-abstract"><p>
    This chapter describes the principles behind Logical Volume Manager (LVM)
    and its basic features that make it useful under many circumstances. The
    YaST LVM configuration can be reached from the YaST Expert Partitioner.
    This partitioning tool enables you to edit and delete existing partitions
    and create new ones that should be used with LVM.
   </p></dd><li><span class="chapter"><a href="#cha-lvm-snapshots"><span class="title-number">6 </span><span class="title-name">LVM volume snapshots</span></a></span></li><dd class="toc-abstract"><p>A Logical Volume Manager (LVM) logical volume snapshot is a copy-on-write technology that monitors changes to an existing volume’s data blocks so that when a write is made to one of the blocks, the block’s value at the snapshot time is copied to a snapshot volume. In this way, a point-in-time copy o…</p></dd></ul></div><section xml:lang="en" class="chapter" id="cha-lvm" data-id-title="LVM configuration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">LVM configuration</span></span> <a title="Permalink" class="permalink" href="#cha-lvm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This chapter describes the principles behind Logical Volume Manager (LVM)
    and its basic features that make it useful under many circumstances. The
    YaST LVM configuration can be reached from the YaST Expert Partitioner.
    This partitioning tool enables you to edit and delete existing partitions
    and create new ones that should be used with LVM.
   </p></div></div></div></div><div id="id-1.11.4.2.3" data-id-title="Risks" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Risks</div><p>
   Using LVM might be associated with increased risk, such as data loss. Risks
   also include application crashes, power failures, and faulty commands. Save
   your data before implementing LVM or reconfiguring volumes. Never work
   without a backup.
  </p></div><section class="sect1" id="sec-lvm-explained" data-id-title="Understanding the logical volume manager"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Understanding the logical volume manager</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-explained">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   LVM enables flexible distribution of hard disk space over several physical
   volumes (hard disks, partitions, LUNs). It was developed because the need to
   change the segmentation of hard disk space might arise only after the
   initial partitioning has already been done during installation. Because it
   is difficult to modify partitions on a running system, LVM provides a
   virtual pool (volume group or VG) of storage space from which logical
   volumes (LVs) can be created as needed. The operating system accesses these
   LVs instead of the physical partitions. Volume groups can span more than one
   disk, so that several disks or parts of them can constitute one single VG.
   In this way, LVM provides a kind of abstraction from the physical disk space
   that allows its segmentation to be changed in a much easier and safer way
   than through physical repartitioning.
  </p><p>
   <a class="xref" href="#fig-lvm-explain" title="Physical partitioning versus LVM">Figure 5.1, “Physical partitioning versus LVM”</a> compares physical
   partitioning (left) with LVM segmentation (right). On the left side, one
   single disk has been divided into three physical partitions (PART), each
   with a mount point (MP) assigned so that the operating system can access
   them. On the right side, two disks have been divided into two and three
   physical partitions each. Two LVM volume groups (VG 1 and VG 2)
   have been defined. VG 1 contains two partitions from DISK 1 and
   one from DISK 2. VG 2 contains the remaining two partitions from
   DISK 2.
  </p><div class="figure" id="fig-lvm-explain"><div class="figure-contents"><div class="mediaobject"><a href="images/lvm.png"><img src="images/lvm.png" width="100%" alt="Physical partitioning versus LVM" title="Physical partitioning versus LVM"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.1: </span><span class="title-name">Physical partitioning versus LVM </span></span><a title="Permalink" class="permalink" href="#fig-lvm-explain">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div><p>
   In LVM, the physical disk partitions that are incorporated in a volume group
   are called physical volumes (PVs). Within the volume groups in
   <a class="xref" href="#fig-lvm-explain" title="Physical partitioning versus LVM">Figure 5.1, “Physical partitioning versus LVM”</a>, four logical
   volumes (LV 1 through LV 4) have been defined, which can be used
   by the operating system via the associated mount points (MP). The border
   between different logical volumes need not be aligned with any partition
   border. See the border between LV 1 and LV 2 in this example.
  </p><p>
   LVM features:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Several hard disks or partitions can be combined in a large logical
     volume.
    </p></li><li class="listitem"><p>
     Provided the configuration is suitable, an LV (such as
     <code class="filename">/usr</code>) can be enlarged when the free space is
     exhausted.
    </p></li><li class="listitem"><p>
     Using LVM, it is possible to add hard disks or LVs in a running system.
     However, this requires hotpluggable hardware that is capable of such
     actions.
    </p></li><li class="listitem"><p>
     It is possible to activate a <span class="emphasis"><em>striping mode</em></span> that
     distributes the data stream of a logical volume over several physical
     volumes. If these physical volumes reside on different disks, this can
     improve the reading and writing performance like RAID 0.
    </p></li><li class="listitem"><p>
     The snapshot feature enables consistent backups (especially for servers)
     in the running system.
    </p></li></ul></div><div id="id-1.11.4.2.4.8" data-id-title="LVM and RAID" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: LVM and RAID</div><p>
    Even though LVM also supports RAID levels 0, 1, 4, 5 and 6, we recommend
    using <code class="literal">mdraid</code> (see <a class="xref" href="#cha-raid" title="Chapter 7. Software RAID configuration">Chapter 7, <em>Software RAID configuration</em></a>). However,
    LVM works fine with RAID 0 and 1, as RAID 0 is similar to common
    logical volume management (individual logical blocks are mapped onto blocks
    on the physical devices). LVM used on top of RAID 1 can keep track of
    mirror synchronization and is fully able to manage the synchronization
    process. With higher RAID levels you need a management daemon that monitors
    the states of attached disks and can inform administrators if there is a
    problem in the disk array. LVM includes such a daemon, but in exceptional
    situations such as a device failure, the daemon does not work properly.
   </p></div><div id="id-1.11.4.2.4.9" data-id-title="IBM Z: LVM root file system" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: IBM Z: LVM root file system</div><p>
    If you configure the system with a root file system on LVM or software RAID
    array, you must place <code class="filename">/boot</code> on a separate, non-LVM or
    non-RAID partition, otherwise the system will fail to boot. The recommended
    size for such a partition is 500 MB and the recommended file system is
    Ext4.
   </p></div><p>
   With these features, using LVM already makes sense for heavily-used home PCs
   or small servers. If you have a growing data stock, as in the case of
   databases, music archives, or user directories, LVM is especially useful. It
   allows file systems that are larger than the physical hard disk. However,
   keep in mind that working with LVM is different from working with
   conventional partitions.
  </p><p>
   You can manage new or existing LVM storage objects by using the YaST
   Partitioner. Instructions and further information about configuring LVM are
   available in the official
   <a class="link" href="https://tldp.org/HOWTO/LVM-HOWTO/" target="_blank"><em class="citetitle">LVM
   HOWTO</em></a>.
  </p></section><section class="sect1" id="sec-lvm-vg" data-id-title="Creating volume groups"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Creating volume groups</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-vg">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   An LVM volume group (VG) organizes the Linux LVM partitions into a logical
   pool of space. You can carve out logical volumes from the available space in
   the group. The Linux LVM partitions in a group can be on the same or
   different disks. You can add partitions or entire disks to expand the size
   of the group.
  </p><p>
   To use an entire disk, it must not contain any partitions. When using
   partitions, they must not be mounted. YaST will automatically change their
   partition type to <code class="literal">0x8E Linux LVM</code> when adding them to a
   VG.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     In case you need to reconfigure your existing partitioning setup, proceed
     as follows. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “<span class="guimenu">Expert Partitioner</span>”, Section 11.1 “Using the <span class="guimenu">Expert Partitioner</span>”</span> for details.
     Skip this step if you only want to use unused disks or partitions that
     already exist.
    </p><div id="id-1.11.4.2.5.4.2.2" data-id-title="Physical volumes on unpartitioned disks" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Physical volumes on unpartitioned disks</div><p>
      You can use an unpartitioned disk as a physical volume (PV) if that disk
      is <span class="emphasis"><em>not</em></span> the one where the operating system is
      installed and from which it boots.
     </p><p>
      As unpartitioned disks appear as <span class="emphasis"><em>unused</em></span> at the
      system level, they can easily be overwritten or wrongly accessed.
     </p></div><ol type="a" class="substeps"><li class="step"><p>
       To use an entire hard disk that already contains partitions, delete all
       partitions on that disk.
      </p></li><li class="step"><p>
       To use a partition that is currently mounted, unmount it.
      </p></li></ol></li><li class="step"><p>
     In the left panel, select <span class="guimenu">Volume Management</span>.
    </p><p>
     A list of existing Volume Groups opens in the right panel.
    </p></li><li class="step"><p>
     At the lower left of the Volume Management page, click <span class="guimenu">Add Volume
     Group</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_lvm4_a.png"><img src="images/yast2_lvm4_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Define the volume group as follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Specify the <span class="guimenu">Volume Group Name</span>.
      </p><p>
       If you are creating a volume group at install time, the name
       <code class="literal">system</code> is suggested for a volume group that will
       contain the <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> system files.
      </p></li><li class="step"><p>
       Specify the <span class="guimenu">Physical Extent Size</span>.
      </p><p>
       The <span class="guimenu">Physical Extent Size</span> defines the size of a
       physical block in the volume group. All the disk space in a volume group
       is handled in chunks of this size. Values can be from 1 KB to 16 GB in
       powers of 2. This value is normally set to 4 MB.
      </p><p>
       In LVM1, a 4 MB physical extent allowed a maximum LV size of 256 GB
       because it supports only up to 65534 extents per LV. LVM2, which is used
       on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, does not restrict the number of physical extents.
       Having many extents has no impact on I/O performance to the logical
       volume, but it slows down the LVM tools.
      </p><div id="id-1.11.4.2.5.4.5.2.2.4" data-id-title="Physical extent sizes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Physical extent sizes</div><p>
        Different physical extent sizes should not be mixed in a single VG. The
        extent should not be modified after the initial setup.
       </p></div></li><li class="step"><p>
       In the <span class="guimenu">Available Physical Volumes</span> list, select the
       Linux LVM partitions that you want to make part of this volume group,
       then click <span class="guimenu">Add</span> to move them to the <span class="guimenu">Selected
       Physical Volumes</span> list.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Finish</span>.
      </p><p>
       The new group appears in the <span class="guimenu">Volume Groups</span> list.
      </p></li></ol></li><li class="step"><p>
     On the Volume Management page, click <span class="guimenu">Next</span>, verify that
     the new volume group is listed, then click <span class="guimenu">Finish</span>.
    </p></li><li class="step"><p>
     To check which physical devices are part of the volume group, open the
     YaST Partitioner at any time in the running system and click
     <span class="guimenu">Volume Management</span> › <span class="guimenu">Edit</span> › <span class="guimenu">Physical Devices</span>. Leave this screen with
     <span class="guimenu">Abort</span>.
    </p><div class="figure" id="fig-yast2-lvm-physical-volumes"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_lvm5_a.png"><img src="images/yast2_lvm5_a.png" width="100%" alt="Physical volumes in the volume group named DATA" title="Physical volumes in the volume group named DATA"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.2: </span><span class="title-name">Physical volumes in the volume group named DATA </span></span><a title="Permalink" class="permalink" href="#fig-yast2-lvm-physical-volumes">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></li></ol></div></div></section><section class="sect1" id="sec-lvm-lv" data-id-title="Creating logical volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Creating logical volumes</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-lv">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A logical volume provides a pool of space similar to what a hard disk does.
   To make this space usable, you need to define logical volumes. A logical
   volume is similar to a regular partition—you can format and mount it.
  </p><p>
   Use the YaST Partitioner to create logical volumes from an existing volume
   group. Assign at least one logical volume to each volume group. You can
   create new logical volumes as needed until all free space in the volume
   group has been exhausted. An LVM logical volume can optionally be thinly
   provisioned, allowing you to create logical volumes with sizes that overbook
   the available free space (see <a class="xref" href="#sec-lvm-lv-thin" title="5.3.1. Thinly provisioned logical volumes">Section 5.3.1, “Thinly provisioned logical volumes”</a> for more
   information).
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Normal volume:</span>
      (Default) The volume’s space is allocated immediately.
     </p></li><li class="listitem"><p><span class="formalpara-title">Thin pool:</span>
      The logical volume is a pool of space that is reserved for use with thin
      volumes. The thin volumes can allocate their needed space from it on
      demand.
     </p></li><li class="listitem"><p><span class="formalpara-title">Thin volume:</span>
      The volume is created as a sparse volume. The volume allocates needed
      space on demand from a thin pool.
     </p></li><li class="listitem"><p><span class="formalpara-title">Mirrored volume:</span>
      The volume is created with a defined count of mirrors.
     </p></li></ul></div><div class="procedure" id="pro-lvm-lv" data-id-title="Setting up a logical volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.1: </span><span class="title-name">Setting up a logical volume </span></span><a title="Permalink" class="permalink" href="#pro-lvm-lv">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     In the left panel, select <span class="guimenu">Volume Management</span>. A list of
     existing Volume Groups opens in the right panel.
    </p></li><li class="step"><p>
     Select the volume group in which you want to create the volume and choose
     <span class="guimenu">Logical Volumes</span> › <span class="guimenu">Add Logical
     Volume</span>.
    </p></li><li class="step"><p>
     Provide a <span class="guimenu">Name</span> for the volume and choose
     <span class="guimenu">Normal Volume</span> (refer to
     <a class="xref" href="#sec-lvm-lv-thin" title="5.3.1. Thinly provisioned logical volumes">Section 5.3.1, “Thinly provisioned logical volumes”</a> for setting up thinly provisioned
     volumes). Proceed with <span class="guimenu">Next</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_lvm9_a.png"><img src="images/yast2_lvm9_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Specify the size of the volume and whether to use multiple stripes.
    </p><p>
     Using a striped volume, the data will be distributed among several
     physical volumes. If these physical volumes reside on different hard
     disks, this generally results in a better reading and writing performance
     (like RAID 0). The maximum number of available stripes is equal to
     the number of physical volumes. The default (<code class="literal">1</code> is to
     not use multiple stripes.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_lvm10_a.png"><img src="images/yast2_lvm10_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Choose a <span class="guimenu">Role</span> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <span class="guimenu">Raw Volume
     (Unformatted)</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_lvm11_a.png"><img src="images/yast2_lvm11_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Under <span class="guimenu">Formatting Options</span>, select <span class="guimenu">Format
     Partition</span>, then select the <span class="guimenu">File system</span>. The
     content of the <span class="guimenu">Options</span> menu depends on the file system.
     Usually there is no need to change the defaults.
    </p><p>
     Under <span class="guimenu">Mounting Options</span>, select <span class="guimenu">Mount
     partition</span>, then select the mount point. Click <span class="guimenu">Fstab
     Options</span> to add special mounting options for the volume.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Finish</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Next</span>, verify that the changes are listed, then
     click <span class="guimenu">Finish</span>.
    </p></li></ol></div></div><section class="sect2" id="sec-lvm-lv-thin" data-id-title="Thinly provisioned logical volumes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Thinly provisioned logical volumes</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-lv-thin">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    An LVM logical volume can optionally be thinly provisioned. Thin
    provisioning allows you to create logical volumes with sizes that overbook
    the available free space. You create a thin pool that contains unused space
    reserved for use with an arbitrary number of thin volumes. A thin volume is
    created as a sparse volume and space is allocated from a thin pool as
    needed. The thin pool can be expanded dynamically when needed for
    cost-effective allocation of storage space. Thinly provisioned volumes also
    support snapshots which can be managed with Snapper—see
    <span class="intraxref">Book “Administration Guide”, Chapter 10 “System recovery and snapshot management with Snapper”</span> for more information.
   </p><p>
    To set up a thinly provisioned logical volume, proceed as described in
    <a class="xref" href="#pro-lvm-lv" title="Setting up a logical volume">Procedure 5.1, “Setting up a logical volume”</a>. When it comes to choosing the volume type, do
    not choose <span class="guimenu">Normal Volume</span>, but rather <span class="guimenu">Thin
    Volume</span> or <span class="guimenu">Thin Pool</span>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.4.2.6.6.4.1"><span class="term"><span class="guimenu">Thin pool</span></span></dt><dd><p>
       The logical volume is a pool of space that is reserved for use with thin
       volumes. The thin volumes can allocate their needed space from it on
       demand.
      </p></dd><dt id="id-1.11.4.2.6.6.4.2"><span class="term"><span class="guimenu">Thin volume</span></span></dt><dd><p>
       The volume is created as a sparse volume. The volume allocates needed
       space on demand from a thin pool.
      </p></dd></dl></div><div id="id-1.11.4.2.6.6.5" data-id-title="Thinly provisioned volumes in a cluster" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Thinly provisioned volumes in a cluster</div><p>
     To use thinly provisioned volumes in a cluster, the thin pool and the thin
     volumes that use it must be managed in a single cluster resource. This
     allows the thin volumes and thin pool to always be mounted exclusively on
     the same node.
    </p></div></section><section class="sect2" id="sec-lvm-lv-mirror" data-id-title="Creating mirrored volumes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Creating mirrored volumes</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-lv-mirror">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A logical volume can be created with several mirrors. LVM ensures that data
    written to an underlying physical volume is mirrored onto a different
    physical volume. Thus even though a physical volume crashes, you can still
    access the data on the logical volume. LVM also keeps a log file to manage
    the synchronization process. The log contains information about which
    volume regions are currently undergoing synchronization with mirrors. By
    default the log is stored on disk and if possible on a different disk than
    are the mirrors. But you may specify a different location for the log, for
    example volatile memory.
   </p><p>
    Currently there are two types of mirror implementation available: "normal"
    (non-raid) <code class="literal">mirror</code> logical volumes and
    <code class="literal">raid1</code> logical volumes.
   </p><p>
    After you create mirrored logical volumes, you can perform standard
    operations with mirrored logical volumes like activating, extending, and
    removing.
   </p><section class="sect3" id="sec-mirroring-procedure" data-id-title="Setting up mirrored non-RAID logical volumes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.2.1 </span><span class="title-name">Setting up mirrored non-RAID logical volumes</span></span> <a title="Permalink" class="permalink" href="#sec-mirroring-procedure">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To create a mirrored volume use the <code class="command">lvcreate</code> command.
     The following example creates a 500 GB logical volume with two
     mirrors called <span class="emphasis"><em>lv1</em></span>, which uses a volume group
     <span class="emphasis"><em>vg1</em></span>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -L 500G -m 2 -n lv1 vg1</pre></div><p>
     Such a logical volume is a linear volume (without striping) that provides
     three copies of the file system. The <code class="literal">m</code> option specifies
     the count of mirrors. The <code class="literal">L</code> option specifies the size
     of the logical volumes.
    </p><p>
     The logical volume is divided into regions of the 512 KB default size. If
     you need a different size of regions, use the <code class="literal">-R</code> option
     followed by the desired region size in megabytes. Or you can configure the
     preferred region size by editing the <code class="literal">mirror_region_size</code>
     option in the <code class="filename">lvm.conf</code> file.
    </p></section><section class="sect3" id="sec-raid1-lvm-mirroring" data-id-title="Setting up raid1 logical volumes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.2.2 </span><span class="title-name">Setting up <code class="literal">raid1</code> logical volumes</span></span> <a title="Permalink" class="permalink" href="#sec-raid1-lvm-mirroring">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     As LVM supports RAID you can implement mirroring by using RAID1. Such
     implementation provides the following advantages compared to the non-raid
     mirrors:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       LVM maintains a fully redundant bitmap area for each mirror image, which
       increases its fault handling capabilities.
      </p></li><li class="listitem"><p>
       Mirror images can be temporarily split from the array and then merged
       back.
      </p></li><li class="listitem"><p>
       The array can handle transient failures.
      </p></li><li class="listitem"><p>
       The LVM RAID 1 implementation supports snapshots.
      </p></li></ul></div><p>
     On the other hand, this type of mirroring implementation does not enable
     to create a logical volume in a clustered volume group.
    </p><p>
     To create a mirror volume by using RAID, issue the command
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate --type raid1 -m 1 -L 1G -n lv1 vg1</pre></div><p>
     where the options/parameters have the following meanings:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">--type</code> - you need to specify
       <code class="literal">raid1</code>, otherwise the command uses the implicit
       segment type <code class="literal">mirror</code> and creates a non-raid mirror.
      </p></li><li class="listitem"><p>
       <code class="literal">-m</code> - specifies the count of mirrors.
      </p></li><li class="listitem"><p>
       <code class="literal">-L</code> - specifies the size of the logical volume.
      </p></li><li class="listitem"><p>
       <code class="literal">-n</code> - by using this option you specify a name of the
       logical volume.
      </p></li><li class="listitem"><p>
       <code class="literal">vg1</code> - is a name of the volume group used by the
       logical volume.
      </p></li></ul></div><p>
     LVM creates a logical volume of one extent size for each data volume in
     the array. If you have two mirrored volumes, LVM creates another two
     volumes that stores metadata.
    </p><p>
     After you create a RAID logical volume, you can use the volume in the same
     way as a common logical volume. You can activate it, extend it, etc.
    </p></section></section></section><section class="sect1" id="sec-lvm-activate-vgs" data-id-title="Automatically activating non-root LVM volume groups"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">Automatically activating non-root LVM volume groups</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-activate-vgs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Activation behavior for non-root LVM volume groups is controlled in the
   <code class="literal">/etc/lvm/lvm.conf</code> file and by the
   <em class="parameter">auto_activation_volume_list</em> parameter. By default,
   the parameter is empty and all volumes are activated. To activate only some
   volume groups, add the names in quotes and separate them with commas, for
   example:
  </p><div class="verbatim-wrap"><pre class="screen">auto_activation_volume_list = [ "vg1", "vg2/lvol1", "@tag1", "@*" ]</pre></div><p>
   If you have defined a list in the
   <em class="parameter">auto_activation_volume_list</em> parameter, the following
   will happen:
   
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Each logical volume is first checked against this list.
    </p></li><li class="listitem"><p>
     If it does not match, the logical volume will not be activated.
    </p></li></ol></div><p>
   By default, non-root LVM volume groups are automatically activated on system
   restart by dracut. This parameter allows you to activate all volume groups
   on system restart, or to activate only specified non-root LVM volume groups.
  </p></section><section class="sect1" id="sec-lvm-vg-resize" data-id-title="Resizing an existing volume group"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.5 </span><span class="title-name">Resizing an existing volume group</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-vg-resize">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The space provided by a volume group can be expanded at any time in the
   running system without service interruption by adding physical volumes. This
   will allow you to add logical volumes to the group or to expand the size of
   existing volumes as described in <a class="xref" href="#sec-lvm-lv-resize" title="5.6. Resizing a logical volume">Section 5.6, “Resizing a logical volume”</a>.
  </p><p>
   It is also possible to reduce the size of the volume group by removing
   physical volumes. YaST only allows to remove physical volumes that are
   currently unused. To find out which physical volumes are currently in use,
   run the following command. The partitions (physical volumes) listed in the
   <code class="literal">PE Ranges</code> column are the ones in use:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> pvs -o vg_name,lv_name,pv_name,seg_pe_ranges
root's password:
  VG   LV    PV         PE Ranges
             /dev/sda1
  DATA DEVEL /dev/sda5  /dev/sda5:0-3839
  DATA       /dev/sda5
  DATA LOCAL /dev/sda6  /dev/sda6:0-2559
  DATA       /dev/sda7
  DATA       /dev/sdb1
  DATA       /dev/sdc1</pre></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     In the left panel, select <span class="guimenu">Volume Management</span>. A list of
     existing Volume Groups opens in the right panel.
    </p></li><li class="step"><p>
     Select the volume group you want to change, activate the <span class="guimenu">Physical
     Volumes</span> tab, then click <span class="guimenu">Change</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_lvm8_a.png"><img src="images/yast2_lvm8_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Do one of the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Add:</span>
        Expand the size of the volume group by moving one or more physical
        volumes (LVM partitions) from the <span class="guimenu">Available Physical
        Volumes</span> list to the <span class="guimenu">Selected Physical
        Volumes</span> list.
       </p></li><li class="listitem"><p><span class="formalpara-title">Remove:</span>
        Reduce the size of the volume group by moving one or more physical
        volumes (LVM partitions) from the <span class="guimenu">Selected Physical
        Volumes</span> list to the <span class="guimenu">Available Physical
        Volumes</span> list.
       </p></li></ul></div></li><li class="step"><p>
     Click <span class="guimenu">Finish</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Next</span>, verify that the changes are listed, then
     click <span class="guimenu">Finish</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-lvm-lv-resize" data-id-title="Resizing a logical volume"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.6 </span><span class="title-name">Resizing a logical volume</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-lv-resize">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In case there is unused free space available in the volume group, you can
   enlarge a logical volume to provide more usable space. You may also reduce
   the size of a volume to free space in the volume group that can be used by
   other logical volumes.
  </p><div id="id-1.11.4.2.9.3" data-id-title="Online resizing" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <span class="quote">“<span class="quote">Online</span>”</span> resizing</div><p>
    When reducing the size of a volume, YaST automatically resizes its file
    system, too. Whether a volume that is currently mounted can be resized
    <span class="quote">“<span class="quote">online</span>”</span> (that is while being mounted), depends on its file
    system. Growing the file system online is supported by Btrfs, XFS, Ext3,
    and Ext4.
   </p><p>
    Shrinking the file system online is only supported by Btrfs. To shrink the
    Ext2/3/4 file systems, you need to unmount them. Shrinking volumes
    formatted with XFS is not possible, since XFS does not support file system
    shrinking.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and under <span class="guimenu">System</span>, open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     In the left panel, select the LVM volume group.
    </p></li><li class="step"><p>
     Select the logical volume you want to change in the right panel. 
    </p></li><li class="step"><p>
         Click <span class="guimenu">Device</span> and then <span class="guimenu">Resize</span>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_lvm12_a.png"><img src="images/yast2_lvm12_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Set the intended size by using one of the following options:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Maximum size. </span>
        Expand the size of the logical volume to use all space left in the
        volume group.
       </p></li><li class="listitem"><p><span class="formalpara-title">Minimum size. </span>
        Reduce the size of the logical volume to the size occupied by the data
        and the file system metadata.
       </p></li><li class="listitem"><p><span class="formalpara-title">Custom size. </span>
        Specify the new size for the volume. The value must be within the range
        of the minimum and maximum values listed above. Use K, M, G, T for
        Kilobytes, Megabytes, Gigabytes and Terabytes (for example
        <code class="literal">20G</code>).
       </p></li></ul></div></li><li class="step"><p>
     Click <span class="guimenu">Next</span>, verify that the change is listed, then
     click <span class="guimenu">Finish</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-lvm-delete" data-id-title="Deleting a volume group or a logical volume"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.7 </span><span class="title-name">Deleting a volume group or a logical volume</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-delete">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.11.4.2.10.2" data-id-title="Data loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Data loss</div><p>
    Deleting a volume group destroys all of the data in each of its member
    partitions. Deleting a logical volume destroys all data stored on the
    volume.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     In the left panel, select <span class="guimenu">Volume Management</span>. A list of
     existing volume groups opens in the right panel.
    </p></li><li class="step"><p>
     Select the volume group or the logical volume you want to remove and click
     <span class="guimenu">Delete</span>.
    </p></li><li class="step"><p>
     Depending on your choice, warning dialogs are shown. Confirm them with
     <span class="guimenu">Yes</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Next</span>, verify that the deleted volume group is
     listed—deletion is indicated by a red-colored font—then click
     <span class="guimenu">Finish</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-lvm-onboot" data-id-title="Disabling LVM on boot"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.8 </span><span class="title-name">Disabling LVM on boot</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-onboot">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If there is an error on the LVM storage, the scanning of LVM volumes may
   prevent the emergency/rescue shell from being entered. This makes further problem diagnosis
   impossible. To disable this scanning in case of an LVM storage failure,
   you can pass the <code class="option">nolvm</code> option on the kernel command line.
  </p></section><section class="sect1" id="sec-lvm-cli" data-id-title="Using LVM commands"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.9 </span><span class="title-name">Using LVM commands</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-cli">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For information about using LVM commands, see the <code class="command">man</code>
   pages for the commands described in the following table. All commands need
   to be executed with <code class="systemitem">root</code> privileges. Either use
   <code class="command">sudo</code> <em class="replaceable">COMMAND</em> (recommended), or
   execute them directly as <code class="systemitem">root</code>.
  </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">LVM commands </span></span><a title="Permalink" class="permalink" href="#id-1.11.4.2.12.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.11.4.2.12.3.2"><span class="term"><code class="command">pvcreate <em class="replaceable">DEVICE</em></code></span></dt><dd><p>
      Initializes a device (such as <code class="filename">/dev/sdb1</code>) for use by
      LVM as a physical volume. If there is any file system on the specified
      device, a warning appears. Bear in mind that <code class="command">pvcreate</code>
      checks for existing file systems only if <code class="command">blkid</code> is
      installed (which is done by default). If <code class="command">blkid</code> is not
      available, <code class="command">pvcreate</code> will not produce any warning and
      you may lose your file system without any warning.
     </p></dd><dt id="id-1.11.4.2.12.3.3"><span class="term"><code class="command">pvdisplay <em class="replaceable">DEVICE</em></code></span></dt><dd><p>
      Displays information about the LVM physical volume, such as whether it is
      currently being used in a logical volume.
     </p></dd><dt id="id-1.11.4.2.12.3.4"><span class="term"><code class="command">vgcreate -c y <em class="replaceable">VG_NAME</em> <em class="replaceable">DEV1</em> [<em class="replaceable">DEV2</em>...]</code></span></dt><dd><p>
      Creates a clustered volume group with one or more specified devices.
     </p></dd><dt id="id-1.11.4.2.12.3.5"><span class="term"><code class="command">vgcreate --activationmode <em class="replaceable">ACTIVATION_MODE</em> <em class="replaceable">VG_NAME</em></code></span></dt><dd><p>
      Configures the mode of volume group activation. You can specify one of
      the following values:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">complete</code> - only the logical volumes that are not
        affected by missing physical volumes can be activated, even though the
        particular logical volume can tolerate such a failure.
       </p></li><li class="listitem"><p>
        <code class="literal">degraded</code> - is the default activation mode. If there
        is a sufficient level of redundancy to activate a logical volume, the
        logical volume can be activated even though some physical volumes are
        missing.
       </p></li><li class="listitem"><p>
        <code class="literal">partial</code> - the LVM tries to activate the volume group
        even though some physical volumes are missing. If a non-redundant
        logical volume is missing important physical volumes, then the logical
        volume usually cannot be activated and is handled as an error target.
       </p></li></ul></div></dd><dt id="id-1.11.4.2.12.3.6"><span class="term"><code class="command">vgchange -a [ey|n] <em class="replaceable">VG_NAME</em></code></span></dt><dd><p>
      Activates (<code class="literal">-a ey</code>) or deactivates (<code class="literal">-a
      n</code>) a volume group and its logical volumes for input/output.
     </p><p>
      When activating a volume in a cluster, ensure that you use the
      <code class="literal">ey</code> option. This option is used by default in the load
      script.
     </p></dd><dt id="id-1.11.4.2.12.3.7"><span class="term"><code class="command">vgremove <em class="replaceable">VG_NAME</em></code></span></dt><dd><p>
      Removes a volume group. Before using this command, remove the logical
      volumes, then deactivate the volume group.
     </p></dd><dt id="id-1.11.4.2.12.3.8"><span class="term"><code class="command">vgdisplay <em class="replaceable">VG_NAME</em></code></span></dt><dd><p>
      Displays information about a specified volume group.
     </p><p>
      To find the total physical extent of a volume group, enter
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>vgdisplay <em class="replaceable">VG_NAME</em> | grep "Total PE"</pre></div></dd><dt id="id-1.11.4.2.12.3.9"><span class="term"><code class="command">lvcreate -L <em class="replaceable">SIZE</em> -n <em class="replaceable">LV_NAME</em> <em class="replaceable">VG_NAME</em></code></span></dt><dd><p>
      Creates a logical volume of the specified size.
     </p></dd><dt id="id-1.11.4.2.12.3.10"><span class="term"><code class="command">lvcreate -L <em class="replaceable">SIZE</em> --thinpool <em class="replaceable">POOL_NAME</em> <em class="replaceable">VG_NAME</em></code></span></dt><dd><p>
      Creates a thin pool named <code class="literal">myPool</code> of the specified size
      from the volume group <em class="replaceable">VG_NAME</em>.
     </p><p>
      The following example creates a thin pool with a size of 5 GB from the
      volume group <code class="literal">LOCAL</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -L 5G --thinpool myPool LOCAL</pre></div></dd><dt id="id-1.11.4.2.12.3.11"><span class="term"><code class="command">lvcreate -T <em class="replaceable">VG_NAME</em>/<em class="replaceable">POOL_NAME</em> -V <em class="replaceable">SIZE</em> -n <em class="replaceable">LV_NAME</em></code></span></dt><dd><p>
      Creates a thin logical volume within the pool
      <em class="replaceable">POOL_NAME</em>. The following example creates a 1GB
      thin volume named <code class="literal">myThin1</code> from the pool
      <code class="literal">myPool</code> on the volume group <code class="literal">LOCAL</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -T LOCAL/myPool -V 1G -n myThin1</pre></div></dd><dt id="id-1.11.4.2.12.3.12"><span class="term"><code class="command">lvcreate -T <em class="replaceable">VG_NAME</em>/<em class="replaceable">POOL_NAME</em> -V <em class="replaceable">SIZE</em> -L <em class="replaceable">SIZE</em> -n <em class="replaceable">LV_NAME</em></code></span></dt><dd><p>
      It is also possible to combine thin pool and thin logical volume creation
      in one command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -T LOCAL/myPool -V 1G -L 5G -n myThin1</pre></div></dd><dt id="id-1.11.4.2.12.3.13"><span class="term"><code class="command">lvcreate --activationmode <em class="replaceable">ACTIVATION_MODE</em> <em class="replaceable">LV_NAME</em></code></span></dt><dd><p>
      Configures the mode of logical volume activation. You can specify one of
      the following values:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">complete</code> - the logical volume can be activated only
        if all its physical volumes are active.
       </p></li><li class="listitem"><p>
        <code class="literal">degraded</code> - is the default activation mode. If there
        is a sufficient level of redundancy to activate a logical volume, the
        logical volume can be activated even though some physical volumes are
        missing.
       </p></li><li class="listitem"><p>
        <code class="literal">partial</code> - the LVM tries to activate the volume even
        though some physical volumes are missing. In this case part of the
        logical volume may be unavailable and it might cause data loss. This
        option is typically not used, but might be useful when restoring data.
       </p></li></ul></div><p>
      You can specify the activation mode also in
      <code class="filename">/etc/lvm/lvm.conf</code> by specifying one of the above
      described values of the <code class="literal">activation_mode</code> configuration
      option.
     </p></dd><dt id="id-1.11.4.2.12.3.14"><span class="term"><code class="command">lvcreate -s [-L <em class="replaceable">SIZE</em>] -n <em class="replaceable">SNAP_VOLUME</em> <em class="replaceable">SOURCE_VOLUME_PATH</em> <em class="replaceable">VG_NAME</em></code></span></dt><dd><p>
      Creates a snapshot volume for the specified logical volume. If the size
      option (<code class="option">-L</code> or <code class="option">--size</code>) is not included,
      the snapshot is created as a thin snapshot.
     </p></dd><dt id="id-1.11.4.2.12.3.15"><span class="term"><code class="command">lvremove /dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">LV_NAME</em></code></span></dt><dd><p>
      Removes a logical volume.
     </p><p>
      Before using this command, close the logical volume by unmounting it with
      the <code class="command">umount</code> command.
     </p></dd><dt id="id-1.11.4.2.12.3.16"><span class="term"><code class="command">lvremove <em class="replaceable">SNAP_VOLUME_PATH</em></code></span></dt><dd><p>
      Removes a snapshot volume.
     </p></dd><dt id="id-1.11.4.2.12.3.17"><span class="term"><code class="command">lvconvert --merge <em class="replaceable">SNAP_VOLUME_PATH</em></code></span></dt><dd><p>
      Reverts the logical volume to the version of the snapshot.
     </p></dd><dt id="id-1.11.4.2.12.3.18"><span class="term"><code class="command">vgextend <em class="replaceable">VG_NAME</em> <em class="replaceable">DEVICE</em></code></span></dt><dd><p>
      Adds the specified device (physical volume) to an existing volume group.
     </p></dd><dt id="id-1.11.4.2.12.3.19"><span class="term"><code class="command">vgreduce <em class="replaceable">VG_NAME</em> <em class="replaceable">DEVICE</em></code></span></dt><dd><p>
      Removes a specified physical volume from an existing volume group.
     </p><p>
      Ensure that the physical volume is not currently being used by a logical
      volume. If it is, you must move the data to another physical volume by
      using the <code class="command">pvmove</code> command.
     </p></dd><dt id="id-1.11.4.2.12.3.20"><span class="term"><code class="command">lvextend -L <em class="replaceable">SIZE</em> /dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">LV_NAME</em></code></span></dt><dd><p>
      Extends the size of a specified logical volume. Afterward, you must also
      expand the file system to take advantage of the newly available space.
      See <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a> for details.
     </p></dd><dt id="id-1.11.4.2.12.3.21"><span class="term"><code class="command">lvreduce -L <em class="replaceable">SIZE</em> /dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">LV_NAME</em></code></span></dt><dd><p>
      Reduces the size of a specified logical volume.
     </p><p>
      Ensure that you reduce the size of the file system first before shrinking
      the volume, otherwise you risk losing data. See
      <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a> for details.
     </p></dd><dt id="id-1.11.4.2.12.3.22"><span class="term"><code class="command">lvrename /dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">LV_NAME</em> /dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">NEW_LV_NAME</em></code></span></dt><dd><p>
      Renames an existing LVM logical volume. It does not change the volume
      group name.
     </p></dd></dl></div><div id="id-1.11.4.2.12.4" data-id-title="Bypassing udev on volume creation" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Bypassing udev on volume creation</div><p>
    In case you want to manage LV device nodes and symbolic links by using LVM
    instead of by using udev rules, you can achieve this by disabling
    notifications from udev with one of the following methods:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Configure <code class="literal">activation/udev_rules = 0</code> and
      <code class="literal">activation/udev_sync = 0</code> in
      <code class="filename">/etc/lvm/lvm.conf</code>.
     </p><p>
      Note that specifying <code class="option">--nodevsync</code> with the
      <code class="command">lvcreate</code> command has the same effect as
      <code class="literal">activation/udev_sync = 0</code>; setting
      <code class="literal">activation/udev_rules = 0</code> is still required.
     </p></li><li class="listitem"><p>
      Setting the environment variable <code class="envar">DM_DISABLE_UDEV</code>:
     </p><div class="verbatim-wrap"><pre class="screen">export DM_DISABLE_UDEV=1</pre></div><p>
      This will also disable notifications from udev. In addition, all udev
      related settings from <code class="filename">/etc/lvm/lvm.conf</code> will be
      ignored.
     </p></li></ul></div></div><section class="sect2" id="sec-lvm-cli-resize" data-id-title="Resizing a logical volume with commands"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.9.1 </span><span class="title-name">Resizing a logical volume with commands</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-cli-resize">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="command">lvresize</code>, <code class="command">lvextend</code>, and
    <code class="command">lvreduce</code> commands are used to resize logical volumes.
    See the man pages for each of these commands for syntax and options
    information. To extend an LV there must be enough unallocated space
    available on the VG.
   </p><p>
    The recommended way to grow or shrink a logical volume is to use the YaST
    Partitioner. When using YaST, the size of the file system in the volume
    will automatically be adjusted, too.
   </p><p>
    LVs can be extended or shrunk manually while they are being used, but this
    may not be true for a file system on them. Extending or shrinking the LV
    does not automatically modify the size of file systems in the volume. You
    must use a different command to grow the file system afterward. For
    information about resizing file systems, see
    <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a>.
   </p><p>
    Ensure that you use the right sequence when manually resizing an LV:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      If you extend an LV, you must extend the LV before you attempt to grow
      the file system.
     </p></li><li class="listitem"><p>
      If you shrink an LV, you must shrink the file system before you attempt
      to shrink the LV.
     </p></li></ul></div><p>
    To extend the size of a logical volume:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      If the logical volume contains an Ext2 or Ext4 file system, which do not
      support online growing, dismount it. In case it contains file systems
      that are hosted for a virtual machine (such as a Xen VM), shut down the
      VM first.
     </p></li><li class="step"><p>
      At the terminal prompt, enter the following command to grow the size of
      the logical volume:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvextend -L +<em class="replaceable">SIZE</em> /dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">LV_NAME</em></pre></div><p>
      For <em class="replaceable">SIZE</em>, specify the amount of space you want
      to add to the logical volume, such as 10 GB. Replace
      <code class="filename">/dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">LV_NAME</em></code>
      with the Linux path to the logical volume, such as
      <code class="filename">/dev/LOCAL/DATA</code>. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvextend -L +10GB /dev/vg1/v1</pre></div></li><li class="step"><p>
      Adjust the size of the file system. See <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a>
      for details.
     </p></li><li class="step"><p>
      In case you have dismounted the file system, mount it again.
     </p></li></ol></div></div><p>
    For example, to extend an LV with a (mounted and active) Btrfs on it by 10
    GB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvextend −L +10G /dev/LOCAL/DATA
<code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem resize +10G /dev/LOCAL/DATA</pre></div><p>
    To shrink the size of a logical volume:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      If the logical volume does not contain a Btrfs file system, dismount it.
      In case it contains file systems that are hosted for a virtual machine
      (such as a Xen VM), shut down the VM first. Note that volumes with the
      XFS file system cannot be reduced in size.
     </p></li><li class="step"><p>
      Adjust the size of the file system. See <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a>
      for details.
     </p></li><li class="step"><p>
      At the terminal prompt, enter the following command to shrink the size of
      the logical volume to the size of the file system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvreduce /dev/<em class="replaceable">VG_NAME</em>/<em class="replaceable">LV_NAME</em></pre></div></li><li class="step"><p>
      In case you have unmounted the file system, mount it again.
     </p></li></ol></div></div><p>
    For example, to shrink an LV with a Btrfs on it by 5 GB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> btrfs filesystem resize -size 5G /dev/LOCAL/DATA
sudo lvreduce /dev/LOCAL/DATA</pre></div><div id="id-1.11.4.2.12.5.15" data-id-title="Resizing the Volume and the File System with a Single Command" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Resizing the Volume and the File System with a Single Command</div><p>
     Starting with <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12 SP1, <code class="command">lvextend</code>,
     <code class="command">lvresize</code>, and <code class="command">lvreduce</code> support the
     option <code class="option">--resizefs</code> which will not only change the size of
     the volume, but will also resize the file system. Therefore the examples
     for <code class="command">lvextend</code> and <code class="command">lvreduce</code> shown
     above can alternatively be run as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvextend --resizefs −L +10G /dev/LOCAL/DATA
<code class="prompt user">&gt; </code><code class="command">sudo</code> lvreduce  --resizefs -L -5G /dev/LOCAL/DATA</pre></div><p>
     Note that the <code class="option">--resizefs</code> is supported for the following
     file systems: ext2/3/4, Btrfs, XFS. Resizing Btrfs with this option is
     currently only available on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, since it is not yet accepted
     upstream.
    </p></div></section><section class="sect2" id="sec-lvm-cli-lvmcache" data-id-title="Using LVM cache volumes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.9.2 </span><span class="title-name">Using LVM cache volumes</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-cli-lvmcache">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    LVM supports the use of fast block devices (such as an SSD device) as
    write-back or write-through caches for large slower block devices. The
    cache logical volume type uses a small and fast LV to improve the
    performance of a large and slow LV.
   </p><p>
    To set up LVM caching, you need to create two logical volumes on the
    caching device. A large one is used for the caching itself, a smaller
    volume is used to store the caching metadata. These two volumes need to be
    part of the same volume group as the original volume. When these volumes
    are created, they need to be converted into a cache pool which needs to be
    attached to the original volume:
   </p><div class="procedure" id="id-1.11.4.2.12.6.4" data-id-title="Setting up a cached logical volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.2: </span><span class="title-name">Setting up a cached logical volume </span></span><a title="Permalink" class="permalink" href="#id-1.11.4.2.12.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the original volume (on a slow device) if not already existing.
     </p></li><li class="step"><p>
      Add the physical volume (from a fast device) to the same volume group the
      original volume is part of and create the cache data volume on the
      physical volume.
     </p></li><li class="step"><p>
      Create the cache metadata volume. The size should be 1/1000 of the size
      of the cache data volume, with a minimum size of 8 MB.
     </p></li><li class="step"><p>
      Combine the cache data volume and metadata volume into a cache pool
      volume:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --type cache-pool --poolmetadata <em class="replaceable">VOLUME_GROUP/METADATA_VOLUME</em> <em class="replaceable">VOLUME_GROUP/CACHING_VOLUME</em></pre></div></li><li class="step"><p>
      Attach the cache pool to the original volume:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --type cache --cachepool <em class="replaceable">VOLUME_GROUP/CACHING_VOLUME</em> <em class="replaceable">VOLUME_GROUP/ORIGINAL_VOLUME</em></pre></div></li></ol></div></div><p>
    For more information on LVM caching, see the lvmcache(7) man page.
   </p></section></section><section class="sect1" id="sec-lvm-tagging" data-id-title="Tagging LVM2 storage objects"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.10 </span><span class="title-name">Tagging LVM2 storage objects</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A tag is an unordered keyword or term assigned to the metadata of a storage
   object. Tagging allows you to classify collections of LVM storage objects in
   ways that you find useful by attaching an unordered list of tags to their
   metadata.
  </p><section class="sect2" id="sec-lvm-tagging-using" data-id-title="Using LVM2 tags"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.10.1 </span><span class="title-name">Using LVM2 tags</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-using">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After you tag the LVM2 storage objects, you can use the tags in commands to
    accomplish the following tasks:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Select LVM objects for processing according to the presence or absence of
      specific tags.
     </p></li><li class="listitem"><p>
      Use tags in the configuration file to control which volume groups and
      logical volumes are activated on a server.
     </p></li><li class="listitem"><p>
      Override settings in a global configuration file by specifying tags in
      the command.
     </p></li></ul></div><p>
    A tag can be used in place of any command line LVM object reference that
    accepts:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      a list of objects
     </p></li><li class="listitem"><p>
      a single object as long as the tag expands to a single object
     </p></li></ul></div><p>
    Replacing the object name with a tag is not supported everywhere yet. After
    the arguments are expanded, duplicate arguments in a list are resolved by
    removing the duplicate arguments, and retaining the first instance of each
    argument.
   </p><p>
    Wherever there might be ambiguity of argument type, you must prefix a tag
    with the commercial at sign (@) character, such as
    <code class="literal">@mytag</code>. Elsewhere, using the <span class="quote">“<span class="quote">@</span>”</span> prefix is
    optional.
   </p></section><section class="sect2" id="sec-lvm-tagging-requirements" data-id-title="Requirements for creating LVM2 tags"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.10.2 </span><span class="title-name">Requirements for creating LVM2 tags</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-requirements">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Consider the following requirements when using tags with LVM:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.4.2.13.4.3.1"><span class="term">Supported characters</span></dt><dd><p>
       An LVM tag word can contain the ASCII uppercase characters A to Z,
       lowercase characters a to z, numbers 0 to 9, underscore (_), plus (+),
       hyphen (-), and period (.). The word cannot begin with a hyphen. The
       maximum length is 128 characters.
      </p></dd><dt id="id-1.11.4.2.13.4.3.2"><span class="term">Supported storage objects</span></dt><dd><p>
       You can tag LVM2 physical volumes, volume groups, logical volumes, and
       logical volume segments. PV tags are stored in its volume group’s
       metadata. Deleting a volume group also deletes the tags in the orphaned
       physical volume. Snapshots cannot be tagged, but their origin can be
       tagged.
      </p><p>
       LVM1 objects cannot be tagged because the disk format does not support
       it.
      </p></dd></dl></div></section><section class="sect2" id="sec-lvm-tagging-syntax" data-id-title="Command line tag syntax"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.10.3 </span><span class="title-name">Command line tag syntax</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-syntax">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.4.2.13.5.2.1"><span class="term"><code class="option">--addtag</code><em class="replaceable">TAG_INFO</em></span></dt><dd><p>
       Add a tag to (or <span class="emphasis"><em>tag</em></span>) an LVM2 storage object.
       Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange --addtag @db1 vg1</pre></div></dd><dt id="id-1.11.4.2.13.5.2.2"><span class="term"><code class="option">--deltag</code><em class="replaceable">TAG_INFO</em></span></dt><dd><p>
       Remove a tag from (or <span class="emphasis"><em>untag</em></span>) an LVM2 storage
       object. Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange --deltag @db1 vg1</pre></div></dd><dt id="id-1.11.4.2.13.5.2.3"><span class="term"><code class="option">--tag</code><em class="replaceable">TAG_INFO</em></span></dt><dd><p>
       Specify the tag to use to narrow the list of volume groups or logical
       volumes to be activated or deactivated.
      </p><p>
       Enter the following to activate the volume if it has a tag that matches
       the tag provided (example):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvchange -ay --tag @db1 vg1/vol2</pre></div></dd></dl></div></section><section class="sect2" id="sec-lvm-tagging-config" data-id-title="Configuration file syntax"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.10.4 </span><span class="title-name">Configuration file syntax</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following sections show example configurations for certain use cases.
   </p><section class="sect3" id="sec-lvm-tagging-configuration-hostnames-enable" data-id-title="Enabling host name tags in the lvm.conf file"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.10.4.1 </span><span class="title-name">Enabling host name tags in the <code class="filename">lvm.conf</code> file</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-configuration-hostnames-enable">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Add the following code to the <code class="filename">/etc/lvm/lvm.conf</code> file
     to enable host tags that are defined separately on host in a
     <code class="filename">/etc/lvm/lvm_&lt;<em class="replaceable">HOSTNAME</em>&gt;.conf</code>
     file.
    </p><div class="verbatim-wrap"><pre class="screen">tags {
   # Enable hostname tags
   hosttags = 1
}</pre></div><p>
     You place the activation code in the
     <code class="filename">/etc/lvm/lvm_&lt;<em class="replaceable">HOSTNAME</em>&gt;.conf</code>
     file on the host. See
     <a class="xref" href="#sec-lvm-tagging-configuration-activate" title="5.10.4.3. Defining activation">Section 5.10.4.3, “Defining activation”</a>.
    </p></section><section class="sect3" id="sec-lvm-tagging-configuration-hostnames-define" data-id-title="Defining tags for host names in the lvm.conf file"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.10.4.2 </span><span class="title-name">Defining tags for host names in the lvm.conf file</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-configuration-hostnames-define">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">tags {

   tag1 { }
      # Tag does not require a match to be set.

   tag2 {
      # If no exact match, tag is not set.
      host_list = [ "hostname1", "hostname2" ]
   }
}</pre></div></section><section class="sect3" id="sec-lvm-tagging-configuration-activate" data-id-title="Defining activation"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.10.4.3 </span><span class="title-name">Defining activation</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-configuration-activate">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You can modify the <code class="filename">/etc/lvm/lvm.conf</code> file to activate
     LVM logical volumes based on tags.
    </p><p>
     In a text editor, add the following code to the file:
    </p><div class="verbatim-wrap"><pre class="screen">  activation {
      volume_list = [ "vg1/lvol0", "@database" ]
  }</pre></div><p>
     Replace <code class="literal">@database</code> with your tag. Use
     <code class="literal">"@*"</code> to match the tag against any tag set on the host.
    </p><p>
     The activation command matches against <em class="replaceable">VGNAME</em>,
     <em class="replaceable">VGNAME/LVNAME</em>, or
     @<em class="replaceable">TAG</em> set in the metadata of volume groups and
     logical volumes. A volume group or logical volume is activated only if a
     metadata tag matches. The default if there is no match, is not to
     activate.
    </p><p>
     If <code class="literal">volume_list</code> is not present and tags are defined on
     the host, then it activates the volume group or logical volumes only if a
     host tag matches a metadata tag.
    </p><p>
     If <code class="literal">volume_list</code> is defined, but empty, and no tags are
     defined on the host, then it does not activate.
    </p><p>
     If volume_list is undefined, it imposes no limits on LV activation (all
     are allowed).
    </p></section><section class="sect3" id="sec-lvm-tagging-configuration-activate-multi" data-id-title="Defining activation in multiple host name configuration files"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.10.4.4 </span><span class="title-name">Defining activation in multiple host name configuration files</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-configuration-activate-multi">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You can use the activation code in a host’s configuration file
     (<code class="filename">/etc/lvm/lvm_&lt;<em class="replaceable">HOST_TAG</em>&gt;.conf</code>)
     when host tags are enabled in the <code class="filename">lvm.conf</code> file. For
     example, a server has two configuration files in the
     <code class="filename">/etc/lvm/</code> directory:
    </p><table style="border: 0; " class="simplelist"><tr><td><code class="filename">lvm.conf</code></td></tr><tr><td><code class="filename">lvm_&lt;<em class="replaceable">HOST_TAG</em>&gt;.conf</code></td></tr></table><p>
     At start-up, load the <code class="filename">/etc/lvm/lvm.conf</code> file, and
     process any tag settings in the file. If any host tags were defined, it
     loads the related
     <code class="filename">/etc/lvm/lvm_&lt;<em class="replaceable">HOST_TAG</em>&gt;.conf</code>
     file. When it searches for a specific configuration file entry, it
     searches the host tag file first, then the <code class="filename">lvm.conf
     </code>file, and stops at the first match. Within the
     <code class="filename">lvm_&lt;<em class="replaceable">HOST_TAG</em>&gt;.conf</code>
     file, use the reverse order that tags were set in. This allows the file
     for the last tag set to be searched first. New tags set in the host tag
     file will trigger additional configuration file loads.
    </p></section></section><section class="sect2" id="sec-lvm-tagging-activate-cluster" data-id-title="Using tags for a simple activation control in a cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.10.5 </span><span class="title-name">Using tags for a simple activation control in a cluster</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-activate-cluster">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can set up a simple host name activation control by enabling the
    <code class="literal">hostname_tags</code> option in the
    <code class="filename">/etc/lvm/lvm.conf</code> file. Use the same file on every
    machine in a cluster so that it is a global setting.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      In a text editor, add the following code to the
      <code class="filename">/etc/lvm/lvm.conf</code> file:
     </p><div class="verbatim-wrap"><pre class="screen">tags {
   hostname_tags = 1
}</pre></div></li><li class="step"><p>
      Replicate the file to all hosts in the cluster.
     </p></li><li class="step"><p>
      From any machine in the cluster, add <code class="literal">db1</code> to the list
      of machines that activate <code class="filename">vg1/lvol2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvchange --addtag @db1 vg1/lvol2</pre></div></li><li class="step"><p>
      On the <code class="filename">db1</code> server, enter the following to activate
      it:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvchange -ay vg1/vol2</pre></div></li></ol></div></div></section><section class="sect2" id="sec-lvm-tagging-activate-cluster-preferred" data-id-title="Using tags to activate on preferred hosts in a cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.10.6 </span><span class="title-name">Using tags to activate on preferred hosts in a cluster</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-activate-cluster-preferred">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The examples in this section demonstrate two methods to accomplish the
    following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Activate volume group <code class="filename">vg1</code> only on the database hosts
      <code class="filename">db1</code> and <code class="filename">db2</code>.
     </p></li><li class="listitem"><p>
      Activate volume group <code class="filename">vg2</code> only on the file server
      host <code class="filename">fs1</code>.
     </p></li><li class="listitem"><p>
      Activate nothing initially on the file server backup host
      <code class="filename">fsb1</code>, but be prepared for it to take over from the
      file server host <code class="filename">fs1</code>.
     </p></li></ul></div><section class="sect3" id="sec-lvm-tagging-activate-cluster-preferred-opt1" data-id-title="Option 1: centralized admin and static configuration replicated between hosts"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.10.6.1 </span><span class="title-name">Option 1: centralized admin and static configuration replicated between hosts</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-activate-cluster-preferred-opt1">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     In the following solution, the single configuration file is replicated
     among multiple hosts.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Add the <code class="literal">@database</code> tag to the metadata of volume group
       <code class="filename">vg1</code>. In a terminal, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange --addtag @database vg1</pre></div></li><li class="step"><p>
       Add the <code class="literal">@fileserver</code> tag to the metadata of volume
       group <code class="filename">vg2</code>. In a terminal, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange --addtag @fileserver vg2</pre></div></li><li class="step"><p>
       In a text editor, modify the<code class="filename"> /etc/lvm/lvm.conf</code> file
       with the following code to define the <code class="literal">@database</code>,
       <code class="literal">@fileserver</code>, <code class="literal">@fileserverbackup</code>
       tags.
      </p><div class="verbatim-wrap"><pre class="screen">tags {
   database {
      host_list = [ "db1", "db2" ]
   }
   fileserver {
      host_list = [ "fs1" ]
   }
   fileserverbackup {
      host_list = [ "fsb1" ]
   }
}

activation {
   # Activate only if host has a tag that matches a metadata tag
   volume_list = [ "@*" ]
}</pre></div></li><li class="step"><p>
       Replicate the modified<code class="filename"> /etc/lvm/lvm.conf</code> file to
       the four hosts: <code class="filename">db1</code>, <code class="filename">db2</code>,
       <code class="filename">fs1</code>, and <code class="filename">fsb1</code>.
      </p></li><li class="step"><p>
       If the file server host goes down, <code class="filename">vg2</code> can be
       brought up on <code class="filename">fsb1</code> by entering the following
       commands in a terminal on any node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange --addtag @fileserverbackup vg2
<code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange -ay vg2</pre></div></li></ol></div></div></section><section class="sect3" id="sec-lvm-tagging-activate-cluster-preferred-opt2" data-id-title="Option 2: localized admin and configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.10.6.2 </span><span class="title-name">Option 2: localized admin and configuration</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-tagging-activate-cluster-preferred-opt2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     In the following solution, each host holds locally the information about
     which classes of volume to activate.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Add the <code class="literal">@database</code> tag to the metadata of volume group
       <code class="filename">vg1</code>. In a terminal, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange --addtag @database vg1</pre></div></li><li class="step"><p>
       Add the <code class="literal">@fileserver</code> tag to the metadata of volume
       group <code class="filename">vg2</code>. In a terminal, enter
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange --addtag @fileserver vg2</pre></div></li><li class="step"><p>
       Enable host tags in the <code class="filename">/etc/lvm/lvm.conf</code> file:
      </p><ol type="a" class="substeps"><li class="step"><p>
         In a text editor, modify the <code class="filename">/etc/lvm/lvm.conf</code>
         file with the following code to enable host tag configuration files.
        </p><div class="verbatim-wrap"><pre class="screen">tags {
   hosttags = 1
}</pre></div></li><li class="step"><p>
         Replicate the modified <code class="filename">/etc/lvm/lvm.conf</code> file to
         the four hosts: <code class="filename">db1</code>, <code class="filename">db2</code>,
         <code class="filename">fs1</code>, and <code class="filename">fsb1</code>.
        </p></li></ol></li><li class="step"><p>
       On host <code class="filename">db1</code>, create an activation configuration
       file for the database host <code class="filename">db1</code>. In a text editor,
       create <code class="filename">/etc/lvm/lvm_db1.conf</code> file and add the
       following code:
      </p><div class="verbatim-wrap"><pre class="screen">activation {
   volume_list = [ "@database" ]
}</pre></div></li><li class="step"><p>
       On host <code class="filename">db2</code>, create an activation configuration
       file for the database host <code class="filename">db2</code>. In a text editor,
       create <code class="filename">/etc/lvm/lvm_db2.conf</code> file and add the
       following code:
      </p><div class="verbatim-wrap"><pre class="screen">activation {
   volume_list = [ "@database" ]
}</pre></div></li><li class="step"><p>
       On host fs1, create an activation configuration file for the file server
       host <code class="filename">fs1</code>. In a text editor, create
       <code class="filename">/etc/lvm/lvm_fs1.conf</code> file and add the following
       code:
      </p><div class="verbatim-wrap"><pre class="screen">activation {
   volume_list = [ "@fileserver" ]
}</pre></div></li><li class="step"><p>
       If the file server host <code class="filename">fs1</code> goes down, to bring up
       a spare file server host fsb1 as a file server:
      </p><ol type="a" class="substeps"><li class="step"><p>
         On host <code class="filename">fsb1</code>, create an activation configuration
         file for the host <code class="filename">fsb1</code>. In a text editor, create
         <code class="filename">/etc/lvm/lvm_fsb1.conf</code> file and add the following
         code:
        </p><div class="verbatim-wrap"><pre class="screen">activation {
   volume_list = [ "@fileserver" ]
}</pre></div></li><li class="step"><p>
         In a terminal, enter one of the following commands:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange -ay vg2
<code class="prompt user">&gt; </code><code class="command">sudo</code> vgchange -ay @fileserver</pre></div></li></ol></li></ol></div></div></section></section></section></section><section xml:lang="en" class="chapter" id="cha-lvm-snapshots" data-id-title="LVM volume snapshots"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">LVM volume snapshots</span></span> <a title="Permalink" class="permalink" href="#cha-lvm-snapshots">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm-snapshots.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  A Logical Volume Manager (LVM) logical volume snapshot is a copy-on-write
  technology that monitors changes to an existing volume’s data blocks so
  that when a write is made to one of the blocks, the block’s value at the
  snapshot time is copied to a snapshot volume. In this way, a point-in-time
  copy of the data is preserved until the snapshot volume is deleted.
 </p><section class="sect1" id="sec-lvm-snapshots-intro" data-id-title="Understanding volume snapshots"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Understanding volume snapshots</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-snapshots-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm-snapshots.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A file system snapshot contains metadata about itself and data blocks from a
   source logical volume that has changed since the snapshot was taken. When
   you access data via the snapshot, you see a point-in-time copy of the source
   logical volume. There is no need to restore data from backup media or to
   overwrite the changed data.
  </p><div id="id-1.11.4.3.4.3" data-id-title="Mounting volumes with snapshots" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Mounting volumes with snapshots</div><p>
    During the snapshot’s lifetime, the snapshot must be mounted before its
    source logical volume can be mounted.
   </p></div><p>
   LVM volume snapshots allow you to create a backup from a point-in-time view
   of the file system. The snapshot is created instantly and persists until you
   delete it. You can back up the file system from the snapshot while the
   volume itself continues to be available for users. The snapshot initially
   contains some metadata about the snapshot, but no actual data from the
   source logical volume. Snapshot uses copy-on-write technology to detect when
   data changes in an original data block. It copies the value it held when the
   snapshot was taken to a block in the snapshot volume, then allows the new
   data to be stored in the source block. As more blocks change from their
   original value on the source logical volume, the snapshot size grows.
  </p><p>
   When you are sizing the snapshot, consider how much data is expected to
   change on the source logical volume and how long you plan to keep the
   snapshot. The amount of space that you allocate for a snapshot volume can
   vary, depending on the size of the source logical volume, how long you plan
   to keep the snapshot, and the number of data blocks that are expected to
   change during the snapshot’s lifetime. The snapshot volume cannot be
   resized after it is created. As a guide, create a snapshot volume that is
   about 10% of the size of the original logical volume. If you anticipate that
   every block in the source logical volume will change at least one time
   before you delete the snapshot, then the snapshot volume should be at least
   as large as the source logical volume plus some additional space for
   metadata about the snapshot volume. Less space is required if the data
   changes infrequently or if the expected lifetime is sufficiently brief.
  </p><p>
   In LVM2, snapshots are read/write by default. When you write data directly
   to the snapshot, that block is marked in the exception table as used, and
   never gets copied from the source logical volume. You can mount the snapshot
   volume, and test application changes by writing data directly to the
   snapshot volume. You can easily discard the changes by dismounting the
   snapshot, removing the snapshot, and then remounting the source logical
   volume.
  </p><p>
   In a virtual guest environment, you can use the snapshot function for LVM
   logical volumes you create on the server’s disks, as you would on a
   physical server.
  </p><p>
   In a virtual host environment, you can use the snapshot function to back up
   the virtual machine’s storage back-end, or to test changes to a virtual
   machine image, such as for patches or upgrades, without modifying the source
   logical volume. The virtual machine must be using an LVM logical volume as
   its storage back-end, as opposed to using a virtual disk file. You can mount
   the LVM logical volume and use it to store the virtual machine image as a
   file-backed disk, or you can assign the LVM logical volume as a physical
   disk to write to it as a block device.
  </p><p>
   An LVM logical volume snapshot can be thinly provisioned. Thin provisioning is assumed if you
   create a snapshot without a specified size. The snapshot is created as a thin volume that uses
   space as needed from a thin pool. A thin snapshot volume has the same characteristics as any
   other thin volume. You can independently activate the volume, extend the volume, rename the
   volume, remove the volume, and even snapshot the volume.
  </p><div id="id-1.11.4.3.4.10" data-id-title="Thinly provisioned volumes in a cluster" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Thinly provisioned volumes in a cluster</div><p>
    To use thinly provisioned snapshots in a cluster, the source logical volume
    and its snapshots must be managed in a single cluster resource. This allows
    the volume and its snapshots to always be mounted exclusively on the same
    node.
   </p></div><p>
   When you are done with the snapshot, it is important to remove it from the
   system. A snapshot eventually fills up completely as data blocks change on
   the source logical volume. When the snapshot is full, it is disabled, which
   prevents you from remounting the source logical volume.
  </p><p>
   If you create multiple snapshots for a source logical volume, remove the
   snapshots in a last created, first deleted order.
  </p></section><section class="sect1" id="sec-lvm-snapshots-create" data-id-title="Creating Linux snapshots with LVM"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Creating Linux snapshots with LVM</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-snapshots-create">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm-snapshots.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Logical Volume Manager (LVM) can be used for creating snapshots of your
   file system.
  </p><p>
   Open a terminal and enter
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -s [-L <em class="replaceable">&lt;size</em>&gt;] -n <em class="replaceable">SNAP_VOLUME</em> <em class="replaceable">SOURCE_VOLUME_PATH</em></pre></div><p>
   If no size is specified, the snapshot is created as a thin snapshot.
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -s -L 1G -n linux01-snap /dev/lvm/linux01</pre></div><p>
   The snapshot is created as the <code class="filename">/dev/lvm/linux01-snap</code>
   volume.
  </p></section><section class="sect1" id="sec-lvm-snapshots-monitor" data-id-title="Monitoring a snapshot"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">Monitoring a snapshot</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-snapshots-monitor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm-snapshots.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Open a terminal and enter
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvdisplay <em class="replaceable">SNAP_VOLUME</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvdisplay /dev/vg01/linux01-snap

--- Logical volume ---
  LV Name                /dev/lvm/linux01
  VG Name                vg01
  LV UUID                QHVJYh-PR3s-A4SG-s4Aa-MyWN-Ra7a-HL47KL
  LV Write Access        read/write
  LV snapshot status     active destination for /dev/lvm/linux01
  LV Status              available
  # open                 0
  LV Size                80.00 GB
  Current LE             1024
  COW-table size         8.00 GB
  COW-table LE           512
  Allocated to snapshot  30%
  Snapshot chunk size    8.00 KB
  Segments               1
  Allocation             inherit
  Read ahead sectors     0
  Block device           254:5</pre></div></section><section class="sect1" id="sec-lvm-snapshots-delete" data-id-title="Deleting Linux snapshots"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Deleting Linux snapshots</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-snapshots-delete">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm-snapshots.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Open a terminal and enter
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvremove <em class="replaceable">SNAP_VOLUME_PATH</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvremove /dev/lvmvg/linux01-snap</pre></div></section><section class="sect1" id="sec-lvm-snapshots-xen-host" data-id-title="Using snapshots for virtual machines on a virtual host"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.5 </span><span class="title-name">Using snapshots for virtual machines on a virtual host</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-snapshots-xen-host">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm-snapshots.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Using an LVM logical volume for a virtual machine’s back-end storage
   allows flexibility in administering the underlying device, such as making it
   easier to move storage objects, create snapshots, and back up data. You can
   mount the LVM logical volume and use it to store the virtual machine image
   as a file-backed disk, or you can assign the LVM logical volume as a
   physical disk to write to it as a block device. You can create a virtual
   disk image on the LVM logical volume, then snapshot the LVM.
  </p><p>
   You can leverage the read/write capability of the snapshot to create
   different instances of a virtual machine, where the changes are made to the
   snapshot for a particular virtual machine instance. You can create a virtual
   disk image on an LVM logical volume, snapshot the source logical volume, and
   modify the snapshot for a particular virtual machine instance. You can
   create another snapshot of the source logical volume, and modify it for a
   different virtual machine instance. The majority of the data for the
   different virtual machine instances resides with the image on the source
   logical volume.
  </p><p>
   You can also leverage the read/write capability of the snapshot to preserve
   the virtual disk image while testing patches or upgrades in the guest
   environment. You create a snapshot of the LVM volume that contains the
   image, and then run the virtual machine on the snapshot location. The source
   logical volume is unchanged, and all changes for that machine are written to
   the snapshot. To return to the source logical volume of the virtual machine
   image, you power off the virtual machine, then remove the snapshot from the
   source logical volume. To start over, you re-create the snapshot, mount the
   snapshot, and restart the virtual machine on the snapshot image.
  </p><p>
   The following procedure uses a file-backed virtual disk image and the Xen
   hypervisor. You can adapt the procedure in this section for other
   hypervisors that run on the SUSE Linux Enterprise platform, such as KVM. To run a
   file-backed virtual machine image from the snapshot volume:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Ensure that the source logical volume that contains the file-backed
     virtual machine image is mounted, such as at mount point
     <code class="filename">/var/lib/xen/images/&lt;<em class="replaceable">IMAGE_NAME</em>&gt;</code>.
    </p></li><li class="step"><p>
     Create a snapshot of the LVM logical volume with enough space to store the
     differences that you expect.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvcreate -s -L 20G -n myvm-snap /dev/lvmvg/myvm</pre></div><p>
     If no size is specified, the snapshot is created as a thin snapshot.
    </p></li><li class="step"><p>
     Create a mount point where you will mount the snapshot volume.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mkdir -p /mnt/xen/vm/myvm-snap</pre></div></li><li class="step"><p>
     Mount the snapshot volume at the mount point you created.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount -t auto /dev/lvmvg/myvm-snap /mnt/xen/vm/myvm-snap</pre></div></li><li class="step"><p>
     In a text editor, copy the configuration file for the source virtual
     machine, modify the paths to point to the file-backed image file on the
     mounted snapshot volume, and save the file such as
     <code class="filename">/etc/xen/myvm-snap.cfg</code>.
    </p></li><li class="step"><p>
     Start the virtual machine using the mounted snapshot volume of the virtual
     machine.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> xm create -c /etc/xen/myvm-snap.cfg</pre></div></li><li class="step"><p>
     (Optional) Remove the snapshot, and use the unchanged virtual machine
     image on the source logical volume.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> umount /mnt/xenvms/myvm-snap
<code class="prompt user">&gt; </code><code class="command">sudo</code> lvremove -f /dev/lvmvg/mylvm-snap</pre></div></li><li class="step"><p>
     (Optional) Repeat this process as desired.
    </p></li></ol></div></div></section><section class="sect1" id="sec-lvm-snapshots-rollback" data-id-title="Merging a snapshot with the source logical volume to revert changes or roll back to a previous state"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.6 </span><span class="title-name">Merging a snapshot with the source logical volume to revert changes or roll back to a previous state</span></span> <a title="Permalink" class="permalink" href="#sec-lvm-snapshots-rollback">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_lvm-snapshots.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Snapshots can be useful if you need to roll back or restore data on a volume
   to a previous state. For example, you might need to revert data changes that
   resulted from an administrator error or a failed or undesirable package
   installation or upgrade.
  </p><p>
   You can use the <code class="command">lvconvert --merge</code> command to revert the
   changes made to an LVM logical volume. The merging begins as follows:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If both the source logical volume and snapshot volume are not open, the
     merge begins immediately.
    </p></li><li class="listitem"><p>
     If the source logical volume or snapshot volume are open, the merge starts
     the first time either the source logical volume or snapshot volume are
     activated and both are closed.
    </p></li><li class="listitem"><p>
     If the source logical volume cannot be closed, such as the root file
     system, the merge is deferred until the next time the server reboots and
     the source logical volume is activated.
    </p></li><li class="listitem"><p>
     If the source logical volume contains a virtual machine image, you must
     shut down the virtual machine, deactivate the source logical volume and
     snapshot volume (by dismounting them in that order), and then issue the
     merge command. Because the source logical volume is automatically
     remounted and the snapshot volume is deleted when the merge is complete,
     you should not restart the virtual machine until after the merge is
     complete. After the merge is complete, you use the resulting logical
     volume for the virtual machine.
    </p></li></ul></div><p>
   After a merge begins, the merge continues automatically after server
   restarts until it is complete. A new snapshot cannot be created for the
   source logical volume while a merge is in progress.
  </p><p>
   While the merge is in progress, reads or writes to the source logical volume
   are transparently redirected to the snapshot that is being merged. This
   allows users to immediately view and access the data as it was when the
   snapshot was created. They do not need to wait for the merge to complete.
  </p><p>
   When the merge is complete, the source logical volume contains the same data
   as it did when the snapshot was taken, plus any data changes made after the
   merge began. The resulting logical volume has the source logical volume’s
   name, minor number, and UUID. The source logical volume is automatically
   remounted, and the snapshot volume is removed.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal and enter
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --merge  [-b] [-i <em class="replaceable">SECONDS</em>] [<em class="replaceable">SNAP_VOLUME_PATH</em>[...<em class="replaceable">snapN</em>]|@<em class="replaceable">VOLUME_TAG</em>]</pre></div><p>
     You can specify one or multiple snapshots on the command line. You can
     alternatively tag multiple source logical volumes with the same volume tag
     then specify
     <code class="literal">@&lt;<em class="replaceable">VOLUME_TAG</em>&gt;</code> on the
     command line. The snapshots for the tagged volumes are merged to their
     respective source logical volumes. For information about tagging logical
     volumes, see
     <a class="xref" href="#sec-lvm-tagging" title="5.10. Tagging LVM2 storage objects">Section 5.10, “Tagging LVM2 storage objects”</a>.
    </p><p>
     The options include:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.4.3.9.8.1.5.1"><span class="term">-b, </span><span class="term">--background</span></dt><dd><p>
        Run the daemon in the background. This allows multiple specified
        snapshots to be merged concurrently in parallel.
       </p></dd><dt id="id-1.11.4.3.9.8.1.5.2"><span class="term">-i, </span><span class="term">--interval &lt;<em class="replaceable">SECONDS</em>&gt;</span></dt><dd><p>
        Report progress as a percentage at regular intervals. Specify the
        interval in seconds.
       </p></dd></dl></div><p>
     For more information about this command, see the
     <code class="command">lvconvert(8)</code> man page.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --merge /dev/lvmvg/linux01-snap</pre></div><p>
     This command merges <code class="filename">/dev/lvmvg/linux01-snap</code> into its
     source logical volume.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lvconvert --merge @mytag</pre></div><p>
     If <code class="filename">lvol1</code>, <code class="filename">lvol2</code>, and
     <code class="filename">lvol3</code> are all tagged with <code class="literal">mytag</code>,
     each snapshot volume is merged serially with its respective source logical
     volume; that is: <code class="filename">lvol1</code>, then
     <code class="filename">lvol2</code>, then <code class="filename">lvol3</code>. If the
     <code class="literal">--background</code> option is specified, the snapshots for the
     respective tagged logical volume are merged concurrently in parallel.
    </p></li><li class="step"><p>
     (Optional) If both the source logical volume and snapshot volume are open
     and they can be closed, you can manually deactivate and activate the
     source logical volume to get the merge to start immediately.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> umount <em class="replaceable">ORIGINAL_VOLUME</em>
<code class="prompt user">&gt; </code><code class="command">sudo</code> lvchange -an <em class="replaceable">ORIGINAL_VOLUME</em>
<code class="prompt user">&gt; </code><code class="command">sudo</code> lvchange -ay <em class="replaceable">ORIGINAL_VOLUME</em>
<code class="prompt user">&gt; </code><code class="command">sudo</code> mount <em class="replaceable">ORIGINAL_VOLUME</em> <em class="replaceable">MOUNT_POINT</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> umount /dev/lvmvg/lvol01
<code class="prompt user">&gt; </code><code class="command">sudo</code> lvchange -an /dev/lvmvg/lvol01
<code class="prompt user">&gt; </code><code class="command">sudo</code> lvchange -ay /dev/lvmvg/lvol01
<code class="prompt user">&gt; </code><code class="command">sudo</code> mount /dev/lvmvg/lvol01 /mnt/lvol01</pre></div></li><li class="step"><p>
     (Optional) If both the source logical volume and snapshot volume are open
     and the source logical volume cannot be closed, such as the <code class="systemitem">root</code>
     file system, you can restart the server and mount the source logical
     volume to get the merge to start immediately after the restart.
    </p></li></ol></div></div></section></section></div><div class="part" id="part-software-raid" data-id-title="Software RAID"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part III </span><span class="title-name">Software RAID </span></span><a title="Permalink" class="permalink" href="#part-software-raid">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/book_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-raid"><span class="title-number">7 </span><span class="title-name">Software RAID configuration</span></a></span></li><dd class="toc-abstract"><p>The purpose of RAID (redundant array of independent disks) is to combine several hard disk partitions into one large virtual hard disk to optimize performance, data security, or both. Most RAID controllers use the SCSI protocol, because it can address a larger number of hard disks in a more effectiv…</p></dd><li><span class="chapter"><a href="#cha-raidroot"><span class="title-number">8 </span><span class="title-name">Configuring software RAID for the root partition</span></a></span></li><dd class="toc-abstract"><p>In SUSE Linux Enterprise Server, the Device Mapper RAID tool has been integrated into the YaST Partitioner. You can use the partitioner at install time to create a software RAID for the system device that contains your root (/) partition. The /boot partition cannot be stored on a RAID partition unle…</p></dd><li><span class="chapter"><a href="#cha-raid10"><span class="title-number">9 </span><span class="title-name">Creating software RAID 10 devices</span></a></span></li><dd class="toc-abstract"><p>This section describes how to set up nested and complex RAID 10 devices. A RAID 10 device consists of nested RAID 1 (mirroring) and RAID 0 (striping) arrays. Nested RAIDs can either be set up as striped mirrors (RAID 1+0) or as mirrored stripes (RAID 0+1). A complex RAID 10 setup also combines mirro…</p></dd><li><span class="chapter"><a href="#cha-raid-degraded"><span class="title-number">10 </span><span class="title-name">Creating a degraded RAID array</span></a></span></li><dd class="toc-abstract"><p>
    A degraded array is one in which some devices are missing. Degraded arrays
    are supported only for RAID 1, RAID 4, RAID 5, and RAID 6. These RAID types
    are designed to withstand some missing devices as part of their
    fault-tolerance features. Typically, degraded arrays occur when a device
    fails. It is possible to create a degraded array on purpose.
   </p></dd><li><span class="chapter"><a href="#cha-raid-resize"><span class="title-number">11 </span><span class="title-name">Resizing software RAID arrays with mdadm</span></a></span></li><dd class="toc-abstract"><p>
    This section describes how to increase or reduce the size of a software
    RAID 1, 4, 5, or 6 device with the Multiple Device Administration
    (<code class="command">mdadm(8)</code>) tool.
   </p></dd><li><span class="chapter"><a href="#cha-raid-leds"><span class="title-number">12 </span><span class="title-name">Storage enclosure LED utilities for MD software RAIDs</span></a></span></li><dd class="toc-abstract"><p>
    
    Storage enclosure LED Monitoring utility (<code class="command">ledmon</code>) and
    LED Control (<code class="command">ledctl</code>) utility are Linux user space
    applications that use a broad range of interfaces and protocols to control
    storage enclosure LEDs. The primary usage is to visualize the status of
    Linux MD software RAID devices created with the mdadm utility. The
    <code class="systemitem">ledmon</code> daemon monitors the status
    of the drive array and updates the status of the drive LEDs. The
    <code class="command">ledctl</code> utility allows you to set LED patterns for
    specified devices.
   </p></dd><li><span class="chapter"><a href="#cha-raidtroubleshooting"><span class="title-number">13 </span><span class="title-name">Troubleshooting software RAIDs</span></a></span></li><dd class="toc-abstract"><p>Check the /proc/mdstat file to find out whether a RAID partition has been damaged. If a disk fails, replace the defective hard disk with a new one partitioned the same way. Then restart your system and enter the command mdadm /dev/mdX --add /dev/sdX. Replace X with your particular device identifiers…</p></dd></ul></div><section xml:lang="en" class="chapter" id="cha-raid" data-id-title="Software RAID configuration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Software RAID configuration</span></span> <a title="Permalink" class="permalink" href="#cha-raid">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large virtual hard disk to optimize
  performance, data security, or both. Most RAID controllers use the SCSI
  protocol, because it can address a larger number of hard disks in a more
  effective way than the IDE protocol and is more suitable for parallel
  processing of commands. There are some RAID controllers that support IDE or
  SATA hard disks. Software RAID provides the advantages of RAID systems
  without the additional cost of hardware RAID controllers. However, this
  requires some CPU time and has memory requirements that make it unsuitable
  for real high performance computers.
 </p><div id="id-1.11.5.2.4" data-id-title="RAID on cluster file systems" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: RAID on cluster file systems</div><p>
   Software RAID underneath clustered file systems needs to be set up using a
   cluster multi-device (Cluster MD). Refer to the
   <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/cha-ha-cluster-md.html" target="_blank">
   <em class="citetitle">Administration Guide for SUSE Linux Enterprise High Availability</em></a>.
  </p></div><p>
  SUSE Linux Enterprise offers the option of combining several hard disks into one soft RAID
  system. RAID implies several strategies for combining several hard disks in a
  RAID system, each with different goals, advantages, and characteristics.
  These variations are commonly known as <span class="emphasis"><em>RAID levels</em></span>.
 </p><section class="sect1" id="sec-raid-intro" data-id-title="Understanding RAID levels"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">Understanding RAID levels</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section describes common RAID levels 0, 1, 2, 3, 4, 5, and nested RAID
   levels.
  </p><section class="sect2" id="sec-raid-intro-raid0" data-id-title="RAID 0"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.1 </span><span class="title-name">RAID 0</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro-raid0">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This level improves the performance of your data access by spreading out
    blocks of each file across multiple disks. Actually, this is not a RAID,
    because it does not provide data backup, but the name
    <span class="emphasis"><em>RAID 0</em></span> for this type of system has become the
    norm. With RAID 0, two or more hard disks are pooled together. The
    performance is very good, but the RAID system is destroyed and your data
    lost if even one hard disk fails.
   </p></section><section class="sect2" id="sec-raid-intro-raid1" data-id-title="RAID 1"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.2 </span><span class="title-name">RAID 1</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro-raid1">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This level provides adequate security for your data, because the data is
    copied to another hard disk 1:1. This is known as <span class="emphasis"><em>hard disk
    mirroring</em></span>. If a disk is destroyed, a copy of its contents is
    available on another mirrored disk. All disks except one could be damaged
    without endangering your data. However, if damage is not detected, damaged
    data might be mirrored to the correct disk and the data is corrupted that
    way. The writing performance suffers a little in the copying process
    compared to when using single disk access (10 to 20 percent slower), but
    read access is significantly faster in comparison to any one of the normal
    physical hard disks, because the data is duplicated so can be scanned in
    parallel. RAID 1 generally provides nearly twice the read transaction rate
    of single disks and almost the same write transaction rate as single disks.
   </p></section><section class="sect2" id="sec-raid-intro-raid23" data-id-title="RAID 2 and RAID 3"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.3 </span><span class="title-name">RAID 2 and RAID 3</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro-raid23">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    These are not typical RAID implementations. Level 2 stripes data at
    the bit level rather than the block level. Level 3 provides byte-level
    striping with a dedicated parity disk and cannot service simultaneous
    multiple requests. Both levels are rarely used.
   </p></section><section class="sect2" id="sec-raid-intro-raid4" data-id-title="RAID 4"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.4 </span><span class="title-name">RAID 4</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro-raid4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Level 4 provides block-level striping like Level 0 combined with
    a dedicated parity disk. If a data disk fails, the parity data is used to
    create a replacement disk. However, the parity disk might create a
    bottleneck for write access. Nevertheless, Level 4 is sometimes used.
   </p></section><section class="sect2" id="sec-raid-intro-raid5" data-id-title="RAID 5"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.5 </span><span class="title-name">RAID 5</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro-raid5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    RAID 5 is an optimized compromise between Level 0 and
    Level 1 in terms of performance and redundancy. The hard disk space
    equals the number of disks used minus one. The data is distributed over the
    hard disks as with RAID 0. <span class="emphasis"><em>Parity blocks</em></span>, created
    on one of the partitions, are there for security reasons. They are linked
    to each other with XOR, enabling the contents to be reconstructed by the
    corresponding parity block in case of system failure. With RAID 5, no
    more than one hard disk can fail at the same time. If one hard disk fails,
    it must be replaced when possible to avoid the risk of losing data.
   </p></section><section class="sect2" id="sec-raid-intro-raid6" data-id-title="RAID 6"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.6 </span><span class="title-name">RAID 6</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro-raid6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    RAID 6 is an extension of RAID 5 that allows for additional fault
    tolerance by using a second independent distributed parity scheme (dual
    parity). Even if two of the hard disks fail during the data recovery
    process, the system continues to be operational, with no data loss.
   </p><p>
    RAID 6 provides for extremely high data fault tolerance by sustaining
    multiple simultaneous drive failures. It handles the loss of any two
    devices without data loss. Accordingly, it requires N+2 drives to store N
    drives worth of data. It requires a minimum of four devices.
   </p><p>
    The performance for RAID 6 is slightly lower but comparable to
    RAID 5 in normal mode and single disk failure mode. It is very slow in
    dual disk failure mode. A RAID 6 configuration needs a considerable
    amount of CPU time and memory for write operations.
   </p><div class="table" id="id-1.11.5.2.6.8.5" data-id-title="Comparison of RAID 5 and RAID 6"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 7.1: </span><span class="title-name">Comparison of RAID 5 and RAID 6 </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.2.6.8.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Feature
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         RAID 5
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         RAID 6
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Number of devices
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N+1, minimum of 3
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         N+2, minimum of 4
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Parity
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Distributed, single
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Distributed, dual
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Performance
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Medium impact on write and rebuild
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         More impact on sequential write than RAID 5
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Fault-tolerance
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         Failure of one component device
        </p>
       </td><td>
        <p>
         Failure of two component devices
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-raid-intro-raid-nested" data-id-title="Nested and complex RAID levels"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.7 </span><span class="title-name">Nested and complex RAID levels</span></span> <a title="Permalink" class="permalink" href="#sec-raid-intro-raid-nested">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Other RAID levels have been developed, such as RAIDn, RAID 10,
    RAID 0+1, RAID 30, and RAID 50. Some are proprietary
    implementations created by hardware vendors. Examples for creating
    RAID 10 configurations can be found in <a class="xref" href="#cha-raid10" title="Chapter 9. Creating software RAID 10 devices">Chapter 9, <em>Creating software RAID 10 devices</em></a>.
   </p></section></section><section class="sect1" id="sec-raid-yast" data-id-title="Soft RAID configuration with YaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Soft RAID configuration with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-raid-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The YaST soft RAID configuration can be reached from the YaST Expert
   Partitioner. This partitioning tool also enables you to edit and delete
   existing partitions and create new ones that should be used with soft RAID.
   These instructions apply on setting up RAID levels 0, 1, 5, and 6. Setting
   up RAID 10 configurations is explained in <a class="xref" href="#cha-raid10" title="Chapter 9. Creating software RAID 10 devices">Chapter 9, <em>Creating software RAID 10 devices</em></a>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step"><p>
     If necessary, create partitions that should be used with your RAID
     configuration. Do not format them and set the partition type to
     <span class="guimenu">0xFD Linux RAID</span>. When using existing partitions it is
     not necessary to change their partition type—YaST will
     automatically do so. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “<span class="guimenu">Expert Partitioner</span>”, Section 11.1 “Using the <span class="guimenu">Expert Partitioner</span>”</span> for
     details.
    </p><p>
     It is strongly recommended to use partitions stored on different hard
     disks to decrease the risk of losing data if one is defective (RAID 1
     and 5) and to optimize the performance of RAID 0.
    </p><p>
     For RAID 0 at least two partitions are needed. RAID 1 requires
     exactly two partitions, while at least three partitions are required for
     RAID 5. A RAID 6 setup requires at least four partitions. It is
     recommended to use only partitions of the same size because each segment
     can contribute only the same amount of space as the smallest sized
     partition.
    </p></li><li class="step"><p>
     In the left panel, select <span class="guimenu">RAID</span>.
    </p><p>
     A list of existing RAID configurations opens in the right panel.
    </p></li><li class="step"><p>
     At the lower left of the RAID page, click <span class="guimenu">Add RAID</span>.
    </p></li><li class="step"><p>
     Select a <span class="guimenu">RAID Type</span> and <span class="guimenu">Add</span> an
     appropriate number of partitions from the <span class="guimenu">Available
     Devices</span> dialog.
    </p><p>
     You can optionally assign a <span class="guimenu">RAID Name</span> to your RAID. It
     will make it available as
     <code class="filename">/dev/md/<em class="replaceable">NAME</em></code>. See
     <a class="xref" href="#sec-raid-yast-names" title="7.2.1. RAID names">Section 7.2.1, “RAID names”</a> for more information.
    </p><div class="figure" id="fig-yast2-raid3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_raid3_a.png"><img src="images/yast2_raid3_a.png" width="100%" alt="Example RAID 5 configuration" title="Example RAID 5 configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.1: </span><span class="title-name">Example RAID 5 configuration </span></span><a title="Permalink" class="permalink" href="#fig-yast2-raid3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div><p>
     Proceed with <span class="guimenu">Next</span>.
    </p></li><li class="step"><p>
     Select the <span class="guimenu">Chunk Size</span> and, if applicable, the
     <span class="guimenu">Parity Algorithm</span>. The optimal chunk size depends on the
     type of data and the type of RAID. See
     <a class="link" href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes" target="_blank">https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes</a>
     for more information. More information on parity algorithms can be found
     with <code class="command">man 8 mdadm</code> when searching for the
     <code class="option">--layout</code> option. If unsure, stick with the defaults.
    </p></li><li class="step"><p>
     Choose a <span class="guimenu">Role</span> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <span class="guimenu">Raw Volume
     (Unformatted)</span>.
    </p></li><li class="step"><p>
     Under <span class="guimenu">Formatting Options</span>, select <span class="guimenu">Format
     Partition</span>, then select the <span class="guimenu">File system</span>. The
     content of the <span class="guimenu">Options</span> menu depends on the file system.
     Usually there is no need to change the defaults.
    </p><p>
     Under <span class="guimenu">Mounting Options</span>, select <span class="guimenu">Mount
     partition</span>, then select the mount point. Click <span class="guimenu">Fstab
     Options</span> to add special mounting options for the volume.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Finish</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Next</span>, verify that the changes are listed, then
     click <span class="guimenu">Finish</span>.
    </p></li></ol></div></div><div id="id-1.11.5.2.7.4" data-id-title="RAID on disks" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: RAID on disks</div><p>
    While the partitioner makes it possible to create a RAID on top of disks
    instead of partitions, we do not recommend this approach for a number of
    reasons. Installing a bootloader on such RAID is not supported, so you need
    to use a separate device for booting. Tools like <span class="package">fdisk</span>
    and <span class="package">parted</span> do not work properly with such RAIDs, which
    may lead to incorrect diagnosis and actions by a person who is unaware of
    the RAID's particular setup.
   </p></div><section class="sect2" id="sec-raid-yast-names" data-id-title="RAID names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2.1 </span><span class="title-name">RAID names</span></span> <a title="Permalink" class="permalink" href="#sec-raid-yast-names">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    By default, software RAID devices have numeric names following the pattern
    <code class="literal">mdN</code>, where <code class="literal">N</code> is a number. As such
    they can be accessed as, for example, <code class="filename">/dev/md127</code> and
    are listed as <code class="literal">md127</code> in <code class="filename">/proc/mdstat</code>
    and <code class="filename">/proc/partitions</code>. Working with these names can be
    clumsy. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers two ways to work around this problem:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.5.2.7.5.3.1"><span class="term">Providing a named link to the device</span></dt><dd><p>
       You can optionally specify a name for the RAID device when creating it
       with YaST or on the command line with <code class="command">mdadm --create
       '/dev/md/</code> <em class="replaceable">NAME</em>'. The device name
       will still be <code class="literal">mdN</code>, but a link
       <code class="filename">/dev/md/<em class="replaceable">NAME</em></code> will be
       created:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>ls -og /dev/md
total 0
lrwxrwxrwx 1 8 Dec  9 15:11 myRAID -&gt; ../md127</pre></div><p>
       The device will still be listed as <code class="literal">md127</code> under
       <code class="filename">/proc</code>.
      </p></dd><dt id="id-1.11.5.2.7.5.3.2"><span class="term">Providing a named device</span></dt><dd><p>
       In case a named link to the device is not sufficient for your setup, add
       the line <code class="literal">CREATE names=yes</code> to
       <code class="filename">/etc/mdadm.conf</code> by running the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>echo "CREATE names=yes" | sudo tee -a  /etc/mdadm.conf</pre></div><p>
       This will cause names like <code class="literal">myRAID</code> to be used as a
       <span class="quote">“<span class="quote">real</span>”</span> device name. The device will not only be accessible
       at <code class="filename">/dev/myRAID</code>, but also be listed as
       <code class="literal">myRAID</code> under <code class="filename">/proc</code>. Note that
       this will only apply to RAIDs configured after the change to the
       configuration file. Active RAIDs will continue to use the
       <code class="literal">mdN</code> names until they get stopped and re-assembled.
      </p><div id="id-1.11.5.2.7.5.3.2.2.4" data-id-title="Incompatible tools" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Incompatible tools</div><p>
        Not all tools may support named RAID devices. In case a tool expects a
        RAID device to be named <code class="literal">mdN</code>, it will fail to
        identify the devices.
       </p></div></dd></dl></div></section></section><section class="sect1" id="sec-arm-raid" data-id-title="Configuring stripe size on RAID 5 on AArch64"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Configuring stripe size on RAID 5 on AArch64</span></span> <a title="Permalink" class="permalink" href="#sec-arm-raid">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   By default, the stripe size is set to 4kB. If you need to change the default
   stripe size, for example, to match the typical page size of 64kB on
   AArch64, you can configure the stripe size manually using CLI:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> echo 16384  &gt; /sys/block/md1/md/stripe_size</pre></div><p>
   The above command sets the stripe size to 16kB. You can set other values
   such as 4096, 8192; but the value must be a power of 2.
  </p></section><section class="sect1" id="sec-raid-counters" data-id-title="Monitoring software RAIDs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">Monitoring software RAIDs</span></span> <a title="Permalink" class="permalink" href="#sec-raid-counters">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can run <code class="command">mdadm</code> as a daemon in the
   <code class="literal">monitor</code> mode to monitor your software RAID. In the
   <code class="literal">monitor</code> mode, <code class="command">mdadm</code> performs regular
   checks on the array for disk failures. If there is a failure,
   <code class="command">mdadm</code> sends an email to the administrator. To define the
   time interval of the checks, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen">mdadm --monitor --mail=root@localhost --delay=1800 /dev/md2</pre></div><p>
   The command above turns on monitoring of the <code class="literal">/dev/md2</code>
   array in intervals of 1800 seconds. In the event of a failure, an email
   will be sent to <code class="literal">root@localhost</code>.
  </p><div id="id-1.11.5.2.9.5" data-id-title="RAID checks are enabled by default" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: RAID checks are enabled by default</div><p>
    The RAID checks are enabled by default. It may happen that the interval
    between each check is not long enough and you may encounter warnings. Thus,
    you can increase the interval by setting a higher value with the
    <code class="literal">delay</code> option.
   </p></div></section><section class="sect1" id="sec-raid-more" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-raid-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Configuration instructions and more details for soft RAID can be found in
   the HOWTOs at:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <em class="citetitle">The Linux RAID wiki</em>:
     <a class="link" href="https://raid.wiki.kernel.org/" target="_blank">https://raid.wiki.kernel.org/</a>
    </p></li><li class="listitem"><p>
     <em class="citetitle">The Software RAID HOWTO</em> in the
     <code class="filename">/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</code>
     file
    </p></li></ul></div><p>
   Linux RAID mailing lists are also available, such as
   <em class="citetitle">linux-raid</em> at
   <a class="link" href="https://marc.info/?l=linux-raid" target="_blank">https://marc.info/?l=linux-raid</a>.
  </p></section></section><section xml:lang="en" class="chapter" id="cha-raidroot" data-id-title="Configuring software RAID for the root partition"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Configuring software RAID for the root partition</span></span> <a title="Permalink" class="permalink" href="#cha-raidroot">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raidroot.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, the Device Mapper RAID tool has been integrated into the
  YaST Partitioner. You can use the partitioner at install time to create a
  software RAID for the system device that contains your root
  (<code class="filename">/</code>) partition. The <code class="filename">/boot</code> partition
  cannot be stored on a RAID partition unless it is RAID 1.
 </p><div id="id-1.11.5.3.4" data-id-title="/boot/efi on RAID 1 might not boot" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="filename">/boot/efi</code> on RAID 1 might not boot</div><p>
   When creating the <code class="filename">/boot/efi</code> partition on
   RAID, keep in mind that in some cases the firmware might not 
   recognize the boot partition on RAID. The firmware then
   refuses to boot.
  </p></div><section class="sect1" id="sec-raidroot-require" data-id-title="Prerequisites for using a software RAID device for the root partition"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">Prerequisites for using a software RAID device for the root partition</span></span> <a title="Permalink" class="permalink" href="#sec-raidroot-require">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raidroot.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Ensure that your configuration meets the following requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     You need two hard disks to create the RAID 1 mirror device. The hard
     disks should be similarly sized. The RAID assumes the size of the smaller
     drive. The block storage devices can be any combination of local (in or
     directly attached to the machine), Fibre Channel storage subsystems, or
     iSCSI storage subsystems.
    </p></li><li class="listitem"><p>
     A separate partition for <code class="filename">/boot</code> is not required if you
     install the boot loader in the MBR. If installing the boot loader in the
     MBR is not an option, <code class="filename">/boot</code> needs to reside on a
     separate partition.
    </p></li><li class="listitem"><p>
     For UEFI machines, you need to set up a dedicated
     <code class="filename">/boot/efi</code> partition. It needs to be VFAT-formatted,
     and may reside on the RAID 1 device to prevent booting problems in case
     the physical disk with <code class="filename">/boot/efi</code> fails.
    </p></li><li class="listitem"><p>
     If you are using hardware RAID devices, do not attempt to run software
     RAIDs on top of it.
    </p></li><li class="listitem"><p>
     If you are using iSCSI target devices, you need to enable the iSCSI
     initiator support before you create the RAID device.
    </p></li><li class="listitem"><p>
     If your storage subsystem provides multiple I/O paths between the server
     and its directly attached local devices, Fibre Channel devices, or iSCSI
     devices that you want to use in the software RAID, you need to enable the
     multipath support before you create the RAID device.
    </p></li></ul></div></section><section class="sect1" id="sec-raidroot-setup" data-id-title="Setting up the system with a software RAID device for the root (/) partition"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.2 </span><span class="title-name">Setting up the system with a software RAID device for the root (<code class="filename">/</code>) partition</span></span> <a title="Permalink" class="permalink" href="#sec-raidroot-setup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raidroot.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start the installation with YaST and proceed as described in
     <span class="intraxref">Book “Deployment Guide”, Chapter 9 “Installation steps”</span> until you reach the <span class="guimenu">Suggested
     Partitioning</span> step.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Expert Partitioner</span> to open the custom
     partitioning tool. You can use the suggested proposal, or you can use the
     existing proposal.
    </p></li><li class="step"><p>
     (Optional) If there are iSCSI target devices that you want to use, you
     need to enable the iSCSI Initiator software by choosing
     <span class="guimenu">System</span> › <span class="guimenu">Configure</span> › <span class="guimenu">Configure iSCSI</span> from the upper-left
     section of the screen. Refer to <a class="xref" href="#cha-iscsi" title="Chapter 15. Mass storage over IP networks: iSCSI">Chapter 15, <em>Mass storage over IP networks: iSCSI</em></a> for further
     details.
    </p></li><li class="step"><p>
     (Optional) If there are FCoE target devices that you want to use, you need
     to configure the interface by clicking
     <span class="guimenu">System</span> › <span class="guimenu">Configure</span> › <span class="guimenu">Configure FCoE</span> from the upper left
     section of the screen.
    </p></li><li class="step"><p>
     (Optional) If you need to discard the partitioning changes, click
     <span class="guimenu">System</span> › <span class="guimenu">Rescan
     Devices</span>.
    </p></li><li class="step"><p>
     Set up the <span class="guimenu">Linux RAID</span> format for each of the devices
     you want to use for the software RAID. You should use RAID for
     <code class="filename">/</code>, <code class="filename">/boot/efi</code>, or swap
     partitions.
    </p><ol type="a" class="substeps"><li class="step"><p>
       In the left panel, select <span class="guimenu">Hard Disks</span> and select the
       device you want to use, then click <span class="guimenu">Add Partition</span>.
      </p></li><li class="step"><p>
       Under <span class="guimenu">New Partition Size</span>, specify the size to use,
       then click <span class="guimenu">Next</span>.
      </p></li><li class="step"><p>
       Under <span class="guimenu">Role</span>, choose <span class="guimenu">Raw Volume
       (unformatted)</span>.
      </p></li><li class="step"><p>
       Select <span class="guimenu">Do not format</span> and <span class="guimenu">Do not
       mount</span> and set the <span class="guimenu">Partition ID</span> to
       <span class="guimenu">Linux RAID</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Next</span> and repeat these instructions for the
       second partition.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid_yast_install2_a.png"><img src="images/raid_yast_install2_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li></ol></li><li class="step"><p>
     Create the RAID device for the <code class="filename">/</code> partition.
    </p><ol type="a" class="substeps"><li class="step"><p>
       In the left panel, select <span class="guimenu">RAID</span> and then <span class="guimenu">Add
       RAID</span>.
      </p></li><li class="step"><p>
       Set the desired <span class="guimenu">RAID Type</span> for the
       <code class="filename">/</code> partition and the <span class="guimenu">RAID name</span> to
       <code class="literal">system</code>.
      </p></li><li class="step"><p>
       Select the two RAID devices you prepared in the previous step from the
       <span class="guimenu">Available Devices</span> section and <span class="guimenu">Add</span>
       them.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid_yast_install3_a.png"><img src="images/raid_yast_install3_a.png" width="100%" alt="Image" title="Image"/></a></div></div><p>
       Proceed with <span class="guimenu">Next</span>.
      </p></li><li class="step"><p>
       Select the chunk size from the drop-down box. Sticking with the default
       is a safe choice.
      </p></li><li class="step"><p>
       In the left panel, click the <span class="guimenu">RAID</span>. In the
       <span class="guimenu">Device Overview</span> tab, select your new RAID and click
       <span class="guimenu">Edit</span>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid_yast_install_editraid.png"><img src="images/raid_yast_install_editraid.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Under <span class="guimenu">Role</span>, select <span class="guimenu">Operating
       System</span> and proceed with <span class="guimenu">Next</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">File System</span> and set the mount point to
       <code class="filename">/</code>. Leave the dialog with <code class="filename">Next</code>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid_yast_install_filesystem.png"><img src="images/raid_yast_install_filesystem.png" width="100%" alt="Image" title="Image"/></a></div></div></li></ol></li><li class="step"><p>
     The software RAID device is managed by Device Mapper, and creates a device
     under the <code class="filename">/dev/md/system</code> path.
    </p></li><li class="step"><p>
     Optionally, you can create a swap partition in RAID. Use similar steps to
     those described above, but under <span class="guimenu">Role</span>, select
     <span class="guimenu">swap</span>. Select the file system and mount point as shown
     below. Click <span class="guimenu">Next</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid_yast_install_swap.png"><img src="images/raid_yast_install_swap.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Optionally, for UEFI machines, use similar steps to create the
     <code class="filename">/boot/efi</code> mounted partition. Remember that only
     RAID 1 is supported for <code class="filename">/boot/efi</code>, and the
     partition needs to be formatted with the FAT32 file system.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid_yast_install_boot.png"><img src="images/raid_yast_install_boot.png" width="100%" alt="Image" title="Image"/></a></div></div><p>
     The partitioning then looks as follows:
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid_yast_install3_b.png"><img src="images/raid_yast_install3_b.png" width="70%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Click <span class="guimenu">Accept</span> to leave the partitioner.
    </p><p>
     The new proposal appears on the <span class="guimenu">Suggested Partitioning</span>
     page.
    </p></li><li class="step"><p>
     Continue with the installation. For UEFI machines with a separate
     <code class="filename">/boot/efi</code> partition, click <span class="guimenu">Booting</span>
     on the <span class="guimenu">Installation Settings</span> screen and set
     <span class="guimenu">GRUB2 for EFI</span> as the <span class="guimenu">Boot Loader</span>.
     Check that the <span class="guimenu">Enable Secure Boot Support</span> option is
     activated.
    </p><p>
     Whenever you reboot your server, Device Mapper is started at boot time so
     that the software RAID is automatically recognized, and the operating
     system on the root (<code class="literal">/</code>) partition can be started.
    </p></li></ol></div></div></section></section><section xml:lang="en" class="chapter" id="cha-raid10" data-id-title="Creating software RAID 10 devices"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Creating software RAID 10 devices</span></span> <a title="Permalink" class="permalink" href="#cha-raid10">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes how to set up nested and complex RAID 10 devices.
  A RAID 10 device consists of nested RAID 1 (mirroring) and RAID 0
  (striping) arrays. Nested RAIDs can either be set up as striped mirrors
  (RAID 1+0) or as mirrored stripes (RAID 0+1). A complex
  RAID 10 setup also combines mirrors and stripes and additional data
  security by supporting a higher data redundancy level.
 </p><section class="sect1" id="sec-raid10-nest" data-id-title="Creating nested RAID 10 devices with mdadm"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.1 </span><span class="title-name">Creating nested RAID 10 devices with <code class="command">mdadm</code></span></span> <a title="Permalink" class="permalink" href="#sec-raid10-nest">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A nested RAID device consists of a RAID array that uses another RAID array
   as its basic element, instead of using physical disks. The goal of this
   configuration is to improve the performance and fault tolerance of the RAID.
   Setting up nested RAID levels is not supported by YaST, but can be done by
   using the <code class="command">mdadm</code> command line tool.
  </p><p>
   Based on the order of nesting, two different nested RAIDs can be set up.
   This document uses the following terminology:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">RAID 1+0:</span>
      RAID 1 (mirror) arrays are built first, then combined to form a RAID 0
      (stripe) array.
     </p></li><li class="listitem"><p><span class="formalpara-title">RAID 0+1:</span>
      RAID 0 (stripe) arrays are built first, then combined to form a RAID 1
      (mirror) array.
     </p></li></ul></div><p>
   The following table describes the advantages and disadvantages of RAID 10
   nesting as 1+0 versus 0+1. It assumes that the storage objects you use
   reside on different disks, each with a dedicated I/O capability.
  </p><div class="table" id="id-1.11.5.4.4.6" data-id-title="Nested RAID levels"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 9.1: </span><span class="title-name">Nested RAID levels </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.4.4.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        RAID Level
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Description
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Performance and Fault Tolerance
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        10 (1+0)
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        RAID 0 (stripe) built with RAID 1 (mirror) arrays
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        RAID 1+0 provides high levels of I/O performance, data redundancy, and
        disk fault tolerance. Because each member device in the RAID 0 is
        mirrored individually, multiple disk failures can be tolerated and data
        remains available as long as the disks that fail are in different
        mirrors.
       </p>
       <p>
        You can optionally configure a spare for each underlying mirrored
        array, or configure a spare to serve a spare group that serves all
        mirrors.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        10 (0+1)
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        RAID 1 (mirror) built with RAID 0 (stripe) arrays
       </p>
      </td><td>
       <p>
        RAID 0+1 provides high levels of I/O performance and data redundancy,
        but slightly less fault tolerance than a 1+0. If multiple disks fail on
        one side of the mirror, then the other mirror is available. However, if
        disks are lost concurrently on both sides of the mirror, all data is
        lost.
       </p>
       <p>
        This solution offers less disk fault tolerance than a 1+0 solution, but
        if you need to perform maintenance or maintain the mirror on a
        different site, you can take an entire side of the mirror offline and
        still have a fully functional storage device. Also, if you lose the
        connection between the two sites, either site operates independently of
        the other. That is not true if you stripe the mirrored segments,
        because the mirrors are managed at a lower level.
       </p>
       <p>
        If a device fails, the mirror on that side fails because RAID 1 is not
        fault-tolerant. Create a new RAID 0 to replace the failed side, then
        resynchronize the mirrors.
       </p>
      </td></tr></tbody></table></div></div><section class="sect2" id="sec-raid10-nest-10" data-id-title="Creating nested RAID 10 (1+0) with mdadm"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.1.1 </span><span class="title-name">Creating nested RAID 10 (1+0) with mdadm</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-nest-10">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A nested RAID 1+0 is built by creating two or more RAID 1 (mirror) devices,
    then using them as component devices in a RAID 0.
   </p><div id="id-1.11.5.4.4.7.3" data-id-title="Multipathing" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Multipathing</div><p>
     If you need to manage multiple connections to the devices, you must
     configure multipath I/O before configuring the RAID devices. For
     information, see
     <a class="xref" href="#cha-multipath" title="Chapter 18. Managing multipath I/O for devices">Chapter 18, <em>Managing multipath I/O for devices</em></a>.
    </p></div><p>
    The procedure in this section uses the device names shown in the following
    table. Ensure that you modify the device names with the names of your own
    devices.
   </p><div class="table" id="id-1.11.5.4.4.7.5" data-id-title="Scenario for creating a RAID 10 (1+0) by nesting"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 9.2: </span><span class="title-name">Scenario for creating a RAID 10 (1+0) by nesting </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.4.4.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Raw Devices
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         RAID 1 (mirror)
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         RAID 1+0 (striped mirrors)
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <table style="border: 0; " class="simplelist"><tr><td><code class="filename">/dev/sdb1</code>
         </td></tr><tr><td><code class="filename">/dev/sdc1</code>
         </td></tr></table>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="filename">/dev/md0</code>
        </p>
       </td><td rowspan="2">
        
        <p>
         <code class="filename">/dev/md2</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <table style="border: 0; " class="simplelist"><tr><td><code class="filename">/dev/sdd1</code>
         </td></tr><tr><td><code class="filename">/dev/sde1</code>
         </td></tr></table>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="filename">/dev/md1</code>
        </p>
       </td></tr></tbody></table></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      If necessary, create four 0xFD Linux RAID partitions of equal size using
      a disk partitioner such as parted.
     </p></li><li class="step"><p>
      Create two software RAID 1 devices, using two different devices for
      each device. At the command prompt, enter these two commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --create /dev/md0 --run --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
sudo mdadm --create /dev/md1 --run --level=1 --raid-devices=2 /dev/sdd1 /dev/sde1</pre></div></li><li class="step"><p>
      Create the nested RAID 1+0 device. At the command prompt, enter the
      following command using the software RAID 1 devices you created in the
      previous step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --create /dev/md2 --run --level=0 --chunk=64 \
--raid-devices=2 /dev/md0 /dev/md1</pre></div><p>
      The default chunk size is 64 KB.
     </p></li><li class="step"><p>
      Create a file system on the RAID 1+0 device
      <code class="filename">/dev/md2</code>, for example an XFS file system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mkfs.xfs /dev/md2</pre></div><p>
      Modify the command to use a different file system.
     </p></li><li class="step"><p>
      Edit the <code class="filename">/etc/mdadm.conf</code> file or create it, if it
      does not exist (for example by running <code class="command">sudo vi
      /etc/mdadm.conf</code>). Add the following lines (if the file already
      exists, the first line probably already exists).
     </p><div class="verbatim-wrap"><pre class="screen">DEVICE containers partitions
ARRAY /dev/md0 UUID=<em class="replaceable">UUID</em>
ARRAY /dev/md1 UUID=<em class="replaceable">UUID</em>
ARRAY /dev/md2 UUID=<em class="replaceable">UUID</em></pre></div><p>
      The UUID of each device can be retrieved with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -D /dev/<em class="replaceable">DEVICE</em> | grep UUID</pre></div></li><li class="step"><p>
      Edit the <code class="filename">/etc/fstab</code> file to add an entry for the
      RAID 1+0 device <code class="filename">/dev/md2</code>. The following example
      shows an entry for a RAID device with the XFS file system and
      <code class="filename">/data</code> as a mount point.
     </p><div class="verbatim-wrap"><pre class="screen">/dev/md2 /data xfs defaults 1 2</pre></div></li><li class="step"><p>
      Mount the RAID device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount /data</pre></div></li></ol></div></div></section><section class="sect2" id="sec-raid10-nest-01" data-id-title="Creating nested RAID 10 (0+1) with mdadm"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.1.2 </span><span class="title-name">Creating nested RAID 10 (0+1) with mdadm</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-nest-01">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A nested RAID 0+1 is built by creating two to four RAID 0 (striping)
    devices, then mirroring them as component devices in a RAID 1.
   </p><div id="id-1.11.5.4.4.8.3" data-id-title="Multipathing" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Multipathing</div><p>
     If you need to manage multiple connections to the devices, you must
     configure multipath I/O before configuring the RAID devices. For
     information, see
     <a class="xref" href="#cha-multipath" title="Chapter 18. Managing multipath I/O for devices">Chapter 18, <em>Managing multipath I/O for devices</em></a>.
    </p></div><p>
    In this configuration, spare devices cannot be specified for the underlying
    RAID 0 devices because RAID 0 cannot tolerate a device loss. If a
    device fails on one side of the mirror, you must create a replacement
    RAID 0 device, than add it into the mirror.
   </p><p>
    The procedure in this section uses the device names shown in the following
    table. Ensure that you modify the device names with the names of your own
    devices.
   </p><div class="table" id="id-1.11.5.4.4.8.6" data-id-title="Scenario for creating a RAID 10 (0+1) by nesting"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 9.3: </span><span class="title-name">Scenario for creating a RAID 10 (0+1) by nesting </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.4.4.8.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Raw Devices
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         RAID 0 (stripe)
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         RAID 0+1 (mirrored stripes)
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <table style="border: 0; " class="simplelist"><tr><td><code class="filename">/dev/sdb1</code>
         </td></tr><tr><td><code class="filename">/dev/sdc1</code>
         </td></tr></table>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="filename">/dev/md0</code>
        </p>
       </td><td rowspan="2">
        
        <p>
         <code class="filename">/dev/md2</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <table style="border: 0; " class="simplelist"><tr><td><code class="filename">/dev/sdd1</code>
         </td></tr><tr><td><code class="filename">/dev/sde1</code>
         </td></tr></table>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="filename">/dev/md1</code>
        </p>
       </td></tr></tbody></table></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      If necessary, create four 0xFD Linux RAID partitions of equal size using
      a disk partitioner such as parted.
     </p></li><li class="step"><p>
      Create two software RAID 0 devices, using two different devices for
      each RAID 0 device. At the command prompt, enter these two commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --create /dev/md0 --run --level=0 --chunk=64 \
--raid-devices=2 /dev/sdb1 /dev/sdc1
sudo mdadm --create /dev/md1 --run --level=0 --chunk=64 \
--raid-devices=2 /dev/sdd1 /dev/sde1</pre></div><p>
      The default chunk size is 64 KB.
     </p></li><li class="step"><p>
      Create the nested RAID 0+1 device. At the command prompt, enter the
      following command using the software RAID 0 devices you created in
      the previous step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --create /dev/md2 --run --level=1 --raid-devices=2 /dev/md0 /dev/md1</pre></div></li><li class="step"><p>
      Create a file system on the RAID 1+0 device
      <code class="filename">/dev/md2</code>, for example an XFS file system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mkfs.xfs /dev/md2</pre></div><p>
      Modify the command to use a different file system.
     </p></li><li class="step"><p>
      Edit the <code class="filename">/etc/mdadm.conf</code> file or create it, if it
      does not exist (for example by running <code class="command">sudo vi
      /etc/mdadm.conf</code>). Add the following lines (if the file exists,
      the first line probably already exists, too).
     </p><div class="verbatim-wrap"><pre class="screen">DEVICE containers partitions
ARRAY /dev/md0 UUID=<em class="replaceable">UUID</em>
ARRAY /dev/md1 UUID=<em class="replaceable">UUID</em>
ARRAY /dev/md2 UUID=<em class="replaceable">UUID</em></pre></div><p>
      The UUID of each device can be retrieved with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -D /dev/<em class="replaceable">DEVICE</em> | grep UUID</pre></div></li><li class="step"><p>
      Edit the <code class="filename">/etc/fstab</code> file to add an entry for the
      RAID 1+0 device <code class="filename">/dev/md2</code>. The following example
      shows an entry for a RAID device with the XFS file system and
      <code class="filename">/data</code> as a mount point.
     </p><div class="verbatim-wrap"><pre class="screen">/dev/md2 /data xfs defaults 1 2</pre></div></li><li class="step"><p>
      Mount the RAID device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount /data</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-raid10-complex" data-id-title="Creating a complex RAID 10"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.2 </span><span class="title-name">Creating a complex RAID 10</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   YaST (and <code class="command">mdadm</code> with the <code class="option">--level=10</code>
   option) creates a single complex software RAID 10 that combines
   features of both RAID 0 (striping) and RAID 1 (mirroring). Multiple copies
   of all data blocks are arranged on multiple drives following a striping
   discipline. Component devices should be the same size.
  </p><p>
   The complex RAID 10 is similar in purpose to a nested RAID 10
   (1+0), but differs in the following ways:
  </p><div class="table" id="id-1.11.5.4.5.4" data-id-title="Complex RAID 10 compared to nested RAID 10"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 9.4: </span><span class="title-name">Complex RAID 10 compared to nested RAID 10 </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.4.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Feature
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Complex RAID 10
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Nested RAID 10 (1+0)
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Number of devices
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Allows an even or odd number of component devices
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Requires an even number of component devices
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Component devices
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Managed as a single RAID device
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Manage as a nested RAID device
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Striping
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Striping occurs in the near or far layout on component devices.
       </p>
       <p>
        The far layout provides sequential read throughput that scales by
        number of drives, rather than number of RAID 1 pairs.
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Striping occurs consecutively across component devices
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Multiple copies of data
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Two or more copies, up to the number of devices in the array
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Copies on each mirrored segment
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Hot spare devices
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        A single spare can service all component devices
       </p>
      </td><td>
       <p>
        Configure a spare for each underlying mirrored array, or configure a
        spare to serve a spare group that serves all mirrors.
       </p>
      </td></tr></tbody></table></div></div><section class="sect2" id="sec-raid10-complex-replicas" data-id-title="Number of devices and replicas in the complex RAID 10"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.2.1 </span><span class="title-name">Number of devices and replicas in the complex RAID 10</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-replicas">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When configuring a complex RAID 10 array, you must specify the number
    of replicas of each data block that are required. The default number of
    replicas is two, but the value can be two to the number of devices in the
    array.
   </p><p>
    You must use at least as many component devices as the number of replicas
    you specify. However, the number of component devices in a RAID 10
    array does not need to be a multiple of the number of replicas of each data
    block. The effective storage size is the number of devices divided by the
    number of replicas.
   </p><p>
    For example, if you specify two replicas for an array created with five
    component devices, a copy of each block is stored on two different devices.
    The effective storage size for one copy of all data is 5/2 or 2.5 times the
    size of a component device.
   </p></section><section class="sect2" id="sec-raid10-complex-layout" data-id-title="Layout"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.2.2 </span><span class="title-name">Layout</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-layout">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The complex RAID 10 setup supports three different layouts which
    define how the data blocks are arranged on the disks. The available layouts
    are near (default), far and offset. They have different performance
    characteristics, so it is important to choose the right layout for your
    workload.
   </p><section class="sect3" id="sec-raid10-complex-layout-near" data-id-title="Near layout"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">9.2.2.1 </span><span class="title-name">Near layout</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-layout-near">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     With the near layout, copies of a block of data are striped near each
     other on different component devices. That is, multiple copies of one data
     block are at similar offsets in different devices. Near is the default
     layout for RAID 10. For example, if you use an odd number of
     component devices and two copies of data, some copies are perhaps one
     chunk further into the device.
    </p><p>
     The near layout for the complex RAID 10 yields read and write
     performance similar to RAID 0 over half the number of drives.
    </p><p>
     Near layout with an even number of disks and two replicas:
    </p><div class="verbatim-wrap"><pre class="screen">sda1 sdb1 sdc1 sde1
  0    0    1    1
  2    2    3    3
  4    4    5    5
  6    6    7    7
  8    8    9    9</pre></div><p>
     Near layout with an odd number of disks and two replicas:
    </p><div class="verbatim-wrap"><pre class="screen">sda1 sdb1 sdc1 sde1 sdf1
  0    0    1    1    2
  2    3    3    4    4
  5    5    6    6    7
  7    8    8    9    9
  10   10   11   11   12</pre></div></section><section class="sect3" id="sec-raid10-complex-layout-far" data-id-title="Far layout"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">9.2.2.2 </span><span class="title-name">Far layout</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-layout-far">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The far layout stripes data over the early part of all drives, then
     stripes a second copy of the data over the later part of all drives,
     making sure that all copies of a block are on different drives. The second
     set of values starts halfway through the component drives.
    </p><p>
     With a far layout, the read performance of the complex RAID 10 is
     similar to a RAID 0 over the full number of drives, but write
     performance is substantially slower than a RAID 0 because there is
     more seeking of the drive heads. It is best used for read-intensive
     operations such as for read-only file servers.
    </p><p>
     The speed of the RAID 10 for writing is similar to other mirrored
     RAID types, like RAID 1 and RAID 10 using near layout, as the
     elevator of the file system schedules the writes in a more optimal way
     than raw writing. Using RAID 10 in the far layout is well suited for
     mirrored writing applications.
    </p><p>
     Far layout with an even number of disks and two replicas:
    </p><div class="verbatim-wrap"><pre class="screen">sda1 sdb1 sdc1 sde1
  0    1    2    3
  4    5    6    7
  . . .
  3    0    1    2
  7    4    5    6</pre></div><p class="intro">
     Far layout with an odd number of disks and two replicas:
    </p><div class="verbatim-wrap"><pre class="screen">sda1 sdb1 sdc1 sde1 sdf1
  0    1    2    3    4
  5    6    7    8    9
  . . .
  4    0    1    2    3
  9    5    6    7    8</pre></div></section><section class="sect3" id="sec-raid10-complex-layout-offset" data-id-title="Offset layout"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">9.2.2.3 </span><span class="title-name">Offset layout</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-layout-offset">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The offset layout duplicates stripes so that the multiple copies of a
     given chunk are laid out on consecutive drives and at consecutive offsets.
     Effectively, each stripe is duplicated and the copies are offset by one
     device. This should give similar read characteristics to a far layout if a
     suitably large chunk size is used, but without as much seeking for writes.
    </p><p>
     Offset layout with an even number of disks and two replicas:
    </p><div class="verbatim-wrap"><pre class="screen">sda1 sdb1 sdc1 sde1
  0    1    2    3
  3    0    1    2
  4    5    6    7
  7    4    5    6
  8    9   10   11
 11    8    9   10</pre></div><p>
     Offset layout with an odd number of disks and two replicas:
    </p><div class="verbatim-wrap"><pre class="screen">sda1 sdb1 sdc1 sde1 sdf1
  0    1    2    3    4
  4    0    1    2    3
  5    6    7    8    9
  9    5    6    7    8
 10   11   12   13   14
 14   10   11   12   13</pre></div></section><section class="sect3" id="sec-raid10-complex-layout-parameter" data-id-title="Specifying the number of replicas and the layout with YaST and mdadm"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">9.2.2.4 </span><span class="title-name">Specifying the number of replicas and the layout with YaST and mdadm</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-layout-parameter">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The number of replicas and the layout is specified as <span class="guimenu">Parity
     Algorithm</span> in YaST or with the <code class="option">--layout</code>
     parameter for mdadm. The following values are accepted:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.5.4.5.6.6.3.1"><span class="term"><code class="literal">n<em class="replaceable">N</em></code>
      </span></dt><dd><p>
        Specify <code class="literal">n</code> for near layout and replace
        <em class="replaceable">N</em> with the number of replicas.
        <code class="literal">n2</code> is the default that is used when not configuring
        layout and the number of replicas.
       </p></dd><dt id="id-1.11.5.4.5.6.6.3.2"><span class="term"><code class="literal">f<em class="replaceable">N</em></code>
      </span></dt><dd><p>
        Specify <code class="literal">f</code> for far layout and replace
        <em class="replaceable">N</em> with the number of replicas.
       </p></dd><dt id="id-1.11.5.4.5.6.6.3.3"><span class="term"><code class="literal">o<em class="replaceable">N</em></code>
      </span></dt><dd><p>
        Specify <code class="literal">o</code> for offset layout and replace
        <em class="replaceable">N</em> with the number of replicas.
       </p></dd></dl></div><div id="id-1.11.5.4.5.6.6.4" data-id-title="Number of replicas" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Number of replicas</div><p>
      YaST automatically offers a selection of all possible values for the
      <span class="guimenu">Parity Algorithm</span> parameter.
     </p></div></section></section><section class="sect2" id="sec-raid10-complex-yast" data-id-title="Creating a complex RAID 10 with the YaST partitioner"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.2.3 </span><span class="title-name">Creating a complex RAID 10 with the YaST partitioner</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Launch YaST and open the Partitioner.
     </p></li><li class="step"><p>
      If necessary, create partitions that should be used with your RAID
      configuration. Do not format them and set the partition type to
      <span class="guimenu">0xFD Linux RAID</span>. When using existing partitions it is
      not necessary to change their partition type—YaST will
      automatically do so. Refer to
      <span class="intraxref">Book “Deployment Guide”, Chapter 11 “<span class="guimenu">Expert Partitioner</span>”, Section 11.1 “Using the <span class="guimenu">Expert Partitioner</span>”</span> for details.
     </p><p>
      For RAID 10 at least four partitions are needed. It is strongly
      recommended to use partitions stored on different hard disks to decrease
      the risk of losing data if one is defective. It is recommended to use
      only partitions of the same size because each segment can contribute only
      the same amount of space as the smallest sized partition.
     </p></li><li class="step"><p>
      In the left panel, select <span class="guimenu">RAID</span>.
     </p><p>
      A list of existing RAID configurations opens in the right panel.
     </p></li><li class="step"><p>
      At the lower left of the RAID page, click <span class="guimenu">Add RAID</span>.
     </p></li><li class="step"><p>
      Under <span class="guimenu">RAID Type</span>, select <span class="guimenu">RAID 10 (Mirroring
      and Striping)</span>.
     </p><p>
      You can optionally assign a <span class="guimenu">RAID Name</span> to your RAID. It
      will make it available as
      <code class="filename">/dev/md/<em class="replaceable">NAME</em></code>. See
      <a class="xref" href="#sec-raid-yast-names" title="7.2.1. RAID names">Section 7.2.1, “RAID names”</a> for more information.
     </p></li><li class="step"><p>
      In the <span class="guimenu">Available Devices</span> list, select the desired
      partitions, then click <span class="guimenu">Add</span> to move them to the
      <span class="guimenu">Selected Devices</span> list.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid10_a.png"><img src="images/raid10_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      (Optional) Click <span class="guimenu">Classify</span> to specify the preferred
      order of the disks in the RAID array.
     </p><p>
      For RAID types such as RAID 10, where the order of added disks
      matters, you can specify the order in which the devices will be used.
      This will ensure that one half of the array resides on one disk subsystem
      and the other half of the array resides on a different disk subsystem.
      For example, if one disk subsystem fails, the system keeps running from
      the second disk subsystem.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Select each disk in turn and click one of the <span class="guimenu">Class
        X</span> buttons, where X is the letter you want to assign to the
        disk. Available classes are A, B, C, D and E but for many cases fewer
        classes are needed (only A and B, for example). Assign all available
        RAID disks this way.
       </p><p>
        You can press the <span class="keycap">Ctrl</span> or
        <span class="keycap">Shift</span> key to select multiple devices. You
        can also right-click a selected device and choose the appropriate class
        from the context menu.
       </p></li><li class="step"><p>
        Specify the order of the devices by selecting one of the sorting
        options:
       </p><p><span class="formalpara-title"><span class="guimenu">Sorted</span>:</span>
         Sorts all devices of class A before all devices of class B and so on.
         For example: <code class="literal">AABBCC</code>.
        </p><p><span class="formalpara-title"><span class="guimenu">Interleaved</span>:</span>
         Sorts devices by the first device of class A, then first device of
         class B, then all the following classes with assigned devices. Then
         the second device of class A, the second device of class B, and so on
         follows. All devices without a class are sorted to the end of the
         devices list. For example: <code class="literal">ABCABC</code>.
        </p><p><span class="formalpara-title">Pattern file:</span>
         Select an existing file that contains multiple lines, where each is a
         regular expression and a class name (<code class="literal">"sda.* A"</code>).
         All devices that match the regular expression are assigned to the
         specified class for that line. The regular expression is matched
         against the kernel name (<code class="filename">/dev/sda1</code>), the udev
         path name
         (<code class="filename">/dev/disk/by-path/pci-0000:00:1f.2-scsi-0:0:0:0-part1</code>)
         and then the udev ID
         (<code class="filename">dev/disk/by-id/ata-ST3500418AS_9VMN8X8L-part1</code>).
         The first match made determines the class if a device’s name matches
         more than one regular expression.
        </p></li><li class="step"><p>
        At the bottom of the dialog, click <span class="guimenu">OK</span> to confirm the
        order.
       </p><div class="informalfigure"><div class="mediaobject"><a href="images/raid10_classify_a.png"><img src="images/raid10_classify_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li></ol></li><li class="step"><p>
      Click <span class="guimenu">Next</span>.
     </p></li><li class="step"><p>
      Under <span class="guimenu">RAID Options</span>, specify the <span class="guimenu">Chunk
      Size</span> and <span class="guimenu">Parity Algorithm</span>, then click
      <span class="guimenu">Next</span>.
     </p><p>
      For a RAID 10, the parity options are n (near), f (far), and o (offset).
      The number indicates the number of replicas of each data block that are
      required. Two is the default. For information, see
      <a class="xref" href="#sec-raid10-complex-layout" title="9.2.2. Layout">Section 9.2.2, “Layout”</a>.
     </p></li><li class="step"><p>
      Add a file system and mount options to the RAID device, then click
      <span class="guimenu">Finish</span>.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Next</span>.
     </p></li><li class="step"><p>
      Verify the changes to be made, then click <span class="guimenu">Finish</span> to
      create the RAID.
     </p></li></ol></div></div></section><section class="sect2" id="sec-raid10-complex-yast-mdadm" data-id-title="Creating a complex RAID 10 with mdadm"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.2.4 </span><span class="title-name">Creating a complex RAID 10 with mdadm</span></span> <a title="Permalink" class="permalink" href="#sec-raid10-complex-yast-mdadm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The procedure in this section uses the device names shown in the following
    table. Ensure that you modify the device names with the names of your own
    devices.
   </p><div class="table" id="id-1.11.5.4.5.8.3" data-id-title="Scenario for creating a RAID 10 using mdadm"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 9.5: </span><span class="title-name">Scenario for creating a RAID 10 using mdadm </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.4.5.8.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid10.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Raw Devices
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         RAID 10
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
        <p>
         <code class="filename">/dev/sdf1</code>
        </p>
        <p>
         <code class="filename">/dev/sdg1</code>
        </p>
        <p>
         <code class="filename">/dev/sdh1</code>
        </p>
        <p>
         <code class="filename">/dev/sdi1</code>
        </p>
       </td><td>
        <p>
         <code class="filename">/dev/md3</code>
        </p>
       </td></tr></tbody></table></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      If necessary, create at least four 0xFD Linux RAID partitions of equal
      size using a disk partitioner such as parted.
     </p></li><li class="step"><p>
      Create a RAID 10 by entering the following command.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --create /dev/md3 --run --level=10 --chunk=32 --raid-devices=4 \
/dev/sdf1 /dev/sdg1 /dev/sdh1 /dev/sdi1</pre></div><p>
      Make sure to adjust the value for <code class="option">--raid-devices</code> and the
      list of partitions according to your setup.
     </p><p>
      The command creates an array with near layout and two replicas. To change
      any of the two values, use the <code class="option">--layout</code> as described in
      <a class="xref" href="#sec-raid10-complex-layout-parameter" title="9.2.2.4. Specifying the number of replicas and the layout with YaST and mdadm">Section 9.2.2.4, “Specifying the number of replicas and the layout with YaST and mdadm”</a>.
     </p></li><li class="step"><p>
      Create a file system on the RAID 10 device
      <code class="filename">/dev/md3</code>, for example an XFS file system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mkfs.xfs /dev/md3</pre></div><p>
      Modify the command to use a different file system.
     </p></li><li class="step"><p>
      Edit the <code class="filename">/etc/mdadm.conf</code> file or create it, if it
      does not exist (for example by running <code class="command">sudo vi
      /etc/mdadm.conf</code>). Add the following lines (if the file exists,
      the first line probably already exists, too).
     </p><div class="verbatim-wrap"><pre class="screen">DEVICE containers partitions
ARRAY /dev/md3 UUID=<em class="replaceable">UUID</em></pre></div><p>
      The UUID of the device can be retrieved with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -D /dev/md3 | grep UUID</pre></div></li><li class="step"><p>
      Edit the <code class="filename">/etc/fstab</code> file to add an entry for the
      RAID 10 device <code class="filename">/dev/md3</code>. The following example
      shows an entry for a RAID device with the XFS file system and
      <code class="filename">/data</code> as a mount point.
     </p><div class="verbatim-wrap"><pre class="screen">/dev/md3 /data xfs defaults 1 2</pre></div></li><li class="step"><p>
      Mount the RAID device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount /data</pre></div></li></ol></div></div></section></section></section><section xml:lang="en" class="chapter" id="cha-raid-degraded" data-id-title="Creating a degraded RAID array"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">Creating a degraded RAID array</span></span> <a title="Permalink" class="permalink" href="#cha-raid-degraded">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_degraded_raid.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    A degraded array is one in which some devices are missing. Degraded arrays
    are supported only for RAID 1, RAID 4, RAID 5, and RAID 6. These RAID types
    are designed to withstand some missing devices as part of their
    fault-tolerance features. Typically, degraded arrays occur when a device
    fails. It is possible to create a degraded array on purpose.
   </p></div></div></div></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       RAID Type
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Allowable Number of Slots Missing
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       RAID 1
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       All but one device
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       RAID 4
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       One slot
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       RAID 5
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       One slot
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       RAID 6
      </p>
     </td><td>
      <p>
       One or two slots
      </p>
     </td></tr></tbody></table></div><p>
  To create a degraded array in which some devices are missing, simply give the
  word <code class="literal">missing</code> in place of a device name. This causes
  <code class="command">mdadm</code> to leave the corresponding slot in the array empty.
 </p><p>
  When creating a RAID 5 array, <code class="command">mdadm</code> automatically creates
  a degraded array with an extra spare drive. This is because building the
  spare into a degraded array is generally faster than resynchronizing the
  parity on a non-degraded, but not clean, array. You can override this feature
  with the <code class="literal">--force</code> option.
 </p><p>
  Creating a degraded array might be useful if you want create a RAID, but one
  of the devices you want to use already has data on it. In that case, you
  create a degraded array with other devices, copy data from the in-use device
  to the RAID that is running in degraded mode, add the device into the RAID,
  then wait while the RAID is rebuilt so that the data is now across all
  devices. An example of this process is given in the following procedure:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    To create a degraded RAID 1 device <code class="filename">/dev/md0</code>, using one
    single drive <code class="filename">/dev/sd1</code>, enter the following at the
    command prompt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --create /dev/md0 -l 1 -n 2 /dev/sda1 missing</pre></div><p>
    The device should be the same size or larger than the device you plan to
    add to it.
   </p></li><li class="step"><p>
    If the device you want to add to the mirror contains data that you want to
    move to the RAID array, copy it now to the RAID array while it is running
    in degraded mode.
   </p></li><li class="step"><p>
    Add the device you copied the data from to the mirror. For example, to add
    <code class="filename">/dev/sdb1</code> to the RAID, enter the following at the
    command prompt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm /dev/md0 -a /dev/sdb1</pre></div><p>
    You can add only one device at a time. You must wait for the kernel to
    build the mirror and bring it fully online before you add another mirror.
   </p></li><li class="step"><p>
    Monitor the build progress by entering the following at the command prompt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> cat /proc/mdstat</pre></div><p>
    To see the rebuild progress while being refreshed every second, enter
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> watch -n 1 cat /proc/mdstat</pre></div></li></ol></div></div></section><section xml:lang="en" class="chapter" id="cha-raid-resize" data-id-title="Resizing software RAID arrays with mdadm"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">11 </span><span class="title-name">Resizing software RAID arrays with mdadm</span></span> <a title="Permalink" class="permalink" href="#cha-raid-resize">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This section describes how to increase or reduce the size of a software
    RAID 1, 4, 5, or 6 device with the Multiple Device Administration
    (<code class="command">mdadm(8)</code>) tool.
   </p></div></div></div></div><p>
  Resizing an existing software RAID device involves increasing or decreasing
  the space contributed by each component partition. The file system that
  resides on the RAID must also be able to be resized to take advantage of the
  changes in available space on the device. In <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, file system
  resizing utilities are available for file systems Btrfs, Ext2, Ext3, Ext4, and
  XFS (increase size only). Refer to <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a> for more
  information.
 </p><p>
  The <code class="command">mdadm</code> tool supports resizing only for software RAID
  levels 1, 4, 5, and 6. These RAID levels provide disk fault tolerance so that
  one component partition can be removed at a time for resizing. In principle,
  it is possible to perform a hot resize for RAID partitions, but you must take
  extra care for your data when doing so.
 </p><div id="id-1.11.5.6.5" data-id-title="Back up your data before resizing" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Back up your data before resizing</div><p>
   Resizing any partition or file system involves some risks that can
   potentially result in losing data. To avoid data loss, ensure that you back
   up your data before you begin any resizing task.
  </p></div><p>
  Resizing the RAID involves the following tasks. The order in which these
  tasks are performed depends on whether you are increasing or decreasing its
  size.
 </p><div class="table" id="id-1.11.5.6.7" data-id-title="Tasks involved in resizing a RAID"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 11.1: </span><span class="title-name">Tasks involved in resizing a RAID </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.6.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Tasks
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Description
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Order If Increasing Size
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Order If Decreasing Size
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Resize each of the component partitions.
      </p>
      
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Increase or decrease the active size of each component partition. You
       remove only one component partition at a time, modify its size, then
       return it to the RAID.
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       1
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       2
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Resize the software RAID itself.
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The RAID does not automatically know about the increases or decreases
       you make to the underlying component partitions. You must inform it
       about the new size.
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       2
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       3
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       Resize the file system.
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       You must resize the file system that resides on the RAID. This is
       possible only for file systems that provide tools for resizing.
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       3
      </p>
     </td><td>
      <p>
       1
      </p>
     </td></tr></tbody></table></div></div><p>
  The procedures in the following sections use the device names shown in the
  following table. Ensure that you modify the names to use the names of your
  own devices.
 </p><div class="table" id="id-1.11.5.6.9" data-id-title="Scenario for increasing the size of component partitions"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 11.2: </span><span class="title-name">Scenario for increasing the size of component partitions </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.6.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       RAID Device
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Component Partitions
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
      <p>
       <code class="filename">/dev/md0</code>
      </p>
     </td><td>
      <p>
       <code class="filename">/dev/sda1</code>
      </p>
      <p>
       <code class="filename">/dev/sdb1</code>
      </p>
      <p>
       <code class="filename">/dev/sdc1</code>
      </p>
     </td></tr></tbody></table></div></div><section class="sect1" id="sec-raid-resize-incr" data-id-title="Increasing the size of a software RAID"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.1 </span><span class="title-name">Increasing the size of a software RAID</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-incr">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Increasing the size of a software RAID involves the following tasks in the
   given order: increase the size of all partitions the RAID consists of,
   increase the size of the RAID itself and, finally, increase the size of the
   file system.
  </p><div id="id-1.11.5.6.10.3" data-id-title="Potential data loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Potential data loss</div><p>
    If a RAID does not have disk fault tolerance, or it is simply not
    consistent, data loss results if you remove any of its partitions. Be very
    careful when removing partitions, and ensure that you have a backup of your
    data available.
   </p></div><section class="sect2" id="sec-raid-resize-incr-partitions" data-id-title="Increasing the size of component partitions"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.1.1 </span><span class="title-name">Increasing the size of component partitions</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-incr-partitions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Apply the procedure in this section to increase the size of a RAID 1, 4, 5,
    or 6. For each component partition in the RAID, remove the partition from
    the RAID, modify its size, return it to the RAID, then wait until the RAID
    stabilizes to continue. While a partition is removed, the RAID operates in
    degraded mode and has no or reduced disk fault tolerance. Even for RAIDs
    that can tolerate multiple concurrent disk failures, do not remove more
    than one component partition at a time. To increase the size of the
    component partitions for the RAID, proceed as follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      Ensure that the RAID array is consistent and synchronized by entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/mdstat</pre></div><p>
      If your RAID array is still synchronizing according to the output of this
      command, you must wait until synchronization is complete before
      continuing.
     </p></li><li class="step"><p>
      Remove one of the component partitions from the RAID array. For example,
      to remove <code class="filename">/dev/sda1</code>, enter
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm /dev/md0 --fail /dev/sda1 --remove /dev/sda1</pre></div><p>
      To succeed, both the fail and remove actions must be specified.
     </p></li><li class="step"><p>
      Increase the size of the partition that you removed in the previous step
      by doing one of the following:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Increase the size of the partition, using a disk partitioner such as
        the YaST Partitioner or the command line tool parted. This option is
        the usual choice.
       </p></li><li class="listitem"><p>
        Replace the disk on which the partition resides with a higher-capacity
        device. This option is possible only if no other file systems on the
        original disk are accessed by the system. When the replacement device
        is added back into the RAID, it takes much longer to synchronize the
        data because all of the data that was on the original device must be
        rebuilt.
       </p></li></ul></div></li><li class="step"><p>
      Re-add the partition to the RAID array. For example, to add
      <code class="filename">/dev/sda1</code>, enter
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -a /dev/md0 /dev/sda1</pre></div><p>
      Wait until the RAID is synchronized and consistent before continuing with
      the next partition.
     </p></li><li class="step"><p>
      Repeat these steps for each of the remaining component devices in the
      array. Ensure that you modify the commands for the correct component
      partition.
     </p></li><li class="step"><p>
      If you get a message that tells you that the kernel could not re-read the
      partition table for the RAID, you must reboot the computer after all
      partitions have been resized to force an update of the partition table.
     </p></li><li class="step"><p>
      Continue with
      <a class="xref" href="#sec-raid-resize-incr-raid" title="11.1.2. Increasing the size of the RAID array">Section 11.1.2, “Increasing the size of the RAID array”</a>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-raid-resize-incr-raid" data-id-title="Increasing the size of the RAID array"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.1.2 </span><span class="title-name">Increasing the size of the RAID array</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-incr-raid">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After you have resized each of the component partitions in the RAID (see
    <a class="xref" href="#sec-raid-resize-incr-partitions" title="11.1.1. Increasing the size of component partitions">Section 11.1.1, “Increasing the size of component partitions”</a>),
    the RAID array configuration continues to use the original array size until
    you force it to be aware of the newly available space. You can specify a
    size for the RAID or use the maximum available space.
   </p><p>
    The procedure in this section uses the device name
    <code class="filename">/dev/md0</code> for the RAID device. Ensure that you modify
    the name to use the name of your own device.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      Ensure that the RAID array is consistent and synchronized by entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/mdstat</pre></div><p>
      If your RAID array is still synchronizing according to the output of this
      command, you must wait until synchronization is complete before
      continuing.
     </p></li><li class="step"><p>
      Check the size of the array and the device size known to the array by
      entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -D /dev/md0 | grep -e "Array Size" -e "Dev Size"</pre></div></li><li class="step"><p>
      Do one of the following:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Increase the size of the array to the maximum available size by
        entering
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --grow /dev/md0 -z max</pre></div></li><li class="listitem"><p>
        Increase the size of the array to the maximum available size by
        entering
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --grow /dev/md0 -z max --assume-clean</pre></div><p>
        The array uses any space that has been added to the devices, but this
        space will not be synchronized. This is recommended for RAID 1
        because the synchronization is not needed. It can be useful for other
        RAID levels if the space that was added to the member devices was
        pre-zeroed.
       </p></li><li class="listitem"><p>
        Increase the size of the array to a specified value by entering
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --grow /dev/md0 -z <em class="replaceable">SIZE</em></pre></div><p>
        Replace <em class="replaceable">SIZE</em> with an integer value in
        kilobytes (a kilobyte is 1024 bytes) for the desired size.
       </p></li></ul></div></li><li class="step"><p>
      Recheck the size of your array and the device size known to the array by
      entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -D /dev/md0 | grep -e "Array Size" -e "Dev Size"</pre></div></li><li class="step"><p>
      Do one of the following:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        If your array was successfully resized, continue with
        <a class="xref" href="#sec-raid-resize-incr-fs" title="11.1.3. Increasing the size of the file system">Section 11.1.3, “Increasing the size of the file system”</a>.
       </p></li><li class="listitem"><p>
        If your array was not resized as you expected, you must reboot, then
        try this procedure again.
       </p></li></ul></div></li></ol></div></div></section><section class="sect2" id="sec-raid-resize-incr-fs" data-id-title="Increasing the size of the file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.1.3 </span><span class="title-name">Increasing the size of the file system</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-incr-fs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After you increase the size of the array (see
    <a class="xref" href="#sec-raid-resize-incr-raid" title="11.1.2. Increasing the size of the RAID array">Section 11.1.2, “Increasing the size of the RAID array”</a>),
    you are ready to resize the file system.
   </p><p>
    You can increase the size of the file system to the maximum space available
    or specify an exact size. When specifying an exact size for the file
    system, ensure that the new size satisfies the following conditions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The new size must be greater than the size of the existing data;
      otherwise, data loss occurs.
     </p></li><li class="listitem"><p>
      The new size must be equal to or less than the current RAID size because
      the file system size cannot extend beyond the space available.
     </p></li></ul></div><p>
    Refer to <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a> for detailed instructions.
   </p></section></section><section class="sect1" id="sec-raid-resize-decr" data-id-title="Decreasing the size of a software RAID"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.2 </span><span class="title-name">Decreasing the size of a software RAID</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-decr">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Decreasing the Size of a Software RAID involves the following tasks in the
   given order: decrease the size of the file system, decrease the size of all
   partitions the RAID consists of, and finally decrease the size of the RAID
   itself.
  </p><div id="id-1.11.5.6.11.3" data-id-title="Potential data loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Potential data loss</div><p>
    If a RAID does not have disk fault tolerance, or it is simply not
    consistent, data loss results if you remove any of its partitions. Be very
    careful when removing partitions, and ensure that you have a backup of your
    data available.
   </p></div><div id="id-1.11.5.6.11.4" data-id-title="XFS" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: XFS</div><p>
    Decreasing the size of a file system formatted with XFS is not possible,
    since such a feature is not supported by XFS. As a consequence, the size of
    a RAID that uses the XFS file system cannot be decreased.
   </p></div><section class="sect2" id="sec-raid-resize-decr-fs" data-id-title="Decreasing the size of the file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.2.1 </span><span class="title-name">Decreasing the size of the file system</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-decr-fs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When decreasing the size of the file system on a RAID device, ensure that
    the new size satisfies the following conditions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The new size must be greater than the size of the existing data;
      otherwise, data loss occurs.
     </p></li><li class="listitem"><p>
      The new size must be equal to or less than the current RAID size because
      the file system size cannot extend beyond the space available.
     </p></li></ul></div><p>
    Refer to <a class="xref" href="#cha-resize-fs" title="Chapter 2. Resizing file systems">Chapter 2, <em>Resizing file systems</em></a> for detailed instructions.
   </p></section><section class="sect2" id="sec-raid-resize-decr-raid" data-id-title="Decreasing the size of the RAID array"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.2.2 </span><span class="title-name">Decreasing the size of the RAID array</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-decr-raid">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After you have resized the file system (see
    <a class="xref" href="#sec-raid-resize-decr-fs" title="11.2.1. Decreasing the size of the file system">Section 11.2.1, “Decreasing the size of the file system”</a>), the
    RAID array configuration continues to use the original array size until you
    force it to reduce the available space. Use the <code class="command">mdadm
    --grow</code> mode to force the RAID to use a smaller segment size. To
    do this, you must use the -z option to specify the amount of space in
    kilobytes to use from each device in the RAID. This size must be a multiple
    of the chunk size, and it must leave about 128 KB of space for the RAID
    superblock to be written to the device.
   </p><p>
    The procedure in this section uses the device name
    <code class="filename">/dev/md0</code> for the RAID device. Ensure that you modify
    commands to use the name of your own device.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      Check the size of the array and the device size known to the array by
      entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -D /dev/md0 | grep -e "Array Size" -e "Dev Size"</pre></div></li><li class="step"><p>
      Decrease the array’s device size to a specified value by entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --grow /dev/md0 -z <em class="replaceable">SIZE</em></pre></div><p>
      Replace <em class="replaceable">SIZE</em> with an integer value in
      kilobytes for the desired size. (A kilobyte is 1024 bytes.)
     </p><p>
      For example, the following command sets the segment size for each RAID
      device to about 40 GB where the chunk size is 64 KB. It includes 128 KB
      for the RAID superblock.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm --grow /dev/md2 -z 41943168</pre></div></li><li class="step"><p>
      Recheck the size of your array and the device size known to the array by
      entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -D /dev/md0 | grep -e "Array Size" -e "Device Size"</pre></div></li><li class="step"><p>
      Do one of the following:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        If your array was successfully resized, continue with
        <a class="xref" href="#sec-raid-resize-decr-partitions" title="11.2.3. Decreasing the size of component partitions">Section 11.2.3, “Decreasing the size of component partitions”</a>.
       </p></li><li class="listitem"><p>
        If your array was not resized as you expected, you must reboot, then
        try this procedure again.
       </p></li></ul></div></li></ol></div></div></section><section class="sect2" id="sec-raid-resize-decr-partitions" data-id-title="Decreasing the size of component partitions"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.2.3 </span><span class="title-name">Decreasing the size of component partitions</span></span> <a title="Permalink" class="permalink" href="#sec-raid-resize-decr-partitions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_mdadm-resize.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After you decrease the segment size that is used on each device in the RAID
    (see
    <a class="xref" href="#sec-raid-resize-decr-raid" title="11.2.2. Decreasing the size of the RAID array">Section 11.2.2, “Decreasing the size of the RAID array”</a>),
    the remaining space in each component partition is not used by the RAID.
    You can leave partitions at their current size to allow for the RAID to
    grow at a future time, or you can reclaim this now unused space.
   </p><p>
    To reclaim the space, you decrease the component partitions one at a time.
    For each component partition, you remove it from the RAID, reduce its
    partition size, return the partition to the RAID, then wait until the RAID
    stabilizes. To allow for metadata, you should specify a slightly larger
    size than the size you specified for the RAID in
    <a class="xref" href="#sec-raid-resize-decr-raid" title="11.2.2. Decreasing the size of the RAID array">Section 11.2.2, “Decreasing the size of the RAID array”</a>.
   </p><p>
    While a partition is removed, the RAID operates in degraded mode and has no
    or reduced disk fault tolerance. Even for RAIDs that can tolerate multiple
    concurrent disk failures, you should never remove more than one component
    partition at a time. To decrease the size of the component partitions for
    the RAID, proceed as follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal.
     </p></li><li class="step"><p>
      Ensure that the RAID array is consistent and synchronized by entering
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/mdstat</pre></div><p>
      If your RAID array is still synchronizing according to the output of this
      command, you must wait until synchronization is complete before
      continuing.
     </p></li><li class="step"><p>
      Remove one of the component partitions from the RAID array. For example,
      to remove <code class="filename">/dev/sda1</code>, enter
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm /dev/md0 --fail /dev/sda1 --remove /dev/sda1</pre></div><p>
      To succeed, both the fail and remove actions must be specified.
     </p></li><li class="step"><p>
      Decrease the size of the partition that you removed in the previous step
      to a size that is slightly larger than the size you set for the segment
      size. The size should be a multiple of the chunk size and allow 128 KB
      for the RAID superblock. Use a disk partitioner such as the YaST
      partitioner or the command line tool parted to decrease the size of the
      partition.
     </p></li><li class="step"><p>
      Re-add the partition to the RAID array. For example, to add
      <code class="filename">/dev/sda1</code>, enter
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mdadm -a /dev/md0 /dev/sda1</pre></div><p>
      Wait until the RAID is synchronized and consistent before continuing with
      the next partition.
     </p></li><li class="step"><p>
      Repeat these steps for each of the remaining component devices in the
      array. Ensure that you modify the commands for the correct component
      partition.
     </p></li><li class="step"><p>
      If you get a message that tells you that the kernel could not re-read the
      partition table for the RAID, you must reboot the computer after resizing
      all of its component partitions.
     </p></li><li class="step"><p>
      (Optional) Expand the size of the RAID and file system to use the maximum
      amount of space in the now smaller component partitions and increase the
      size of the file system afterward. Refer to
      <a class="xref" href="#sec-raid-resize-incr-raid" title="11.1.2. Increasing the size of the RAID array">Section 11.1.2, “Increasing the size of the RAID array”</a> for instructions.
     </p></li></ol></div></div></section></section></section><section xml:lang="en" class="chapter" id="cha-raid-leds" data-id-title="Storage enclosure LED utilities for MD software RAIDs"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">12 </span><span class="title-name">Storage enclosure LED utilities for MD software RAIDs</span></span> <a title="Permalink" class="permalink" href="#cha-raid-leds">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    
    Storage enclosure LED Monitoring utility (<code class="command">ledmon</code>) and
    LED Control (<code class="command">ledctl</code>) utility are Linux user space
    applications that use a broad range of interfaces and protocols to control
    storage enclosure LEDs. The primary usage is to visualize the status of
    Linux MD software RAID devices created with the mdadm utility. The
    <code class="systemitem">ledmon</code> daemon monitors the status
    of the drive array and updates the status of the drive LEDs. The
    <code class="command">ledctl</code> utility allows you to set LED patterns for
    specified devices.
   </p></div></div></div></div><p>
  These LED utilities use the SGPIO (Serial General Purpose Input/Output)
  specification (Small Form Factor (SFF) 8485) and the SCSI Enclosure Services
  (SES) 2 protocol to control LEDs. They implement the International Blinking
  Pattern Interpretation (IBPI) patterns of the SFF-8489 specification for
  SGPIO. The IBPI defines how the SGPIO standards are interpreted as states for
  drives and slots on a backplane and how the backplane should visualize the
  states with LEDs.
 </p><p>
  Some storage enclosures do not adhere strictly to the SFF-8489 specification.
  An enclosure processor might accept an IBPI pattern but not blink the LEDs
  according to the SFF-8489 specification, or the processor might support only
  a limited number of the IBPI patterns.
 </p><p>
  LED management (AHCI) and SAF-TE protocols are not supported by the
  <code class="command">ledmon</code> and <code class="command">ledctl</code> utilities.
 </p><p>
  The <code class="systemitem">ledmon</code> and
  <code class="command">ledctl</code> applications have been verified to work with Intel
  storage controllers such as the Intel AHCI controller and Intel SAS
  controller. They also support PCIe-SSD (solid-state drive) enclosure LEDs to
  control the storage enclosure status (OK, Fail, Rebuilding) LEDs of PCIe-SSD
  devices that are part of an MD software RAID volume. The applications might
  also work with the IBPI-compliant storage controllers of other vendors
  (especially SAS/SCSI controllers); however, other vendors’ controllers have
  not been tested.
 </p><p>
  <code class="systemitem">ledmon</code> and <code class="command">ledctl</code>
  are part of the <code class="systemitem">ledmon</code> package,
  which is not installed by default. Run <code class="command">sudo zypper in
  ledmon</code> to install it.
 </p><section class="sect1" id="sec-raid-leds-ledmon" data-id-title="The storage enclosure LED monitor service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.1 </span><span class="title-name">The storage enclosure LED monitor service</span></span> <a title="Permalink" class="permalink" href="#sec-raid-leds-ledmon">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="systemitem">ledmon</code> application is a daemon
   process that constantly monitors the state of MD software RAID devices or
   the state of block devices in a storage enclosure or drive bay. Only a
   single instance of the daemon should be running at a time. The
   <code class="systemitem">ledmon</code> daemon is part of Intel
   Enclosure LED Utilities.
  </p><p>
   The state is visualized on LEDs associated with each slot in a storage array
   enclosure or a drive bay. The application monitors all software RAID devices
   and visualizes their state. It does not provide a way to monitor only
   selected software RAID volumes.
  </p><p>
   The <code class="systemitem">ledmon</code> daemon supports two types
   of LED systems: A two-LED system (Activity LED and Status LED) and a
   three-LED system (Activity LED, Locate LED, and Fail LED). This tool has the
   highest priority when accessing the LEDs.
  </p><p>
   To start <code class="systemitem">ledmon</code>, enter
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> ledmon [options]</pre></div><p>
   where [options] is one or more of the following:
  </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Options for <code class="systemitem">ledmon</code> </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.7.8.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.11.5.7.8.8.2"><span class="term"><code class="option">-c <em class="replaceable">PATH</em></code>
    , </span><span class="term"><code class="option">--confg=<em class="replaceable">PATH</em></code>
    </span></dt><dd><p>
      The configuration is read from <code class="filename">~/.ledctl</code> or from
      <code class="filename">/etc/ledcfg.conf</code> if existing. Use this option to
      specify an alternative configuration file.
     </p><p>
      Currently this option has no effect, since support for configuration
      files has not been implemented yet. See <code class="command">man 5
      ledctl.conf</code> for details.
     </p></dd><dt id="id-1.11.5.7.8.8.3"><span class="term">-l <em class="replaceable">PATH</em>
    , </span><span class="term">--log=<em class="replaceable">PATH</em>
    </span></dt><dd><p>
      Sets a path to local log file. If this user-defined file is specified,
      the global log file <code class="filename">/var/log/ledmon.log</code> is not used.
     </p></dd><dt id="id-1.11.5.7.8.8.4"><span class="term"><code class="option">-t <em class="replaceable">SECONDS</em></code>
    , </span><span class="term"><code class="option">--interval=<em class="replaceable">SECONDS</em></code>
    </span></dt><dd><p>
      Sets the time interval between scans of <code class="filename">sysfs</code>. The
      value is given in seconds. The minimum is 5 seconds. The maximum is not
      specified.
     </p></dd><dt id="id-1.11.5.7.8.8.5"><span class="term"><code class="option">--quiet</code>, <code class="option">--error</code>,
    <code class="option">--warning</code>, <code class="option">--info</code>,
    <code class="option">--debug</code>, <code class="option">--all</code>
    </span></dt><dd><p>
      Specifies the verbosity level. The level options are specified in the
      order of no information to the most information. Use the
      <code class="option">--quiet</code> option for no logging. Use the
      <code class="option">--all</code> option to log everything. If you specify more than
      one verbose option, the last option in the command applies.
     </p></dd><dt id="id-1.11.5.7.8.8.6"><span class="term"><code class="option">-h</code>
    , </span><span class="term"><code class="option">--help</code>
    </span></dt><dd><p>
      Prints the command information to the console, then exits.
     </p></dd><dt id="id-1.11.5.7.8.8.7"><span class="term"><code class="option">-v</code>
    , </span><span class="term"><code class="option">--version</code>
    </span></dt><dd><p>
      Displays version of <code class="command">ledmon</code> and information about the
      license, then exits.
     </p></dd></dl></div><div id="id-1.11.5.7.8.9" data-id-title="Known issues" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Known issues</div><p>
    The <code class="command">ledmon</code> daemon does not recognize the PFA (Predicted
    Failure Analysis) state from the SFF-8489 specification. Thus, the PFA
    pattern is not visualized.
   </p></div></section><section class="sect1" id="sec-raid-leds-ledctl" data-id-title="The storage enclosure LED control application"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.2 </span><span class="title-name">The storage enclosure LED control application</span></span> <a title="Permalink" class="permalink" href="#sec-raid-leds-ledctl">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Enclosure LED Application (<code class="command">ledctl</code>) is a user space
   application that controls LEDs associated with each slot in a storage
   enclosure or a drive bay. The <code class="command">ledctl</code> application is a
   part of Intel Enclosure LED Utilities.
  </p><p>
   When you issue the command, the LEDs of the specified devices are set to a
   specified pattern and all other LEDs are turned off. This application needs
   to be run with <code class="systemitem">root</code> privileges. Because the
   <code class="systemitem">ledmon</code> application has the
   highest priority when accessing LEDs, some patterns set by
   <code class="command">ledctl</code> might have no effect if the
   <code class="systemitem">ledmon</code> daemon is running (except for
   the Locate pattern).
  </p><p>
   The <code class="command">ledctl</code> application supports two types of LED systems:
   A two-LED system (Activity LED and Status LED) and a three-LED system
   (Activity LED, Fail LED, and Locate LED).
  </p><p>
   To start <code class="command">ledctl</code>, enter
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> [options] <em class="replaceable">PATTERN_NAME</em>=list_of_devices</pre></div><p>
   where [options] is one or more of the following:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.5.7.9.8.1"><span class="term"><code class="option">-c <em class="replaceable">PATH</em></code>
    , </span><span class="term"><code class="option">--confg=<em class="replaceable">PATH</em></code>
    </span></dt><dd><p>
      Sets a path to local configuration file. If this option is specified, the
      global configuration file and user configuration file have no effect.
     </p></dd><dt id="id-1.11.5.7.9.8.2"><span class="term">-l <em class="replaceable">PATH</em>
    , </span><span class="term">--log=<em class="replaceable">PATH</em>
    </span></dt><dd><p>
      Sets a path to local log file. If this user-defined file is specified,
      the global log file <code class="filename">/var/log/ledmon.log</code> is not used.
     </p></dd><dt id="id-1.11.5.7.9.8.3"><span class="term"><code class="option">--quiet</code>
    </span></dt><dd><p>
      Turns off all messages sent to <code class="filename">stdout</code> or
      <code class="filename">stderr</code> out. The messages are still logged to local
      file and the <code class="filename">syslog</code> facility.
     </p></dd><dt id="id-1.11.5.7.9.8.4"><span class="term"><code class="option">-h</code>
    , </span><span class="term"><code class="option">--help</code>
    </span></dt><dd><p>
      Prints the command information to the console, then exits.
     </p></dd><dt id="id-1.11.5.7.9.8.5"><span class="term"><code class="option">-v</code>
    , </span><span class="term"><code class="option">--version</code>
    </span></dt><dd><p>
      Displays version of <code class="command">ledctl</code> and information about the
      license, then exits.
     </p></dd></dl></div><section class="sect2" id="sec-raid-leds-ledctl-patterns" data-id-title="Pattern names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.2.1 </span><span class="title-name">Pattern names</span></span> <a title="Permalink" class="permalink" href="#sec-raid-leds-ledctl-patterns">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="command">ledctl</code> application accepts the following names for
    <span class="guimenu">pattern_name</span> argument, according to the SFF-8489
    specification.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.5.7.9.9.3.1"><span class="term"><code class="literal">locate</code>
     </span></dt><dd><p>
       Turns on the Locate LED associated with the specified devices or empty
       slots. This state is used to identify a slot or drive.
      </p></dd><dt id="id-1.11.5.7.9.9.3.2"><span class="term"><code class="literal">locate_off</code>
     </span></dt><dd><p>
       Turns off the Locate LED associated with the specified devices or empty
       slots.
      </p></dd><dt id="id-1.11.5.7.9.9.3.3"><span class="term"><code class="literal">normal</code>
     </span></dt><dd><p>
       Turns off the Status LED, Failure LED, and Locate LED associated with
       the specified devices.
      </p></dd><dt id="id-1.11.5.7.9.9.3.4"><span class="term"><code class="literal">off</code>
     </span></dt><dd><p>
       Turns off only the Status LED and Failure LED associated with the
       specified devices.
      </p></dd><dt id="id-1.11.5.7.9.9.3.5"><span class="term"><code class="literal">ica</code>
     , </span><span class="term"><code class="literal">degraded</code>
     </span></dt><dd><p>
       Visualizes the <code class="literal">In a Critical Array</code> pattern.
      </p></dd><dt id="id-1.11.5.7.9.9.3.6"><span class="term"><code class="literal">rebuild</code>
     , </span><span class="term"><code class="literal">rebuild_p</code>
     </span></dt><dd><p>
       Visualizes the <code class="literal">Rebuild</code> pattern. This supports both of
       the rebuild states for compatibility and legacy reasons.
      </p></dd><dt id="id-1.11.5.7.9.9.3.7"><span class="term"><code class="literal">ifa</code>
     , </span><span class="term"><code class="literal">failed_array</code>
     </span></dt><dd><p>
       Visualizes the <code class="literal">In a Failed Array</code> pattern.
      </p></dd><dt id="id-1.11.5.7.9.9.3.8"><span class="term"><code class="literal">hotspare</code>
     </span></dt><dd><p>
       Visualizes the <code class="literal">Hotspare</code> pattern.
      </p></dd><dt id="id-1.11.5.7.9.9.3.9"><span class="term"><code class="literal">pfa</code>
     </span></dt><dd><p>
       Visualizes the <code class="literal">Predicted Failure Analysis</code> pattern.
      </p></dd><dt id="id-1.11.5.7.9.9.3.10"><span class="term"><code class="literal">failure</code>
     , </span><span class="term"><code class="literal">disk_failed</code>
     </span></dt><dd><p>
       Visualizes the <code class="literal">Failure</code> pattern.
      </p></dd><dt id="id-1.11.5.7.9.9.3.11"><span class="term">ses_abort</span></dt><dd><p>
       SES-2 R/R ABORT
      </p></dd><dt id="id-1.11.5.7.9.9.3.12"><span class="term"><code class="literal">ses_rebuild</code>
     </span></dt><dd><p>
       SES-2 REBUILD/REMAP
      </p></dd><dt id="id-1.11.5.7.9.9.3.13"><span class="term"><code class="literal">ses_ifa</code>
     </span></dt><dd><p>
       SES-2 IN FAILED ARRAY
      </p></dd><dt id="id-1.11.5.7.9.9.3.14"><span class="term"><code class="literal">ses_ica</code>
     </span></dt><dd><p>
       SES-2 IN CRITICAL ARRAY
      </p></dd><dt id="id-1.11.5.7.9.9.3.15"><span class="term"><code class="literal">ses_cons_check</code>
     </span></dt><dd><p>
       SES-2 CONS CHECK
      </p></dd><dt id="id-1.11.5.7.9.9.3.16"><span class="term"><code class="literal">ses_hotspare</code>
     </span></dt><dd><p>
       SES-2 HOTSPARE
      </p></dd><dt id="id-1.11.5.7.9.9.3.17"><span class="term"><code class="literal">ses_rsvd_dev</code>
     </span></dt><dd><p>
       SES-2 RSVD DEVICE
      </p></dd><dt id="id-1.11.5.7.9.9.3.18"><span class="term"><code class="literal">ses_ok</code>
     </span></dt><dd><p>
       SES-2 OK
      </p></dd><dt id="id-1.11.5.7.9.9.3.19"><span class="term"><code class="literal">ses_ident</code>
     </span></dt><dd><p>
       SES-2 IDENT
      </p></dd><dt id="id-1.11.5.7.9.9.3.20"><span class="term"><code class="literal">ses_rm</code>
     </span></dt><dd><p>
       SES-2 REMOVE
      </p></dd><dt id="id-1.11.5.7.9.9.3.21"><span class="term"><code class="literal">ses_insert</code>
     </span></dt><dd><p>
       SES-2 INSERT
      </p></dd><dt id="id-1.11.5.7.9.9.3.22"><span class="term"><code class="literal">ses_missing</code>
     </span></dt><dd><p>
       SES-2 MISSING
      </p></dd><dt id="id-1.11.5.7.9.9.3.23"><span class="term"><code class="literal">ses_dnr</code>
     </span></dt><dd><p>
       SES-2 DO NOT REMOVE
      </p></dd><dt id="id-1.11.5.7.9.9.3.24"><span class="term"><code class="literal">ses_active</code>
     </span></dt><dd><p>
       SES-2 ACTIVE
      </p></dd><dt id="id-1.11.5.7.9.9.3.25"><span class="term"><code class="literal">ses_enable_bb</code>
     </span></dt><dd><p>
       SES-2 ENABLE BYP B
      </p></dd><dt id="id-1.11.5.7.9.9.3.26"><span class="term"><code class="literal">ses_enable_ba</code>
     </span></dt><dd><p>
       SES-2 ENABLE BYP A
      </p></dd><dt id="id-1.11.5.7.9.9.3.27"><span class="term"><code class="literal">ses_devoff</code>
     </span></dt><dd><p>
       SES-2 DEVICE OFF
      </p></dd><dt id="id-1.11.5.7.9.9.3.28"><span class="term"><code class="literal">ses_fault</code>
     </span></dt><dd><p>
       SES-2 FAULT
      </p></dd></dl></div><p>
    When a non-SES-2 pattern is sent to a device in an enclosure, the pattern
    is automatically translated to the SCSI Enclosure Services (SES) 2 pattern
    as shown above.
   </p><div class="table" id="id-1.11.5.7.9.9.5" data-id-title="Translation between non-SES-2 patterns and SES-2 patterns"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 12.1: </span><span class="title-name">Translation between non-SES-2 patterns and SES-2 patterns </span></span><a title="Permalink" class="permalink" href="#id-1.11.5.7.9.9.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Non-SES-2 Pattern
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         SES-2 Pattern
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         locate
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ident
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         locate_off
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ident
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         normal
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ok
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         off
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ok
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         ica
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ica
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         degraded
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ica
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         rebuild
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_rebuild
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         rebuild_p
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_rebuild
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         ifa
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ifa
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         failed_array
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_ifa
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         hotspare
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_hotspare
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         pfa
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_rsvd_dev
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         failure
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         ses_fault
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         disk_failed
        </p>
       </td><td>
        <p>
         ses_fault
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-raid-leds-ledctl-devices" data-id-title="List of devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.2.2 </span><span class="title-name">List of devices</span></span> <a title="Permalink" class="permalink" href="#sec-raid-leds-ledctl-devices">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When you issue the <code class="command">ledctl</code> command, the LEDs of the
    specified devices are set to the specified pattern and all other LEDs are
    turned off. The list of devices can be provided in one of two formats:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      A list of devices separated by a comma and no spaces
     </p></li><li class="listitem"><p>
      A list in curly braces with devices separated by a space
     </p></li></ul></div><p>
    If you specify multiple patterns in the same command, the device list for
    each pattern can use the same or different format. For examples that show
    the two list formats, see
    <a class="xref" href="#sec-raid-leds-ledctl-examples" title="12.2.3. Examples">Section 12.2.3, “Examples”</a>.
   </p><p>
    A device is a path to file in the <code class="filename">/dev</code> directory or in
    the <code class="filename">/sys/block</code> directory. The path can identify a
    block device, an MD software RAID device, or a container device. For a
    software RAID device or a container device, the reported LED state is set
    for all of the associated block devices.
   </p><p>
    The LEDs of devices listed in list_of_devices are set to the given pattern
    pattern_name and all other LEDs are turned off.
   </p></section><section class="sect2" id="sec-raid-leds-ledctl-examples" data-id-title="Examples"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.2.3 </span><span class="title-name">Examples</span></span> <a title="Permalink" class="permalink" href="#sec-raid-leds-ledctl-examples">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To locate a single block device:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> ledctl locate=/dev/sda</pre></div><p>
    To turn off the Locate LED for a single block device:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> ledctl locate_off=/dev/sda</pre></div><p>
    To locate disks of an MD software RAID device and to set a rebuild pattern
    for two of its block devices at the same time:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> ledctl locate=/dev/md127 rebuild={ /sys/block/sd[a-b] }</pre></div><p>
    To turn off the Status LED and Failure LED for the specified devices:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> ledctl off={ /dev/sda /dev/sdb }</pre></div><p>
    To locate three block devices, run one of the following commands (both are
    equivalent):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> ledctl locate=/dev/sda,/dev/sdb,/dev/sdc
<code class="prompt user">&gt; </code><code class="command">sudo</code> ledctl locate={ /dev/sda /dev/sdb /dev/sdc }</pre></div></section></section><section class="sect1" id="sec-raid-leds-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.3 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-raid-leds-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid-leds.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   See the following resources for details about the LED patterns and
   monitoring tools:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="link" href="https://github.com/intel/ledmon.git" target="_blank">LEDMON open
     source project on GitHub.com</a>
    </p></li></ul></div></section></section><section xml:lang="en" class="chapter" id="cha-raidtroubleshooting" data-id-title="Troubleshooting software RAIDs"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">13 </span><span class="title-name">Troubleshooting software RAIDs</span></span> <a title="Permalink" class="permalink" href="#cha-raidtroubleshooting">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Check the <code class="filename">/proc/mdstat</code> file to find out whether a RAID
   partition has been damaged. If a disk fails, 
   replace the defective hard disk with a new one partitioned the same way.
   Then restart your system and enter the command <code class="command">mdadm /dev/mdX --add
   /dev/sdX</code>. Replace <code class="literal">X</code> with your particular device
   identifiers. This integrates the hard disk automatically into the RAID
   system and fully reconstructs it (for all RAID levels except for
   RAID 0).
  </p><p>
   Although you can access all data during the rebuild, you might encounter
   some performance issues until the RAID has been fully rebuilt.
  </p><section class="sect1" id="sec-raid-trouble-autorecovery" data-id-title="Recovery after failing disk is back again"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.1 </span><span class="title-name">Recovery after failing disk is back again</span></span> <a title="Permalink" class="permalink" href="#sec-raid-trouble-autorecovery">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_raid_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are several reasons a disk included in a RAID array may fail. Here is
    a list of the most common ones:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Problems with the disk media.
     </p></li><li class="listitem"><p>
      Disk drive controller failure.
     </p></li><li class="listitem"><p>
      Broken connection to the disk.
     </p></li></ul></div><p>
    In the case of disk media or controller failure, the device needs to be
    replaced or repaired. If a hot spare was not configured within the RAID,
    then manual intervention is required.
   </p><p>
    In the last case, the failed device can be automatically re-added with the
    <code class="command">mdadm</code> command after the connection is repaired (which
    might be automatic).
   </p><p>
    Because <code class="command">md</code>/<code class="command">mdadm</code> cannot reliably
    determine what caused the disk failure, it assumes a serious disk error and
    treats any failed device as faulty until it is explicitly told that the
    device is reliable.
   </p><p>
    Under some circumstances—such as storage devices with an internal
    RAID array—connection problems are very often the cause of the
    device failure. In such case, you can tell <code class="command">mdadm</code> that it
    is safe to automatically <code class="option">--re-add</code> the device after it
    appears. You can do this by adding the following line to
    <code class="filename">/etc/mdadm.conf</code>:
   </p><div class="verbatim-wrap"><pre class="screen">POLICY action=re-add</pre></div><p>
    Note that the device will be automatically re-added after re-appearing only
    if the <code class="systemitem">udev</code> rules cause <code class="command">mdadm -I
    <em class="replaceable">DISK_DEVICE_NAME</em></code> to be run on any
    device that spontaneously appears (default behavior), and if write-intent
    bitmaps are configured (they are by default).
   </p><p>
    If you want this policy to only apply to some devices and not to the
    others, then the <code class="literal">path=</code> option can be added to the
    <code class="literal">POLICY</code> line in <code class="filename">/etc/mdadm.conf</code> to
    restrict the non-default action to only selected devices. Wild cards can be
    used to identify groups of devices. See <code class="command">man 5 mdadm.conf</code>
    for more information.
   </p></section></section></div><div class="part" id="part-net-storage" data-id-title="Network storage"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part IV </span><span class="title-name">Network storage </span></span><a title="Permalink" class="permalink" href="#part-net-storage">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/book_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-isns"><span class="title-number">14 </span><span class="title-name">iSNS for Linux</span></a></span></li><dd class="toc-abstract"><p>
  Storage area networks (SANs) can contain many disk drives that are dispersed
  across complex networks. This can make device discovery and device ownership
  difficult. iSCSI initiators must be able to identify storage resources in the
  SAN and determine whether they have access to them.
 </p></dd><li><span class="chapter"><a href="#cha-iscsi"><span class="title-number">15 </span><span class="title-name">Mass storage over IP networks: iSCSI</span></a></span></li><dd class="toc-abstract"><p>One of the primary tasks of a computer center, or any site that supports servers, is to provide adequate disk capacity. Fibre Channel is often used for this purpose. iSCSI (Internet SCSI) solutions provide a lower-cost alternative to Fibre Channel that can leverage commodity servers and Ethernet net…</p></dd><li><span class="chapter"><a href="#cha-fcoe"><span class="title-number">16 </span><span class="title-name">Fibre Channel storage over Ethernet networks: FCoE</span></a></span></li><dd class="toc-abstract"><p>Many enterprise data centers rely on Ethernet for their LAN and data traffic, and on Fibre Channel networks for their storage infrastructure. Open Fibre Channel over Ethernet (FCoE) Initiator software allows servers with Ethernet adapters to connect to a Fibre Channel storage subsystem over an Ether…</p></dd><li><span class="chapter"><a href="#cha-nvmeof"><span class="title-number">17 </span><span class="title-name">NVMe-oF</span></a></span></li><dd class="toc-abstract"><p>
        This chapter describes how to set up an NVMe over Fabrics host and target.
      </p></dd><li><span class="chapter"><a href="#cha-multipath"><span class="title-number">18 </span><span class="title-name">Managing multipath I/O for devices</span></a></span></li><dd class="toc-abstract"><p>
  This section describes how to manage failover and path load balancing for
  multiple paths between the servers and block storage devices by using
  Multipath I/O (MPIO).
 </p></dd><li><span class="chapter"><a href="#cha-nfs"><span class="title-number">19 </span><span class="title-name">Sharing file systems with NFS</span></a></span></li><dd class="toc-abstract"><p>
        The <span class="emphasis"><em>Network File System</em></span> (<span class="emphasis"><em>NFS</em></span>)
        is a protocol that allows access to files on a server in a manner
        similar to accessing local files.
      </p><p>
        <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> installs NFS v4.2, which introduces support for sparse
        files, file pre-allocation, server-side clone and copy, application
        data block (ADB), and labeled NFS for mandatory access control (MAC)
        (requires MAC on both client and server).
      </p></dd><li><span class="chapter"><a href="#cha-samba"><span class="title-number">20 </span><span class="title-name">Samba</span></a></span></li><dd class="toc-abstract"><p>
        Using Samba, a Unix machine can be configured as a file and print
        server for macOS, Windows, and OS/2 machines. Samba has developed into
        a fully fledged and rather complex product. Configure Samba with
        YaST, or by editing the configuration file manually.
      </p></dd><li><span class="chapter"><a href="#cha-autofs"><span class="title-number">21 </span><span class="title-name">On-demand mounting with autofs</span></a></span></li><dd class="toc-abstract"><p>
    <code class="systemitem">autofs</code> is a program that automatically mounts
    specified directories on an on-demand basis. It is based on a kernel module
    for high efficiency, and can manage both local directories and network
    shares. These automatic mount points are mounted only when they are
    accessed, and unmounted after a certain period of inactivity. This
    on-demand behavior saves bandwidth and results in better performance than
    static mounts managed by <code class="filename">/etc/fstab</code>. While
    <code class="systemitem">autofs</code> is a control script,
    <code class="command">automount</code> is the command (daemon) that does the actual
    auto-mounting.
   </p></dd></ul></div><section xml:lang="en" class="chapter" id="cha-isns" data-id-title="iSNS for Linux"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">14 </span><span class="title-name">iSNS for Linux</span></span> <a title="Permalink" class="permalink" href="#cha-isns">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Storage area networks (SANs) can contain many disk drives that are dispersed
  across complex networks. This can make device discovery and device ownership
  difficult. iSCSI initiators must be able to identify storage resources in the
  SAN and determine whether they have access to them.
 </p><p>
  Internet Storage Name Service (iSNS) is a standards-based service that
  simplifies the automated discovery, management, and configuration of iSCSI
  devices on a TCP/IP network. iSNS provides intelligent storage discovery and
  management services comparable to those found in Fibre Channel networks.
 </p><p>
  Without iSNS, you must know the host name or IP address of each node where
  targets of interest are located. In addition, you must manually manage which
  initiators can access which targets yourself using mechanisms such as access
  control lists.
 </p><div id="id-1.11.6.2.6" data-id-title="Security considerations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Security considerations</div><p>
   iSNS should only be used in secure internal networking environments, as
   the network traffic is not encrypted.
  </p></div><section class="sect1" id="sec-isns-overview" data-id-title="How iSNS works"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.1 </span><span class="title-name">How iSNS works</span></span> <a title="Permalink" class="permalink" href="#sec-isns-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For an iSCSI initiator to discover iSCSI targets, it needs to identify which
   devices in the network are storage resources and what IP addresses it needs
   to access them. A query to an iSNS server returns a list of iSCSI targets
   and the IP addresses that the initiator has permission to access.
  </p><p>
   Using iSNS, you create iSNS discovery domains into which you then group or
   organize iSCSI targets and initiators. By dividing storage nodes into
   domains, you can limit the discovery process of each host to the most
   appropriate subset of targets registered with iSNS, which allows the storage
   network to scale by reducing the number of unnecessary discoveries and by
   limiting the amount of time each host spends establishing discovery
   relationships. This lets you control and simplify the number of targets and
   initiators that must be discovered.
  </p><div class="figure" id="id-1.11.6.2.7.4"><div class="figure-contents"><div class="mediaobject"><a href="images/isns_a.png"><img src="images/isns_a.png" width="100%" alt="iSNS discovery domains" title="iSNS discovery domains"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 14.1: </span><span class="title-name">iSNS discovery domains </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.2.7.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div><p>
   Both iSCSI targets and iSCSI initiators can use iSNS clients to initiate
   transactions with iSNS servers by using the iSNS protocol. They then
   register device attribute information in a common discovery domain, download
   information about other registered clients, and receive asynchronous
   notification of events that occur in their discovery domain.
  </p><p>
   iSNS servers respond to iSNS protocol queries and requests made by iSNS
   clients using the iSNS protocol. iSNS servers initiate iSNS protocol state
   change notifications and store properly authenticated information submitted
   by a registration request in an iSNS database.
  </p><p>
   Benefits provided by iSNS for Linux include:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Provides an information facility for registration, discovery, and
     management of networked storage assets.
    </p></li><li class="listitem"><p>
     Integrates with the DNS infrastructure.
    </p></li><li class="listitem"><p>
     Consolidates registration, discovery, and management of iSCSI storage.
    </p></li><li class="listitem"><p>
     Simplifies storage management implementations.
    </p></li><li class="listitem"><p>
     Improves scalability compared to other discovery methods.
    </p></li></ul></div><p>
   iSNS offers several important benefits.
  </p><p>
   For example, in a setup with 100 iSCSI initiators and 100 iSCSI targets, all
   iSCSI initiators could potentially try to discover and connect to any of the
   100 iSCSI targets.  By grouping initiators and targets into discovery
   domains, you can prevent iSCSI initiators in one department from discovering
   the iSCSI targets in another department.
  </p><p>
   Another advantage of using iSNS is that the iSCSI clients only need to know
   the host name or IP address of the iSNS server, rather than having to know
   the host names or IP addresses of a hundred servers.
  </p></section><section class="sect1" id="sec-isns-install" data-id-title="Installing iSNS server for Linux"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.2 </span><span class="title-name">Installing iSNS server for Linux</span></span> <a title="Permalink" class="permalink" href="#sec-isns-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   iSNS Server for Linux is included with <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, but is not installed
   or configured by default. You need to install the package
   <code class="systemitem">open-isns</code> and configure the iSNS
   service.
  </p><div id="id-1.11.6.2.8.3" data-id-title="iSNS and iSCSI on the same server" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: iSNS and iSCSI on the same server</div><p>
    iSNS can be installed on the same server where iSCSI target or iSCSI
    initiator software is installed. Installing both the iSCSI target software
    and iSCSI initiator software on the same server is not supported.
   </p></div><p>
   To install iSNS for Linux:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start YaST and select <span class="guimenu">Network Services</span> › <span class="guimenu">iSNS Server</span>.
    </p></li><li class="step"><p>
     In case <code class="systemitem">open-isns</code> is not
     installed yet, you are prompted to install it now. Confirm by clicking
     <span class="guimenu">Install</span>.
    </p></li><li class="step"><p>
     The iSNS Service configuration dialog opens automatically to the
     <span class="guimenu">Service</span> tab.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/isns_config_a.png"><img src="images/isns_config_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     In <span class="guimenu">Service Start</span>, select one of the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">When booting:</span>
        The iSNS service starts automatically on server start-up.
       </p></li><li class="listitem"><p><span class="formalpara-title">Manually (default):</span>
        The iSNS service must be started manually by entering <code class="command">sudo
        systemctl start isnsd</code> at the server console of the server
        where you install it.
       </p></li></ul></div></li><li class="step"><p>
     Specify the following firewall settings:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Open port in firewall:</span>
        Select the check box to open the firewall and allow access to the
        service from remote computers. The firewall port is closed by default.
       </p></li><li class="listitem"><p><span class="formalpara-title">Firewall details:</span>
        If you open the firewall port, the port is open on all network
        interfaces by default. Click <span class="guimenu">Firewall Details</span> to
        select interfaces on which to open the port, select the network
        interfaces to use, then click <span class="guimenu">OK</span>.
       </p></li></ul></div></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to apply the configuration settings and
     complete the installation.
    </p></li><li class="step"><p>
     Continue with
     <a class="xref" href="#sec-isns-domains" title="14.3. Configuring iSNS discovery domains">Section 14.3, “Configuring iSNS discovery domains”</a>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-isns-domains" data-id-title="Configuring iSNS discovery domains"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.3 </span><span class="title-name">Configuring iSNS discovery domains</span></span> <a title="Permalink" class="permalink" href="#sec-isns-domains">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For iSCSI initiators and targets to use the iSNS service, they must belong
   to a discovery domain.
  </p><div id="id-1.11.6.2.9.3" data-id-title="The iSNS service must be active" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: The iSNS service must be active</div><p>
    The iSNS service must be installed and running to configure iSNS discovery
    domains. For information, see
    <a class="xref" href="#sec-isns-start" title="14.4. Starting the iSNS service">Section 14.4, “Starting the iSNS service”</a>.
   </p></div><section class="sect2" id="sec-isns-domains-ddcreate" data-id-title="Creating iSNS discovery domains"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">14.3.1 </span><span class="title-name">Creating iSNS discovery domains</span></span> <a title="Permalink" class="permalink" href="#sec-isns-domains-ddcreate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A default discovery domain named <span class="guimenu">default DD</span> is
    automatically created when you install the iSNS service. The existing iSCSI
    targets and initiators that have been configured to use iSNS are
    automatically added to the default discovery domain.
   </p><p>
    To create a new discovery domain:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start YaST and under <span class="guimenu">Network Services</span>, select
      <span class="guimenu">iSNS Server</span>.
     </p></li><li class="step"><p>
      Click the <span class="guimenu">Discovery Domains</span> tab.
     </p><p>
      The <span class="guimenu">Discovery Domains</span> area lists all existing
      discovery domains. You can <span class="guimenu">Create Discovery Domains</span>,
      or <span class="guimenu">Delete</span> existing ones. Keep in mind that deleting
     an iSCSI node from domain membership removes it from the domain, but
     it does not delete the iSCSI node.
     </p><p>
      The <span class="guimenu">Discovery Domain Members</span> area lists all iSCSI
      nodes assigned to a selected discovery domain. Selecting a different
      discovery domain refreshes the list with members from that discovery
      domain. You can add and delete iSCSI nodes from a selected discovery
      domain. Deleting an iSCSI node removes it from the domain, but it does
      not delete the iSCSI node.
     </p><p>
      <span class="guimenu">Create iSCSI Node Member</span> allows a node that is not yet
      registered to be added as a member of the discovery domain. When the
      iSCSI initiator or target registers this node, then it becomes part of
      this domain.
     </p><p class="intro">
      When an iSCSI initiator performs a discovery request, the iSNS service
      returns all iSCSI node targets that are members of the same discovery
      domain.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/isns_discdomains_a.png"><img src="images/isns_discdomains_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Click the <span class="guimenu">Create Discovery Domain</span> button.
     </p><p>
      You can also select an existing discovery domain and click the
      <span class="guimenu">Delete</span> button to remove that discovery domain.
     </p></li><li class="step"><p>
      Specify the name of the discovery domain you are creating, then click
      <span class="guimenu">OK</span>.
     </p></li><li class="step"><p>
      Continue with
      <a class="xref" href="#sec-isns-domains-iscsi-nodes" title="14.3.2. Adding iSCSI nodes to a discovery domain">Section 14.3.2, “Adding iSCSI nodes to a discovery domain”</a>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-isns-domains-iscsi-nodes" data-id-title="Adding iSCSI nodes to a discovery domain"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">14.3.2 </span><span class="title-name">Adding iSCSI nodes to a discovery domain</span></span> <a title="Permalink" class="permalink" href="#sec-isns-domains-iscsi-nodes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start YaST and under <span class="guimenu">Network Services</span>, select
      <span class="guimenu">iSNS Server</span>.
     </p></li><li class="step"><p>
      Click the <span class="guimenu">iSCSI Nodes</span> tab.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/isns_iscsinodes_a.png"><img src="images/isns_iscsinodes_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Review the list of nodes to ensure that the iSCSI targets and initiators
      that you want to use the iSNS service are listed.
     </p><p>
      If an iSCSI target or initiator is not listed, you might need to restart
      the iSCSI service on the node. You can do this by running
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart iscsid.socket
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart iscsi</pre></div><p>
      to restart an initiator or
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart target-isns</pre></div><p>
      to restart a target.
     </p><p>
      You can select an iSCSI node and click the <span class="guimenu">Delete</span>
      button to remove that node from the iSNS database. This is useful if you
      are no longer using an iSCSI node or have renamed it.
     </p><p>
      The iSCSI node is automatically added to the list (iSNS database) again
      when you restart the iSCSI service or reboot the server unless you remove
      or comment out the iSNS portion of the iSCSI configuration file.
     </p></li><li class="step"><p>
      Click the <span class="guimenu">Discovery Domains</span> tab and select the desired
      discovery domain.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Add existing iSCSI Node</span>, select the node you
      want to add to the domain, then click <span class="guimenu">Add Node</span>.
     </p></li><li class="step"><p>
      Repeat the previous step for as many nodes as you want to add to the
      discovery domain, then click <span class="guimenu">Done</span> when you are
      finished adding nodes.
     </p><p>
      Note that an iSCSI node can belong to more than one discovery domain.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-isns-start" data-id-title="Starting the iSNS service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.4 </span><span class="title-name">Starting the iSNS service</span></span> <a title="Permalink" class="permalink" href="#sec-isns-start">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   iSNS must be started at the server where you install it. If you have not
   configured it to be started at boot time (see
   <a class="xref" href="#sec-isns-install" title="14.2. Installing iSNS server for Linux">Section 14.2, “Installing iSNS server for Linux”</a> for details), enter the following command
   at a terminal:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl start isnsd</pre></div><p>
   You can also use the <code class="command">stop</code>, <code class="command">status</code>, and
   <code class="command">restart</code> options with iSNS.
  </p></section><section class="sect1" id="sec-isns-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.5 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-isns-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_isns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following projects provide further information on iSNS and iSCSI:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="link" href="https://github.com/open-iscsi/open-isns" target="_blank">iSNS server and client for Linux project</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://github.com/open-iscsi/target-isns" target="_blank">iSNS client for the Linux LIO iSCSI target </a>
    </p></li><li class="listitem"><p>
     <a class="link" href="https://www.open-iscsi.com" target="_blank">iSCSI tools for Linux</a>
    </p></li></ul></div><p>
   General information about iSNS is available in <em class="citetitle">RFC 4171: Internet
   Storage Name Service</em> at
   <a class="link" href="https://datatracker.ietf.org/doc/html/rfc4171" target="_blank">https://datatracker.ietf.org/doc/html/rfc4171</a>.
  </p></section></section><section xml:lang="en" class="chapter" id="cha-iscsi" data-id-title="Mass storage over IP networks: iSCSI"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">15 </span><span class="title-name">Mass storage over IP networks: iSCSI</span></span> <a title="Permalink" class="permalink" href="#cha-iscsi">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  One of the primary tasks of a computer center, or any site that supports
  servers, is to provide adequate disk capacity. Fibre Channel is often used
  for this purpose. iSCSI (Internet SCSI) solutions provide a lower-cost
  alternative to Fibre Channel that can leverage commodity servers and Ethernet
  networking equipment. Linux iSCSI provides iSCSI initiator and iSCSI LIO
  target software for connecting Linux servers to central storage systems.
 </p><div class="figure" id="id-1.11.6.3.4"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi_san_a.png"><img src="images/iscsi_san_a.png" width="100%" alt="iSCSI SAN with an iSNS server" title="iSCSI SAN with an iSNS server"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 15.1: </span><span class="title-name">iSCSI SAN with an iSNS server </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div><div id="id-1.11.6.3.5" data-id-title="LIO" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: LIO</div><p>
   LIO is the standard open
   source multiprotocol SCSI target for Linux. LIO replaced the STGT (SCSI
   Target) framework as the standard unified storage target in Linux with Linux
   kernel version 2.6.38 and later. In <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12 the iSCSI LIO Target
   Server replaces the iSCSI Target Server from previous versions.
  </p></div><p>
  iSCSI is a storage networking protocol that simplifies data transfers of SCSI
  packets over TCP/IP networks between block storage devices and servers. iSCSI
  target software runs on the target server and defines the logical units as
  iSCSI target devices. iSCSI initiator software runs on different servers and
  connects to the target devices to make the storage devices available on that
  server.
 </p><p>
  The iSCSI LIO target server and iSCSI initiator servers communicate by
  sending SCSI packets at the IP level in your LAN. When an application running
  on the initiator server starts an inquiry for an iSCSI LIO target device, the
  operating system produces the necessary SCSI commands. The SCSI commands are
  then embedded in IP packets and encrypted as necessary by software that is
  commonly known as the <span class="emphasis"><em>iSCSI initiator</em></span>. The packets are
  transferred across the internal IP network to the corresponding iSCSI remote
  station, called the <span class="emphasis"><em>iSCSI LIO target server</em></span>, or simply
  the <span class="emphasis"><em>iSCSI target</em></span>.
 </p><p>
  Many storage solutions provide access over iSCSI, but it is also possible to
  run a Linux server that provides an iSCSI target. In this case, it is
  important to set up a Linux server that is optimized for file system
  services. For more information about RAID, also see
  <a class="xref" href="#cha-raid" title="Chapter 7. Software RAID configuration">Chapter 7, <em>Software RAID configuration</em></a>.
 </p><section class="sect1" id="sec-iscsi-install" data-id-title="Installing the iSCSI LIO target server and iSCSI initiator"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.1 </span><span class="title-name">Installing the iSCSI LIO target server and iSCSI initiator</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   While the iSCSI initiator is installed by default (packages
   <code class="systemitem">open-iscsi</code> and
   <code class="systemitem">yast2-iscsi-client</code>), the iSCSI LIO
   target packages need to be installed manually.
  </p><div id="id-1.11.6.3.9.3" data-id-title="Initiator and target on the same server" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Initiator and target on the same server</div><p>
    While it is possible to run initiator and target in the same system, this
    setup is not recommended.
   </p></div><p>
   To install the iSCSI LIO Target Server, run the following command in a
   terminal:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper in yast2-iscsi-lio-server</pre></div><p>
   In case you need to install the iSCSI initiator or any of its dependencies,
   run the command <code class="command">sudo zypper in yast2-iscsi-client</code>.
  </p><p>
   Alternatively, use the YaST Software Management module for installation.
  </p><p>
   Any packages required in addition to the ones mentioned above will either be
   automatically pulled in by the installer, or be installed when you first run
   the respective YaST module.
  </p></section><section class="sect1" id="sec-iscsi-target" data-id-title="Setting up an iSCSI LIO target server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.2 </span><span class="title-name">Setting up an iSCSI LIO target server</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section describes how to use YaST to configure an iSCSI LIO Target
   Server and set up iSCSI LIO target devices. You can use any iSCSI initiator
   software to access the target devices.
  </p><section class="sect2" id="sec-iscsi-target-start" data-id-title="iSCSI LIO target service start-up and firewall settings"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.1 </span><span class="title-name">iSCSI LIO target service start-up and firewall settings</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target-start">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The iSCSI LIO Target service is by default configured to be started
    manually. You can configure the service to start automatically at boot
    time. If you use a firewall on the server and you want the iSCSI LIO
    targets to be available to other computers, you must open a port in the
    firewall for each adapter that you want to use for target access. TCP port
    3260 is the port number for the iSCSI protocol, as defined by IANA
    (Internet Assigned Numbers Authority).
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start YaST and launch <span class="guimenu">Network Services</span> › <span class="guimenu">iSCSI LIO Target</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Service</span> tab.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/lio_service_a.png"><img src="images/lio_service_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Under <span class="guimenu">Service Start</span>, specify how you want the iSCSI
      LIO target service to be started:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">When booting:</span>
         The service starts automatically on server restart.
        </p></li><li class="listitem"><p><span class="formalpara-title">Manually:</span>
         (Default) You must start the service manually after a server restart
         by running <code class="command">sudo systemctl start targetcli</code>. The
         target devices are not available until you start the service.
        </p></li></ul></div></li><li class="step"><p>
      If you use a firewall on the server and you want the iSCSI LIO targets to
      be available to other computers, open port 3260 in the firewall for each
      adapter interface that you want to use for target access. If the port is
      closed for all of the network interfaces, the iSCSI LIO targets are not
      available to other computers.
     </p><p>
      If you do not use a firewall on the server, the firewall settings are
      disabled. In this case skip the following steps and leave the
      configuration dialog with <span class="guimenu">Finish</span> or switch to another
      tab to continue with the configuration.
     </p><ol type="a" class="substeps"><li class="step"><p>
        On the <span class="guimenu">Services</span> tab, select the <span class="guimenu">Open Port
        in Firewall</span> check box to enable the firewall settings.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Firewall Details</span> to view or configure the
        network interfaces to use. All available network interfaces are listed,
        and all are selected by default. Deselect all interfaces on which the
        port should <span class="emphasis"><em>not</em></span> be opened. Save your settings with
        <span class="guimenu">OK</span>.
       </p></li></ol></li><li class="step"><p>
      Click <span class="guimenu">Finish</span> to save and apply the iSCSI LIO Target
      service settings.
     </p></li></ol></div></div></section><section class="sect2" id="sec-iscsi-target-authenticate" data-id-title="Configuring authentication for discovery of iSCSI LIO targets and initiators"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.2 </span><span class="title-name">Configuring authentication for discovery of iSCSI LIO targets and initiators</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target-authenticate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The iSCSI LIO Target Server software supports the PPP-CHAP (Point-to-Point
    Protocol Challenge Handshake Authentication Protocol), a three-way
    authentication method defined in the <em class="citetitle">Internet Engineering Task
    Force (IETF) RFC 1994</em>
    (<a class="link" href="https://datatracker.ietf.org/doc/html/rfc1994" target="_blank">https://datatracker.ietf.org/doc/html/rfc1994</a>). The
    server uses this authentication method for the discovery of iSCSI LIO
    targets and initiators, not for accessing files on the targets. If you do
    not want to restrict the access to the discovery, use <span class="guimenu">No
    Authentication</span>. The <span class="guimenu">No Discovery
    Authentication</span> option is enabled by default. Without requiring
    authentication all iSCSI LIO targets on this server can be discovered by
    any iSCSI initiator on the same network.
   </p><p>
    If authentication is needed for a more secure configuration, you can use
    incoming authentication, outgoing authentication, or both.
    <span class="guimenu">Authentication by Initiators</span> requires an iSCSI initiator
    to prove that it has the permissions to run a discovery on the iSCSI LIO
    target. The initiator must provide the incoming user name and password.
    <span class="guimenu">Authentication by Targets</span> requires the iSCSI LIO target
    to prove to the initiator that it is the expected target. The iSCSI LIO
    target must provide the outgoing user name and password to the iSCSI
    initiator. The password needs to be different for incoming and outgoing
    discovery. If authentication for discovery is enabled, its settings apply
    to all iSCSI LIO target groups.
   </p><div id="id-1.11.6.3.10.4.4" data-id-title="Security" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Security</div><p>
     We recommend that you use authentication for target and initiator
     discovery in production environments for security reasons.
    </p></div><p>
    To configure authentication preferences for iSCSI LIO targets:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start YaST and launch <span class="guimenu">Network Services</span> › <span class="guimenu">iSCSI LIO Target</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Global</span> tab.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/lio_global_noauth_a.png"><img src="images/lio_global_noauth_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      By default, authentication is disabled (<span class="guimenu">No Discovery
      Authentication</span>). To enable Authentication, select
      <span class="guimenu">Authentication by Initiators</span>, <span class="guimenu">Outgoing
      Authentication</span> or both.
     </p></li><li class="step"><p>
      Provide credentials for the selected authentication method(s). The user
      name and password pair must be different for incoming and outgoing
      discovery.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Finish</span> to save and apply the settings.
     </p></li></ol></div></div></section><section class="sect2" id="sec-iscsi-target-storage" data-id-title="Preparing the storage space"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.3 </span><span class="title-name">Preparing the storage space</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target-storage">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before you configure LUNs for your iSCSI Target servers, you must prepare
    the storage you want to use. You can use the entire unformatted block
    device as a single LUN, or you can subdivide a device into unformatted
    partitions and use each partition as a separate LUN. The iSCSI target
    configuration exports the LUNs to iSCSI initiators.
   </p><p>
    You can use the Partitioner in YaST or the command line to set up the
    partitions. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “<span class="guimenu">Expert Partitioner</span>”, Section 11.1 “Using the <span class="guimenu">Expert Partitioner</span>”</span> for details.
    iSCSI LIO targets can use unformatted partitions with Linux, Linux LVM, or
    Linux RAID file system IDs.
   </p><div id="id-1.11.6.3.10.5.4" data-id-title="Do not mount iSCSI target devices" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Do not mount iSCSI target devices</div><p>
     After you set up a device or partition for use as an iSCSI target, you
     never access it directly via its local path. Do not mount the partitions
     on the target server.
    </p></div><section class="sect3" id="sec-iscsi-target-storage-vm" data-id-title="Partitioning devices in a virtual environment"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.2.3.1 </span><span class="title-name">Partitioning devices in a virtual environment</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target-storage-vm">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You can use a virtual machine guest server as an iSCSI LIO Target Server.
     This section describes how to assign partitions to a Xen virtual
     machine. You can also use other virtual environments that are supported by
     <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>.
    </p><p>
     In a Xen virtual environment, you must assign the storage space you want
     to use for the iSCSI LIO target devices to the guest virtual machine, then
     access the space as virtual disks within the guest environment. Each
     virtual disk can be a physical block device, such as an entire disk,
     partition, or volume, or it can be a file-backed disk image where the
     virtual disk is a single image file on a larger physical disk on the Xen
     host server. For the best performance, create each virtual disk from a
     physical disk or a partition. After you set up the virtual disks for the
     guest virtual machine, start the guest server, then configure the new
     blank virtual disks as iSCSI target devices by following the same process
     as for a physical server.
    </p><p>
     File-backed disk images are created on the Xen host server, then
     assigned to the Xen guest server. By default, Xen stores file-backed
     disk images in the
     <code class="filename">/var/lib/xen/images/<em class="replaceable">VM_NAME</em></code>
     directory, where <code class="filename"><em class="replaceable">VM_NAME</em></code>
     is the name of the virtual machine.
    </p></section></section><section class="sect2" id="sec-iscsi-target-target-group" data-id-title="Setting up an iSCSI LIO target group"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.4 </span><span class="title-name">Setting up an iSCSI LIO target group</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target-target-group">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can use YaST to configure iSCSI LIO target devices. YaST uses the
    <code class="command">targetcli</code> software. iSCSI LIO targets can use partitions
    with Linux, Linux LVM, or Linux RAID file system IDs.
   </p><div id="id-1.11.6.3.10.6.3" data-id-title="Partitions" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Partitions</div><p>
     Before you begin, choose the partitions you wish to use for back-end
     storage. The partitions do not have to be formatted—the iSCSI client
     can format them when connected, overwriting all existing formatting.
    </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start YaST and launch <span class="guimenu">Network Services</span> › <span class="guimenu">iSCSI LIO Target</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Targets</span> tab.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/lio_targets_a.png"><img src="images/lio_targets_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Click <span class="guimenu">Add</span>, then define a new iSCSI LIO target group
      and devices:
     </p><p>
      The iSCSI LIO Target software automatically completes the
      <span class="guimenu">Target</span>, <span class="guimenu">Identifier</span>, <span class="guimenu">Portal
      Group</span>, <span class="guimenu">IP Address</span>, and <span class="guimenu">Port
      Number</span> fields. <span class="guimenu">Use Authentication</span> is
      selected by default.
     </p><ol type="a" class="substeps"><li class="step"><p>
        If you have multiple network interfaces, use the IP address drop-down
        box to select the IP address of the network interface to use for this
        target group. To make the server accessible under all addresses, choose
        <span class="guimenu">Bind All IP Addresses</span>.
       </p></li><li class="step"><p>
        Deselect <span class="guimenu">Use Authentication</span> if you do not want to
        require initiator authentication for this target group (not
        recommended).
       </p></li><li class="step"><p>
        Click <span class="guimenu">Add</span>. Enter the path of the device or partition
        or <span class="guimenu">Browse</span> to add it. Optionally specify a name, then
        click <span class="guimenu">OK</span>. The LUN number is automatically generated,
        beginning with 0. A name is automatically generated if you leave the
        field empty.
       </p></li><li class="step"><p>
        (Optional) Repeat the previous steps to add targets to this target
        group.
       </p></li><li class="step"><p>
        After all desired targets have been added to the group, click
        <span class="guimenu">Next</span>.
       </p></li></ol></li><li class="step"><p>
      On the <span class="guimenu">Modify iSCSI Target Initiator Setup</span> page,
      configure information for the initiators that are permitted to access
      LUNs in the target group:
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/lio_target_client_a.png"><img src="images/lio_target_client_a.png" width="100%" alt="Image" title="Image"/></a></div></div><p>
      After you specify at least one initiator for the target group, the
      <span class="guimenu">Edit LUN</span>, <span class="guimenu">Edit Auth</span>,
      <span class="guimenu">Delete</span>, and <span class="guimenu">Copy</span> buttons are
      enabled. You can use <span class="guimenu">Add</span> or <span class="guimenu">Copy</span> to
      add initiators for the target group:
     </p><div class="itemizedlist" id="il-iscsi-target-target-group-options" data-id-title="Modify iSCSI target: options"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Modify iSCSI target: options </span></span><a title="Permalink" class="permalink" href="#il-iscsi-target-target-group-options">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Add:</span>
         Add a new initiator entry for the selected iSCSI LIO target group.
        </p></li><li class="listitem"><p><span class="formalpara-title">Edit LUN:</span>
         Configure which LUNs in the iSCSI LIO target group to map to a
         selected initiator. You can map each of the allocated targets to a
         preferred initiator.
        </p></li><li class="listitem"><p><span class="formalpara-title">Edit auth:</span>
         Configure the preferred authentication method for a selected
         initiator. You can specify no authentication, or you can configure
         incoming authentication, outgoing authentication, or both.
        </p></li><li class="listitem"><p><span class="formalpara-title">Delete:</span>
         Remove a selected initiator entry from the list of initiators
         allocated to the target group.
        </p></li><li class="listitem"><p><span class="formalpara-title">Copy:</span>
         Add a new initiator entry with the same LUN mappings and
         authentication settings as a selected initiator entry. This allows you
         to easily allocate the same shared LUNs, in turn, to each node in a
         cluster.
        </p></li></ul></div><ol type="a" class="substeps"><li class="step"><p>
        Click <span class="guimenu">Add</span>, specify the initiator name, select or
        deselect the <span class="guimenu">Import LUNs from TPG</span> check box, then
        click <span class="guimenu">OK</span> to save the settings.
       </p></li><li class="step"><p>
        Select an initiator entry, click <span class="guimenu">Edit LUN</span>, modify
        the LUN mappings to specify which LUNs in the iSCSI LIO target group to
        allocate to the selected initiator, then click <span class="guimenu">OK</span> to
        save the changes.
       </p><p>
        If the iSCSI LIO target group consists of multiple LUNs, you can
        allocate one or multiple LUNs to the selected initiator. By default,
        each of the available LUNs in the group are assigned to an initiator
        LUN.
       </p><p>
        To modify the LUN allocation, perform one or more of the following
        actions:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Add:</span>
           Click <span class="guimenu">Add</span> to create a new <span class="guimenu">Initiator
           LUN</span> entry, then use the <span class="guimenu">Change</span>
           drop-down box to map a target LUN to it.
          </p></li><li class="listitem"><p><span class="formalpara-title">Delete:</span>
           Select the <span class="guimenu">Initiator LUN</span> entry, then click
           <span class="guimenu">Delete</span> to remove a target LUN mapping.
          </p></li><li class="listitem"><p><span class="formalpara-title">Change:</span>
           Select the <span class="guimenu">Initiator LUN</span> entry, then use the
           <span class="guimenu">Change</span> drop-down box to select which Target LUN
           to map to it.
          </p></li></ul></div><p>
        Typical allocation plans include the following:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          A single server is listed as an initiator. All of the LUNs in the
          target group are allocated to it.
         </p><p>
          You can use this grouping strategy to logically group the iSCSI SAN
          storage for a given server.
         </p></li><li class="listitem"><p>
          Multiple independent servers are listed as initiators. One or
          multiple target LUNs are allocated to each server. Each LUN is
          allocated to only one server.
         </p><p>
          You can use this grouping strategy to logically group the iSCSI SAN
          storage for a given department or service category in the data
          center.
         </p></li><li class="listitem"><p>
          Each node of a cluster is listed as an initiator. All of the shared
          target LUNs are allocated to each node. All nodes are attached to the
          devices, but for most file systems, the cluster software locks a
          device for access and mounts it on only one node at a time. Shared
          file systems (such as OCFS2) make it possible for multiple nodes to
          concurrently mount the same file structure and to open the same files
          with read and write access.
         </p><p>
          You can use this grouping strategy to logically group the iSCSI SAN
          storage for a given server cluster.
         </p></li></ul></div></li><li class="step"><p>
        Select an initiator entry, click <span class="guimenu">Edit Auth</span>, specify
        the authentication settings for the initiator, then click
        <span class="guimenu">OK</span> to save the settings.
       </p><p>
        You can require <span class="guimenu">No Discovery Authentication</span>, or you
        can configure <span class="guimenu">Authentication by Initiators</span>,
        <span class="guimenu">Outgoing Authentication</span>, or both. You can specify
        only one user name and password pair for each initiator. The
        credentials can be different for incoming and outgoing authentication
        for an initiator. The credentials can be different for each initiator.
       </p></li><li class="step"><p>
        Repeat the previous steps for each iSCSI initiator that can access this
        target group.
       </p></li><li class="step"><p>
        After the initiator assignments are configured, click
        <span class="guimenu">Next</span>.
       </p></li></ol></li><li class="step"><p>
      Click <span class="guimenu">Finish</span> to save and apply the settings.
     </p></li></ol></div></div></section><section class="sect2" id="sec-iscsi-target-tg-modify" data-id-title="Modifying an iSCSI LIO target group"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.5 </span><span class="title-name">Modifying an iSCSI LIO target group</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target-tg-modify">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can modify an existing iSCSI LIO target group as follows:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Add or remove target LUN devices from a target group
     </p></li><li class="listitem"><p>
      Add or remove initiators for a target group
     </p></li><li class="listitem"><p>
      Modify the initiator LUN-to-target LUN mappings for an initiator of a
      target group
     </p></li><li class="listitem"><p>
      Modify the user name and password credentials for an initiator
      authentication (incoming, outgoing, or both)
     </p></li></ul></div><p>
    To view or modify the settings for an iSCSI LIO target group:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start YaST and launch <span class="guimenu">Network Services</span> › <span class="guimenu">iSCSI LIO Target</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Targets</span> tab.
     </p></li><li class="step"><p>
      Select the iSCSI LIO target group to be modified, then click
      <span class="guimenu">Edit</span>.
     </p></li><li class="step"><p>
      On the Modify iSCSI Target LUN Setup page, add LUNs to the target group,
      edit the LUN assignments, or remove target LUNs from the group. After all
      desired changes have been made to the group, click
      <span class="guimenu">Next</span>.
     </p><p>
      For option information, see
      <a class="xref" href="#il-iscsi-target-target-group-options" title="Modify iSCSI target: options">Modify iSCSI target: options</a>.
     </p></li><li class="step"><p>
      On the Modify iSCSI Target Initiator Setup page, configure information
      for the initiators that are permitted to access LUNs in the target group.
      After all desired changes have been made to the group, click
      <span class="guimenu">Next</span>.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Finish</span> to save and apply the settings.
     </p></li></ol></div></div></section><section class="sect2" id="sec-iscsi-target-tg-delete" data-id-title="Deleting an iSCSI LIO target group"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.6 </span><span class="title-name">Deleting an iSCSI LIO target group</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-target-tg-delete">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Deleting an iSCSI LIO target group removes the definition of the group, and
    the related setup for initiators, including LUN mappings and authentication
    credentials. It does not destroy the data on the partitions. To give
    initiators access again, you can allocate the target LUNs to a different or
    new target group, and configure the initiator access for them.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start YaST and launch <span class="guimenu">Network Services</span> › <span class="guimenu">iSCSI LIO Target</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Targets</span> tab.
     </p></li><li class="step"><p>
      Select the iSCSI LIO target group to be deleted, then click
      <span class="guimenu">Delete</span>.
     </p></li><li class="step"><p>
      When you are prompted, click <span class="guimenu">Continue</span> to confirm the
      deletion, or click <span class="guimenu">Cancel</span> to cancel it.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Finish</span> to save and apply the settings.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-iscsi-initiator" data-id-title="Configuring iSCSI initiator"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.3 </span><span class="title-name">Configuring iSCSI initiator</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The iSCSI initiator can be used to connect to any iSCSI target. This is not
   restricted to the iSCSI target solution explained in
   <a class="xref" href="#sec-iscsi-target" title="15.2. Setting up an iSCSI LIO target server">Section 15.2, “Setting up an iSCSI LIO target server”</a>. The
   configuration of iSCSI initiator involves two major steps: the discovery of
   available iSCSI targets and the setup of an iSCSI session. Both can be done
   with YaST.
  </p><section class="sect2" id="sec-iscsi-initiator-yast" data-id-title="Using YaST for the iSCSI initiator configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.3.1 </span><span class="title-name">Using YaST for the iSCSI initiator configuration</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The iSCSI Initiator Overview in YaST is divided into three tabs:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.3.11.3.3.1"><span class="term">Service:</span></dt><dd><p>
       The <span class="guimenu">Service</span> tab can be used to enable the iSCSI
       initiator at boot time. It also offers to set a unique
       <span class="guimenu">Initiator Name</span> and an iSNS server to use for the
       discovery.
      </p></dd><dt id="id-1.11.6.3.11.3.3.2"><span class="term">Connected targets:</span></dt><dd><p>
       The <span class="guimenu">Connected Targets</span> tab gives an overview of the
       currently connected iSCSI targets. Like the <span class="guimenu">Discovered
       Targets</span> tab, it also gives the option to add new targets to
       the system.
      </p></dd><dt id="id-1.11.6.3.11.3.3.3"><span class="term">Discovered targets:</span></dt><dd><p>
       The <span class="guimenu">Discovered Targets</span> tab provides the possibility
       of manually discovering iSCSI targets in the network.
      </p></dd></dl></div><section class="sect3" id="sec-iscsi-initiator-yast-configuration" data-id-title="Configuring the iSCSI initiator"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.3.1.1 </span><span class="title-name">Configuring the iSCSI initiator</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator-yast-configuration">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Start YaST and launch <span class="guimenu">Network Services</span> › <span class="guimenu">iSCSI Initiator</span>.
      </p></li><li class="step"><p>
       Switch to the <span class="guimenu">Service</span> tab.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/iscsi_init_service_a.png"><img src="images/iscsi_init_service_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Under <span class="guimenu">After writing configuration</span>, define what to do
       if there is a configuration change. Bear in mind that available options
       depend on the service current status.
      </p><p>
       The <span class="guimenu">Keep current state</span> option keeps the service in
       the same status.
      </p></li><li class="step"><p>
       In the <span class="guimenu">After reboot</span> menu specify action that will
       take place after reboot:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="guimenu">Start on boot</span> - to start the service automatically
         on boot.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Start on demand</span> - the associated socket will be
         running and if needed, the service will be started.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Do not start</span> - the service is not started
         automatically.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Keep current settings</span> - the service configuration
         is not changed.
        </p></li></ul></div></li><li class="step"><p>
       Specify or verify the <span class="guimenu">Initiator Name</span>.
      </p><p>
       Specify a well-formed iSCSI qualified name (IQN) for the iSCSI initiator
       on this server. The initiator name must be globally unique on your
       network. The IQN uses the following general format:
      </p><div class="verbatim-wrap"><pre class="screen">iqn.yyyy-mm.com.mycompany:n1:n2</pre></div><p>
       where n1 and n2 are alphanumeric characters. For example:
      </p><div class="verbatim-wrap"><pre class="screen">iqn.1996-04.de.suse:01:a5dfcea717a</pre></div><p>
       The <span class="guimenu">Initiator Name</span> is automatically completed with
       the corresponding value from the
       <code class="filename">/etc/iscsi/initiatorname.iscsi</code> file on the server.
      </p><p>
       If the server has iBFT (iSCSI Boot Firmware Table) support, the
       <span class="guimenu">Initiator Name</span> is completed with the corresponding
       value in the IBFT, and you are not able to change the initiator name in
       this interface. Use the BIOS Setup to modify it instead. The iBFT is a
       block of information containing various parameters useful to the iSCSI
       boot process, including iSCSI target and initiator descriptions for the
       server.
      </p></li><li class="step"><p>
       Use either of the following methods to discover iSCSI targets on the
       network.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">iSNS:</span>
          To use iSNS (Internet Storage Name Service) for discovering iSCSI
          targets, continue with
          <a class="xref" href="#sec-iscsi-initiator-yast-isn" title="15.3.1.2. Discovering iSCSI targets by using iSNS">Section 15.3.1.2, “Discovering iSCSI targets by using iSNS”</a>.
         </p></li><li class="listitem"><p><span class="formalpara-title">Discovered targets:</span>
          To discover iSCSI target devices manually, continue with
          <a class="xref" href="#sec-iscsi-initiator-yast-discovered" title="15.3.1.3. Discovering iSCSI targets manually">Section 15.3.1.3, “Discovering iSCSI targets manually”</a>.
         </p></li></ul></div></li></ol></div></div></section><section class="sect3" id="sec-iscsi-initiator-yast-isn" data-id-title="Discovering iSCSI targets by using iSNS"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.3.1.2 </span><span class="title-name">Discovering iSCSI targets by using iSNS</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator-yast-isn">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Before you can use this option, you must have already installed and
     configured an iSNS server in your environment. For information, see
     <a class="xref" href="#cha-isns" title="Chapter 14. iSNS for Linux">Chapter 14, <em>iSNS for Linux</em></a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In YaST, select<span class="guimenu"> iSCSI Initiator</span>, then select the
       <span class="guimenu">Service</span> tab.
      </p></li><li class="step"><p>
       Specify the IP address of the iSNS server and port. The default port is
       3205.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save and apply your changes.
      </p></li></ol></div></div></section><section class="sect3" id="sec-iscsi-initiator-yast-discovered" data-id-title="Discovering iSCSI targets manually"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.3.1.3 </span><span class="title-name">Discovering iSCSI targets manually</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator-yast-discovered">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Repeat the following process for each of the iSCSI target servers that you
     want to access from the server where you are setting up the iSCSI
     initiator.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In YaST, select <span class="guimenu">iSCSI Initiator</span>, then select the
       <span class="guimenu">Discovered Targets</span> tab.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Discovery</span> to open the iSCSI Initiator Discovery
       dialog.
      </p></li><li class="step"><p>
       Enter the IP address and change the port if needed. The default port is
       3260.
      </p></li><li class="step"><p>
       If authentication is required, deselect <span class="guimenu">No Discovery
       Authentication</span>, then specify the credentials for
       <span class="guimenu">Authentication by Initiator</span> or
       <span class="guimenu">Authentication by Targets</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Next</span> to start the discovery and connect to the
       iSCSI target server.
      </p></li><li class="step"><p>
       If credentials are required, after a successful discovery, use
       <span class="guimenu">Connect</span> to activate the target.
      </p><p>
       You are prompted for authentication credentials to use the selected
       iSCSI target.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Next</span> to finish the configuration.
      </p><p>
       The target now appears in <span class="guimenu">Connected Targets</span> and the
       virtual iSCSI device is now available.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save and apply your changes.
      </p></li><li class="step"><p>
       You can find the local device path for the iSCSI target device by using
       the <code class="command">lsscsi</code> command.
      </p></li></ol></div></div></section><section class="sect3" id="sec-iscsi-initiator-yast-startup" data-id-title="Setting the start-up preference for iSCSI target devices"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.3.1.4 </span><span class="title-name">Setting the start-up preference for iSCSI target devices</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator-yast-startup">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In YaST, select<span class="guimenu"> iSCSI Initiator</span>, then select the
       <span class="guimenu">Connected Targets</span> tab to view a list of the iSCSI
       target devices that are currently connected to the server.
      </p></li><li class="step"><p>
       Select the iSCSI target device that you want to manage.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Toggle Start-Up</span> to modify the setting:
      </p><p><span class="formalpara-title">Automatic:</span>
        This option is used for iSCSI targets that are to be connected when the
        iSCSI service itself starts up. This is the typical configuration.
       </p><p><span class="formalpara-title">Onboot:</span>
        This option is used for iSCSI targets that are to be connected during
        boot; that is, when root (<code class="filename">/</code>) is on iSCSI. As such,
        the iSCSI target device will be evaluated from the initrd on server
        boots. This option is ignored on platforms that cannot boot from iSCSI,
        such as IBM Z. Therefore it should not be used on these platforms;
        use <span class="guimenu">Automatic</span> instead.
       </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save and apply your changes.
      </p></li></ol></div></div></section></section><section class="sect2" id="sec-iscsi-initiator-manually" data-id-title="Setting up the iSCSI initiator manually"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.3.2 </span><span class="title-name">Setting up the iSCSI initiator manually</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator-manually">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Both the discovery and the configuration of iSCSI connections require a
    running iscsid. When running the discovery the first time, the internal
    database of the iSCSI initiator is created in the directory
    <code class="filename">/etc/iscsi/</code>.
   </p><p>
    If your discovery is password protected, provide the authentication
    information to iscsid. Because the internal database does not exist when
    doing the first discovery, it cannot be used now. Instead, the
    configuration file <code class="filename">/etc/iscsid.conf</code> must be edited to
    provide the information. To add your password information for the
    discovery, add the following lines to the end of
    <code class="filename">/etc/iscsid.conf</code>:
   </p><div class="verbatim-wrap"><pre class="screen">discovery.sendtargets.auth.authmethod = CHAP
discovery.sendtargets.auth.username = <em class="replaceable">USERNAME</em>
discovery.sendtargets.auth.password = <em class="replaceable">PASSWORD</em></pre></div><p>
    The discovery stores all received values in an internal persistent
    database. In addition, it displays all detected targets. Run this discovery
    with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> iscsiadm <code class="option">-m discovery --type=st --portal=<em class="replaceable">TARGET_IP</em></code></pre></div><p>
    The output should look like the following:
   </p><div class="verbatim-wrap"><pre class="screen">10.44.171.99:3260,1 iqn.2006-02.com.example.iserv:systems</pre></div><p>
    To discover the available targets on an <code class="literal">iSNS</code> server, use
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen">sudo iscsiadm --mode discovery --type isns --portal <em class="replaceable">TARGET_IP</em></pre></div><p>
    For each target defined on the iSCSI target, one line appears. For more
    information about the stored data, see
    <a class="xref" href="#sec-iscsi-initiator-database" title="15.3.3. The iSCSI initiator databases">Section 15.3.3, “The iSCSI initiator databases”</a>.
   </p><p>
    The special <code class="option">--login</code> option of <code class="command">iscsiadm</code>
    creates all needed devices:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> iscsiadm -m node -n iqn.2006-02.com.example.iserv:systems --login</pre></div><p>
    The newly generated devices show up in the output of
    <code class="command">lsscsi</code> and can now be mounted.
   </p></section><section class="sect2" id="sec-iscsi-initiator-database" data-id-title="The iSCSI initiator databases"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.3.3 </span><span class="title-name">The iSCSI initiator databases</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-initiator-database">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    All information that was discovered by the iSCSI initiator is stored in two
    database files that reside in <code class="filename">/etc/iscsi</code>. There is one
    database for the discovery of targets and one for the discovered nodes.
    When accessing a database, you first must select if you want to get your
    data from the discovery or from the node database. Do this with the
    <code class="option">-m discovery</code> and <code class="option">-m node</code> parameters of
    <code class="command">iscsiadm</code>. Using <code class="command">iscsiadm</code> with one of
    these parameters gives an overview of the stored records:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> iscsiadm -m discovery
10.44.171.99:3260,1 iqn.2006-02.com.example.iserv:systems</pre></div><p>
    The target name in this example is
    <code class="literal">iqn.2006-02.com.example.iserv:systems</code>. This name is
    needed for all actions that relate to this special data set. To examine the
    content of the data record with the ID
    <code class="literal">iqn.2006-02.com.example.iserv:systems</code>, use the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> iscsiadm -m node --targetname iqn.2006-02.com.example.iserv:systems
node.name = iqn.2006-02.com.example.iserv:systems
node.transport_name = tcp
node.tpgt = 1
node.active_conn = 1
node.startup = manual
node.session.initial_cmdsn = 0
node.session.reopen_max = 32
node.session.auth.authmethod = CHAP
node.session.auth.username = joe
node.session.auth.password = ********
node.session.auth.username_in = <em class="replaceable">EMPTY</em>
node.session.auth.password_in = <em class="replaceable">EMPTY</em>
node.session.timeo.replacement_timeout = 0
node.session.err_timeo.abort_timeout = 10
node.session.err_timeo.reset_timeout = 30
node.session.iscsi.InitialR2T = No
node.session.iscsi.ImmediateData = Yes
....</pre></div><p>
    To edit the value of one of these variables, use the command
    <code class="command">iscsiadm</code> with the <code class="option">update</code> operation. For
    example, if you want iscsid to log in to the iSCSI target when it
    initializes, set the variable <code class="option">node.startup</code> to the value
    <code class="option">automatic</code>:
   </p><div class="verbatim-wrap"><pre class="screen">sudo iscsiadm -m node -n iqn.2006-02.com.example.iserv:systems \
-p ip:port --op=update --name=node.startup --value=automatic</pre></div><p>
    Remove obsolete data sets with the <code class="literal">delete</code> operation. If
    the target <code class="literal">iqn.2006-02.com.example.iserv:systems</code> is no
    longer a valid record, delete this record with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> iscsiadm -m node -n iqn.2006-02.com.example.iserv:systems \
-p ip:port --op=delete</pre></div><div id="id-1.11.6.3.11.5.10" data-id-title="No confirmation" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No confirmation</div><p>
     Use this option with caution because it deletes the record without any
     additional confirmation prompt.
    </p></div><p>
    To get a list of all discovered targets, run the <code class="command">sudo iscsiadm -m
    node</code> command.
   </p></section></section><section class="sect1" id="sec-iscsi-targetcli" data-id-title="Setting up software targets using targetcli-fb"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.4 </span><span class="title-name">Setting up software targets using targetcli-fb</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-targetcli">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="systemitem">targetcli</code> is a shell for managing the configuration
   of the LinuxIO (LIO) target subsystem. The shell can be called
   interactively, or you can execute one command at a time, much like a
   conventional shell. Similar to a conventional shell, you can traverse the
   targetcli functional hierarchy using the <code class="command">cd</code> command and
   list contents with the <code class="command">ls</code> command.
  </p><p>
   The available commands depend on the current directory. While each directory
   has its own set of commands, there are also commands that are available in
   all directories (for example, the <code class="command">cd</code> and
   <code class="command">ls</code> commands).
  </p><p>
   <code class="systemitem">targetcli</code> commands have the following format:
  </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">[DIRECTORY]</em> <code class="command">command</code> <em class="replaceable">[ARGUMENTS]</em></pre></div><p>
   You can use the <code class="command">help</code> command in any directory to view a
   list of available commands or information about any command in particular.
  </p><p>
   The <code class="systemitem">targetcli</code> tool is part of the
   <span class="package">targetcli-fb</span> package. This package is available in the
   official <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> software repository, and it can be installed using
   the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper install targetcli-fb</pre></div><p>
   After the <span class="package">targetcli-fb</span> package has been installed, enable
   the <code class="literal">targetcli</code> service:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl enable targetcli
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl start targetcli</pre></div><p>
   To switch to the targetcli shell, run the <code class="command">targetcli</code> as
   root:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> targetcli</pre></div><p>
   You can then run the <code class="command">ls</code> command to see the default
   configuration.
  </p><div class="verbatim-wrap"><pre class="screen">/&gt; ls
o- / ............................ [...]
  o- backstores ................. [...]
  | o- block ..... [Storage Objects: 0]
  | o- fileio .... [Storage Objects: 0]
  | o- pscsi ..... [Storage Objects: 0]
  | o- ramdisk ... [Storage Objects: 0]
  | o- rbd ....... [Storage Objects: 0]
  o- iscsi ............... [Targets: 0]
  o- loopback ............ [Targets: 0]
  o- vhost ............... [Targets: 0]
  o- xen-pvscsi .......... [Targets: 0]
/&gt;</pre></div><p>
   As the output of the <code class="command">ls</code> command indicates, there are no
   configured back-ends. So the first step is to configure one of the supported
   software targets.
  </p><p>
   targetcli supports the following back-ends:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">fileio</code>, local image file
    </p></li><li class="listitem"><p>
     <code class="literal">block</code>, block storage on a dedicated disk or partition
    </p></li><li class="listitem"><p>
     <code class="literal">pscsi</code>, SCSI pass-through devices
    </p></li><li class="listitem"><p>
     <code class="literal">ramdisk</code>, memory-based back-end
    </p></li><li class="listitem"><p>
     <code class="literal">rbd</code>, Ceph RADOS block devices
    </p></li></ul></div><p>
   To familiarize yourself with the functionality of targetcli, set up a local
   image file as a software target using the <code class="command">create</code> command:
  </p><div class="verbatim-wrap"><pre class="screen">/backstores/fileio create test-disc <em class="replaceable">/alt</em>/test.img 1G</pre></div><p>
   This creates a 1 GB <code class="filename">test.img</code> image in the
   specified location (in this case <code class="filename">/alt</code>). Run
   <code class="command">ls</code>, and you should see the following result:
  </p><div class="verbatim-wrap"><pre class="screen">/&gt; ls
o- / ........................................................... [...]
  o- backstores ................................................ [...]
  | o- block .................................... [Storage Objects: 0]
  | o- fileio ................................... [Storage Objects: 1]
  | | o- test-disc ... [/alt/test.img (1.0GiB) write-back deactivated]
  | |   o- alua ......     .......................... [ALUA Groups: 1]
  | |     o- default_tg_pt_gp      .... [ALUA state: Active/optimized]
  | o- pscsi .................................... [Storage Objects: 0]
  | o- ramdisk .................................. [Storage Objects: 0]
  | o- rbd ...................................... [Storage Objects: 0]
  o- iscsi .............................................. [Targets: 0]
  o- loopback ........................................... [Targets: 0]
  o- vhost .............................................. [Targets: 0]
  o- xen-pvscsi ......................................... [Targets: 0]
/&gt;</pre></div><p>
   The output indicates that there is now a file-based backstore, under the
   <code class="filename">/backstores/fileio</code> directory, called
   <code class="literal">test-disc</code>, which is linked to the created file
   <code class="filename">/alt/test.img</code>. Note that the new backstore is not yet
   activated.
  </p><p>
   The next step is to connect an iSCSI target front-end to the back-end
   storage. Each target must have an <code class="literal">IQN</code> (iSCSI Qualified
   Name). The most commonly used IQN format is as follows:
  </p><div class="verbatim-wrap"><pre class="screen">iqn.<em class="replaceable">YYYY-MM.NAMING-AUTHORITY:UNIQUE-NAME</em></pre></div><p>
   The following parts of an IQN are required:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <em class="replaceable">YYYY-MM</em>, the year and month when the naming
     authority was established
    </p></li><li class="listitem"><p>
     <em class="replaceable">NAMING-AUTHORITY</em>, reverse-syntax of the
     Internet Domain Name of the naming authority
    </p></li><li class="listitem"><p>
     <em class="replaceable">UNIQUE-NAME</em>, a domain-unique name chosen by the
     naming authority
    </p></li></ul></div><p>
   For example, for the domain <code class="literal">open-iscsi.com</code>, the IQN can
   be as follows:
  </p><div class="verbatim-wrap"><pre class="screen">iqn.2005-03.com.open-iscsi:<em class="replaceable">UNIQUE-NAME</em></pre></div><p>
   When creating an iSCSI target, the <code class="command">targetcli</code> command
   allows you to assign your own IQN, as long as it follows the specified
   format. You can also let the command create an IQN for you by omitting a
   name when creating the target, for example:
  </p><div class="verbatim-wrap"><pre class="screen">/&gt; iscsi/ create</pre></div><p>
   Run the <code class="command">ls</code> command again:
  </p><div class="verbatim-wrap"><pre class="screen">/&gt; ls
o- / ............................................................... [...]
  o- backstores .................................................... [...]
  | o- block ........................................ [Storage Objects: 0]
  | o- fileio ....................................... [Storage Objects: 1]
  | | o- test-disc ....... [/alt/test.img (1.0GiB) write-back deactivated]
  | |   o- alua ......................................... [ALUA Groups: 1]
  | |     o- default_tg_pt_gp ............. [ALUA state: Active/optimized]
  | o- pscsi ........................................ [Storage Objects: 0]
  | o- ramdisk ...................................... [Storage Objects: 0]
  | o- rbd .......................................... [Storage Objects: 0]
  o- iscsi .................................................. [Targets: 1]
  | o- iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456 ... [TPGs: 1]
  |   o- tpg1 ..................................... [no-gen-acls, no-auth]
  |     o- acls ................................................ [ACLs: 0]
  |     o- luns ................................................ [LUNs: 0]
  |     o- portals .......................................... [Portals: 1]
  |       o- 0.0.0.0:3260 ........................................... [OK]
  o- loopback ............................................... [Targets: 0]
  o- vhost .................................................. [Targets: 0]
  o- xen-pvscsi ............................................. [Targets: 0]
/&gt;</pre></div><p>
   The output shows the created iSCSI target node with its automatically
   generated IQN
   <code class="literal">iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456</code>
  </p><p>
   Note that <code class="command">targetcli</code> has also created and enabled the
   default target portal group <code class="literal">tpg1</code>. This is done because
   the variables <code class="literal">auto_add_default_portal</code> and
   <code class="literal">auto_enable_tpgt</code> at the root level are set to
   <code class="literal">true</code> by default.
  </p><p>
   The command also created the default portal with the
   <code class="literal">0.0.0.0</code> IPv4 wildcard. This means that any IPv4 address
   can access the configured target.
  </p><p>
   The next step is to create a LUN (Logical Unit Number) for the iSCSI target.
   The best way to do this is to let <code class="command">targetcli</code> assign its
   name and number automatically. Switch to the directory of the iSCSI target,
   and then use the <code class="command">create</code> command in the
   <code class="filename">lun</code> directory to assign a LUN to the backstore.
  </p><div class="verbatim-wrap"><pre class="screen">/&gt; cd /iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456/
/iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456&gt; cd tpg1
/iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456/tpg1&gt; luns/
create /backstores/fileio/test-disc</pre></div><p>
   Run the <code class="command">ls</code> command to see the changes:
  </p><div class="verbatim-wrap"><pre class="screen">/iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456/tpg1&gt; ls
o- tpg1 .............................................. [no-gen-acls, no-auth]
      o- acls ..................................................... [ACLs: 0]
      o- luns ..................................................... [LUNs: 1]
      | o- lun0 ....... [fileio/test-disc (/alt/test.img) (default_tg_pt_gp)]
      o- portals ............................................... [Portals: 1]
        o- 0.0.0.0:3260 ................................................ [OK]</pre></div><p>
   There is now an iSCSI target that has a 1 GB file-based backstore. The
   target has the
   <code class="literal">iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456</code>
   name, and it can be accessed from any network port of the system.
  </p><p>
   Finally, you need to ensure that initiators have access to the configured
   target. One way to do this is to create an ACL rule for each initiator that
   allows them to connect to the target. In this case, you must list each
   desired initiator using its IQN. The IQNs of the existing initiators can be
   found in the <code class="filename">/etc/iscsi/initiatorname.iscsi</code> file. Use
   the following command to add the desired initiator (in this case, it's
   <code class="literal">iqn.1996-04.de.suse:01:54cab487975b</code>):
  </p><div class="verbatim-wrap"><pre class="screen">/iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456/tpg1&gt; acls/ create iqn.1996-04.de.suse:01:54cab487975b
Created Node ACL for iqn.1996-04.de.suse:01:54cab487975b
Created mapped LUN 0.
/iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456/tpg1&gt;</pre></div><p>
   Alternatively, you can run the target in a demo mode with no access
   restrictions. This method is less secure, but it can be useful for
   demonstration purposes and running on closed networks. To enable the demo
   mode, use the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">/iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456/tpg1&gt; set attribute generate_node_acls=1
/iscsi/iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456/tpg1&gt; set attribute demo_mode_write_protect=0</pre></div><p>
   The last step is to save the created configuration using the
   <code class="command">saveconfig</code> command available in the root directory:
  </p><div class="verbatim-wrap"><pre class="screen">/&gt; saveconfig /etc/target/example.json</pre></div><p>
   If at some point you need to restore configuration from the saved file, you
   need to clear the current configuration first. Keep in mind that clearing
   the current configuration results in data loss unless you save your
   configuration first. Use the following command to clear and reload the
   configuration:
  </p><div class="verbatim-wrap"><pre class="screen">/&gt; clearconfig
As a precaution, confirm=True needs to be set
/&gt; clearconfig confirm=true
All configuration cleared
/&gt; restoreconfig /etc/target/example.json
Configuration restored from /etc/target/example.json
/&gt;</pre></div><p>
   To test whether the configured target is working, connect to it using the
   <span class="package">open-iscsi</span> iSCSI initiator installed on the same system
   (replace <em class="replaceable">HOSTNAME</em> with the host name of the local
   machine):
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>iscsiadm -m discovery -t st -p <em class="replaceable">HOSTNAME</em></pre></div><p>
   This command returns a list of found targets, for example:
  </p><div class="verbatim-wrap"><pre class="screen">192.168.20.3:3260,1 iqn.2003-01.org.linux-iscsi.e83.x8664:sn.8b35d04dd456</pre></div><p>
   You can then connect to the listed target using the <code class="command">login</code>
   iSCSI command. This makes the target available as a local disk.
  </p></section><section class="sect1" id="sec-iscsi-installation" data-id-title="Using iSCSI disks when installing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.5 </span><span class="title-name">Using iSCSI disks when installing</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Booting from an iSCSI disk on AMD64/Intel 64 and IBM POWER architectures
   is supported when iSCSI-enabled firmware is used.
  </p><p>
   To use iSCSI disks during installation, it is necessary to add the following
   parameter to the boot parameter line:
  </p><div class="verbatim-wrap"><pre class="screen">withiscsi=1</pre></div><p>
   During installation, an additional screen appears that provides the option
   to attach iSCSI disks to the system and use them in the installation
   process.
  </p><div id="id-1.11.6.3.13.6" data-id-title="Mount point support" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Mount point support</div><p>
    iSCSI devices will appear asynchronously during the boot process. While the
    initrd guarantees that those devices are set up correctly for the root file
    system, there are no such guarantees for any other file systems or mount
    points like <code class="filename">/usr</code>. Hence any system mount points like
    <code class="filename">/usr</code> or <code class="filename">/var</code> are not supported.
    To use those devices, ensure correct synchronization of the respective
    services and devices.
   </p></div></section><section class="sect1" id="sec-iscsi-trouble" data-id-title="Troubleshooting iSCSI"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.6 </span><span class="title-name">Troubleshooting iSCSI</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section describes some known issues and possible solutions for iSCSI
   target and iSCSI initiator issues.
  </p><section class="sect2" id="sec-iscsi-trouble-no-service" data-id-title="Portal error when setting up target LUNs on an iSCSI LIO target server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.6.1 </span><span class="title-name">Portal error when setting up target LUNs on an iSCSI LIO target server</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble-no-service">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When adding or editing an iSCSI LIO target group, you get an error:
   </p><div class="verbatim-wrap"><pre class="screen">Problem setting network portal <em class="replaceable">IP_ADDRESS</em>:3260</pre></div><p>
    The <code class="filename">/var/log/YasT2/y2log</code> log file contains the
    following error:
   </p><div class="verbatim-wrap"><pre class="screen">find: `/sys/kernel/config/target/iscsi': No such file or directory</pre></div><p>
    This problem occurs if the iSCSI LIO Target Server software is not
    currently running. To resolve this issue, exit YaST, manually start iSCSI
    LIO at the command line with <code class="command">systemctl start targetcli</code>,
    then try again.
   </p><p>
    You can also enter the following to check if <code class="command">configfs</code>,
    <code class="command">iscsi_target_mod</code>, and <code class="command">target_core_mod</code>
    are loaded. A sample response is shown.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> lsmod | grep iscsi
iscsi_target_mod      295015  0
target_core_mod       346745  4
iscsi_target_mod,target_core_pscsi,target_core_iblock,target_core_file
configfs               35817  3 iscsi_target_mod,target_core_mod
scsi_mod              231620  16
iscsi_target_mod,target_core_pscsi,target_core_mod,sg,sr_mod,mptctl,sd_mod,
scsi_dh_rdac,scsi_dh_emc,scsi_dh_alua,scsi_dh_hp_sw,scsi_dh,libata,mptspi,
mptscsih,scsi_transport_spi</pre></div></section><section class="sect2" id="sec-iscsi-trouble-not-visible" data-id-title="iSCSI LIO targets are not visible from other computers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.6.2 </span><span class="title-name">iSCSI LIO targets are not visible from other computers</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble-not-visible">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you use a firewall on the target server, you must open the iSCSI port
    that you are using to allow other computers to see the iSCSI LIO targets.
    For information, see <a class="xref" href="#sec-iscsi-target-start" title="15.2.1. iSCSI LIO target service start-up and firewall settings">Section 15.2.1, “iSCSI LIO target service start-up and firewall settings”</a>.
   </p></section><section class="sect2" id="sec-iscsi-trouble-package-loss" data-id-title="Data packets dropped for iSCSI traffic"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.6.3 </span><span class="title-name">Data packets dropped for iSCSI traffic</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble-package-loss">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A firewall might drop packets if it gets too busy. The default for the SUSE
    Firewall is to drop packets after three minutes. If you find that iSCSI
    traffic packets are being dropped, consider configuring the SUSE Firewall
    to queue packets instead of dropping them when it gets too busy.
   </p></section><section class="sect2" id="sec-iscsi-trouble-lvm" data-id-title="Using iSCSI volumes with LVM"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.6.4 </span><span class="title-name">Using iSCSI volumes with LVM</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble-lvm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Use the troubleshooting tips in this section when using LVM on iSCSI
    targets.
   </p><section class="sect3" id="sec-iscsi-trouble-lvm-boot-initiator" data-id-title="Check if the iSCSI initiator discovery occurs at boot"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.6.4.1 </span><span class="title-name">Check if the iSCSI initiator discovery occurs at boot</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble-lvm-boot-initiator">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     When you set up the iSCSI Initiator, ensure that you enable discovery at
     boot time so that udev can discover the iSCSI devices at boot time and set
     up the devices to be used by LVM.
    </p></section><section class="sect3" id="sec-iscsi-trouble-lvm-boot-target" data-id-title="Check that iSCSI target discovery occurs at boot"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.6.4.2 </span><span class="title-name">Check that iSCSI target discovery occurs at boot</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble-lvm-boot-target">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Remember that <code class="filename">udev</code> provides the default setup for
     devices. Ensure that all of the applications that create devices are
     started at boot time so that <code class="command">udev</code> can recognize and
     assign devices for them at system start-up. If the application or service
     is not started until later, <code class="command">udev</code> does not create the
     device automatically as it would at boot time.
    </p></section></section><section class="sect2" id="sec-iscsi-trouble-mount" data-id-title="iSCSI targets are mounted when the configuration file is set to manual"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.6.5 </span><span class="title-name">iSCSI targets are mounted when the configuration file is set to manual</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-trouble-mount">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When Open-iSCSI starts, it can mount the targets even if the
    <code class="literal">node.startup</code> option is set to manual in the
    <code class="filename">/etc/iscsi/iscsid.conf</code> file if you manually modified
    the configuration file.
   </p><p>
    Check the
    <code class="filename">/etc/iscsi/nodes/<em class="replaceable">TARGET_NAME</em>/<em class="replaceable">IP_ADDRESS</em>,<em class="replaceable">PORT</em>/default</code>
    file. It contains a <code class="literal">node.startup</code> setting that overrides
    the <code class="filename">/etc/iscsi/iscsid.conf</code> file. Setting the mount
    option to manual by using the YaST interface also sets
    <code class="literal">node.startup = manual</code> in the
    <code class="filename">/etc/iscsi/nodes/<em class="replaceable">TARGET_NAME</em>/<em class="replaceable">IP_ADDRESS</em>,<em class="replaceable">PORT</em>/default
    </code>files.
   </p></section></section><section class="sect1" id="sec-iscsi-terminology" data-id-title="iSCSI LIO target terminology"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.7 </span><span class="title-name">iSCSI LIO target terminology</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-terminology">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.3.15.2.1"><span class="term">backstore</span></dt><dd><p>
      A physical storage object that provides the actual storage underlying an
      iSCSI endpoint.
     </p></dd><dt id="id-1.11.6.3.15.2.2"><span class="term">CDB (command descriptor block)</span></dt><dd><p>
      The standard format for SCSI commands. CDBs are commonly 6, 10, or 12
      bytes long, though they can be 16 bytes or of variable length.
     </p></dd><dt id="id-1.11.6.3.15.2.3"><span class="term">CHAP (challenge handshake authentication protocol)</span></dt><dd><p>
      A point-to-point protocol (PPP) authentication method used to confirm the
      identity of one computer to another. After the Link Control Protocol
      (LCP) connects the two computers, and the CHAP method is negotiated, the
      authenticator sends a random Challenge to the peer. The peer issues a
      cryptographically hashed Response that depends upon the Challenge and a
      secret key. The authenticator verifies the hashed Response against its
      own calculation of the expected hash value, and either acknowledges the
      authentication or terminates the connection. CHAP is defined in the RFC
      1994.
     </p></dd><dt id="id-1.11.6.3.15.2.4"><span class="term">CID (connection identifier)</span></dt><dd><p>
      A 16‐bit number, generated by the initiator, that uniquely identifies a
      connection between two iSCSI devices. This number is presented during the
      login phase.
     </p></dd><dt id="id-1.11.6.3.15.2.5"><span class="term">endpoint</span></dt><dd><p>
      The combination of an iSCSI Target Name with an iSCSI TPG (IQN + Tag).
     </p></dd><dt id="id-1.11.6.3.15.2.6"><span class="term">EUI (extended unique identifier)</span></dt><dd><p>
      A 64‐bit number that uniquely identifies every device in the world. The
      format consists of 24 bits that are unique to a given company, and 40
      bits assigned by the company to each device it builds.
     </p></dd><dt id="id-1.11.6.3.15.2.7"><span class="term">initiator</span></dt><dd><p>
      The originating end of an SCSI session. Typically a controlling device
      such as a computer.
     </p></dd><dt id="id-1.11.6.3.15.2.8"><span class="term">IPS (Internet protocol storage)</span></dt><dd><p>
      The class of protocols or devices that use the IP protocol to move data
      in a storage network. FCIP (Fibre Channel over Internet Protocol), iFCP
      (Internet Fibre Channel Protocol), and iSCSI (Internet SCSI) are all
      examples of IPS protocols.
     </p></dd><dt id="id-1.11.6.3.15.2.9"><span class="term">IQN (iSCSI qualified name)</span></dt><dd><p>
      A name format for iSCSI that uniquely identifies every device in the
      world (for example:
      <code class="filename">iqn.5886.com.acme.tapedrive.sn‐a12345678</code>).
     </p></dd><dt id="id-1.11.6.3.15.2.10"><span class="term">ISID (initiator session identifier)</span></dt><dd><p>
      A 48‐bit number, generated by the initiator, that uniquely identifies a
      session between the initiator and the target. This value is created
      during the login process, and is sent to the target with a Login PDU.
     </p></dd><dt id="id-1.11.6.3.15.2.11"><span class="term">MCS (multiple connections per session)</span></dt><dd><p>
      A part of the iSCSI specification that allows multiple TCP/IP connections
      between an initiator and a target.
     </p></dd><dt id="id-1.11.6.3.15.2.12"><span class="term">MPIO (multipath I/O)</span></dt><dd><p>
      A method by which data can take multiple redundant paths between a server
      and storage.
     </p></dd><dt id="id-1.11.6.3.15.2.13"><span class="term">network portal</span></dt><dd><p>
      The combination of an iSCSI endpoint with an IP address plus a TCP
      (Transmission Control Protocol) port. TCP port 3260 is the port number
      for the iSCSI protocol, as defined by IANA (Internet Assigned Numbers
      Authority).
     </p></dd><dt id="id-1.11.6.3.15.2.14"><span class="term">SAM (SCSI architectural model)</span></dt><dd><p>
      A document that describes the behavior of SCSI in general terms, allowing
      for different types of devices communicating over various media.
     </p></dd><dt id="id-1.11.6.3.15.2.15"><span class="term">target</span></dt><dd><p>
      The receiving end of an SCSI session, typically a device such as a disk
      drive, tape drive, or scanner.
     </p></dd><dt id="id-1.11.6.3.15.2.16"><span class="term">target group (TG)</span></dt><dd><p>
      A list of SCSI target ports that are all treated the same when creating
      views. Creating a view can help simplify LUN (logical unit number)
      mapping. Each view entry specifies a target group, host group, and a LUN.
     </p></dd><dt id="id-1.11.6.3.15.2.17"><span class="term">target port</span></dt><dd><p>
      The combination of an iSCSI endpoint with one or more LUNs.
     </p></dd><dt id="id-1.11.6.3.15.2.18"><span class="term">target port group (TPG)</span></dt><dd><p>
      A list of IP addresses and TCP port numbers that determines which
      interfaces a specific iSCSI target will listen to.
     </p></dd><dt id="id-1.11.6.3.15.2.19"><span class="term">target session identifier (TSID)</span></dt><dd><p>
      A 16‐bit number, generated by the target, that uniquely identifies a
      session between the initiator and the target. This value is created
      during the login process, and is sent to the initiator with a Login
      Response PDU (protocol data units).
     </p></dd></dl></div></section><section class="sect1" id="sec-iscsi-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.8 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-iscsi-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_iscsi.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The iSCSI protocol has been available for several years. There are many
   reviews comparing iSCSI with SAN solutions, benchmarking performance, and
   there also is documentation describing hardware solutions. For more
   information, see the Open-iSCSI project home page at
   <a class="link" href="https://www.open-iscsi.com/" target="_blank">https://www.open-iscsi.com/</a>.
  </p><p>
   Additionally, see the man pages for <code class="command">iscsiadm</code>,
   <code class="command">iscsid</code>, and the example configuration file
   <code class="filename">/etc/iscsid.conf</code>.
  </p></section></section><section xml:lang="en" class="chapter" id="cha-fcoe" data-id-title="Fibre Channel storage over Ethernet networks: FCoE"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">16 </span><span class="title-name">Fibre Channel storage over Ethernet networks: FCoE</span></span> <a title="Permalink" class="permalink" href="#cha-fcoe">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Many enterprise data centers rely on Ethernet for their LAN and data traffic,
  and on Fibre Channel networks for their storage infrastructure. Open Fibre
  Channel over Ethernet (FCoE) Initiator software allows servers with Ethernet
  adapters to connect to a Fibre Channel storage subsystem over an Ethernet
  network. This connectivity was previously reserved exclusively for systems
  with Fibre Channel adapters over a Fibre Channel fabric. The FCoE technology
  reduces complexity in the data center by aiding network convergence. This
  helps to preserve your existing investments in a Fibre Channel storage
  infrastructure and to simplify network management.
 </p><div class="figure" id="id-1.11.6.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/fcoe_san_a.png"><img src="images/fcoe_san_a.png" width="100%" alt="Open Fibre channel over Ethernet SAN" title="Open Fibre channel over Ethernet SAN"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 16.1: </span><span class="title-name">Open Fibre channel over Ethernet SAN </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div><p>
  Open-FCoE allows you to run the Fibre Channel protocols on the host, instead
  of on proprietary hardware on the host bus adapter. It is targeted for 10
  Gbps (gigabit per second) Ethernet adapters, but can work on any Ethernet
  adapter that supports pause frames. The initiator software provides a Fibre
  Channel protocol processing module and an Ethernet-based transport module.
  The Open-FCoE module acts as a low-level driver for SCSI. The Open-FCoE
  transport uses <code class="command">net_device</code> to send and receive packets.
  Data Center Bridging (DCB) drivers provide the quality of service for FCoE.
 </p><p>
  FCoE is an encapsulation protocol that moves the Fibre Channel protocol
  traffic over Ethernet connections without changing the Fibre Channel frame.
  This allows your network security and traffic management infrastructure to
  work the same with FCoE as it does with Fibre Channel.
 </p><p>
  You might choose to deploy FCoE in your enterprise if the following
  conditions exist:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Your enterprise already has a Fibre Channel storage subsystem and
    administrators with Fibre Channel skills and knowledge.
   </p></li><li class="listitem"><p>
    You are deploying 10 Gbps Ethernet in the network.
   </p></li></ul></div><p>
  This section describes how to set up FCoE in your network.
 </p><section class="sect1" id="sec-fcoe-installation" data-id-title="Configuring FCoE interfaces during the installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.1 </span><span class="title-name">Configuring FCoE interfaces during the installation</span></span> <a title="Permalink" class="permalink" href="#sec-fcoe-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The YaST installation for <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> allows you to configure FCoE disks
   during the operating system installation if FCoE is enabled at the switch
   for the connections between the server and the Fibre Channel storage
   infrastructure. Some system BIOS types can automatically detect the FCoE
   disks, and report the disks to the YaST Installation software. However,
   automatic detection of FCoE disks is not supported by all BIOS types. To
   enable automatic detection in this case, you can add the
   <code class="option">withfcoe</code> option to the kernel command line when you begin
   the installation:
  </p><div class="verbatim-wrap"><pre class="screen">withfcoe=1</pre></div><p>
   When the FCoE disks are detected, the YaST installation offers the option
   to configure FCoE instances at that time. On the Disk Activation page,
   select <span class="guimenu">Configure FCoE Interfaces</span> to access the FCoE
   configuration. For information about configuring the FCoE interfaces, see
   <a class="xref" href="#sec-fcoe-yast" title="16.3. Managing FCoE services with YaST">Section 16.3, “Managing FCoE services with YaST”</a>.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/fcoe_inst_disk_activation_a.png"><img src="images/fcoe_inst_disk_activation_a.png" width="10%" alt="Image" title="Image"/></a></div></div><div id="id-1.11.6.4.10.6" data-id-title="Mount point support" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Mount point support</div><p>
    FCoE devices will appear asynchronously during the boot process. While the
    initrd guarantees that those devices are set up correctly for the root file
    system, there are no such guarantees for any other file systems or mount
    points like <code class="filename">/usr</code>. Hence any system mount points like
    <code class="filename">/usr</code> or <code class="filename">/var</code> are not supported.
    To use those devices, ensure correct synchronization of the respective
    services and devices.
   </p></div></section><section class="sect1" id="sec-fcoe-install" data-id-title="Installing FCoE and the YaST FCoE client"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.2 </span><span class="title-name">Installing FCoE and the YaST FCoE client</span></span> <a title="Permalink" class="permalink" href="#sec-fcoe-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can set up FCoE disks in your storage infrastructure by enabling FCoE at
   the switch for the connections to a server. If FCoE disks are available when
   the <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> operating system is installed, the FCoE Initiator software
   is automatically installed at that time.
  </p><p>
   If the FCoE Initiator software and the YaST FCoE Client software are not
   installed, use the following procedure to manually install them with the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper in yast2-fcoe-client fcoe-utils</pre></div><p>
   Alternatively, use the YaST Software Manager to install the packages
   listed above.
  </p></section><section class="sect1" id="sec-fcoe-yast" data-id-title="Managing FCoE services with YaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.3 </span><span class="title-name">Managing FCoE services with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-fcoe-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can use the YaST FCoE Client Configuration option to create,
   configure, and remove FCoE interfaces for the FCoE disks in your Fibre
   Channel storage infrastructure. To use this option, the FCoE Initiator
   service (the <code class="systemitem">fcoemon</code> daemon) and the
   Link Layer Discovery Protocol agent daemon
   (<code class="systemitem">llpad</code>) must be installed and
   running, and the FCoE connections must be enabled at the FCoE-capable
   switch.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Launch YaST and select <span class="guimenu">Network
     Services</span> › <span class="guimenu">FCoE Client
     Configuration</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/fcoe_services_a.png"><img src="images/fcoe_services_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     On the <span class="guimenu">Services</span> tab, view or modify the FCoE service
     and Lldpad (Link Layer Discovery Protocol agent daemon) service start time
     as necessary.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">FCoE service start:</span>
        Specifies whether to start the Fibre Channel over Ethernet service
        <code class="command">fcoemon</code> daemon at the server boot time or manually.
        The daemon controls the FCoE interfaces and establishes a connection
        with the <code class="systemitem">llpad</code> daemon. The
        values are <span class="guimenu">When Booting</span> (default) or
        <span class="guimenu">Manually</span>.
       </p></li><li class="listitem"><p><span class="formalpara-title">Lldpad service start:</span>
        Specifies whether to start the Link Layer Discovery Protocol agent
        <code class="systemitem">llpad</code> daemon at the server boot
        time or manually. The <code class="systemitem">llpad</code>
        daemon informs the <code class="command">fcoemon</code> daemon about the Data
        Center Bridging features and the configuration of the FCoE interfaces.
        The values are <span class="guimenu">When Booting</span> (default) or
        <span class="guimenu">Manually</span>.
       </p></li></ul></div><p>
     If you modify a setting, click <span class="guimenu">OK</span> to save and apply the
     change.
    </p></li><li class="step"><p>
     On the <span class="guimenu">Interfaces</span> tab, view information about all
     detected network adapters on the server, including information about
     VLAN and FCoE configuration. You can also create an FCoE VLAN interface,
     change settings for an existing FCoE interface, or remove an FCoE
     interface.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/fcoe_notconfig_interface_a.png"><img src="images/fcoe_notconfig_interface_a.png" width="100%" alt="Image" title="Image"/></a></div></div><p>
     Use the <span class="guimenu">FCoE VLAN Interface</span> column to determine whether
     FCoE is available or not:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.4.12.3.3.4.1"><span class="term"><em class="replaceable">Interface Name</em></span></dt><dd><p>
        If a name is assigned to the interface, such as
        <code class="filename">eth4.200</code>, FCoE is available on the switch, and the
        FCoE interface is activated for the adapter.
       </p></dd><dt id="id-1.11.6.4.12.3.3.4.2"><span class="term">Not configured:</span></dt><dd><p>
        If the status is <span class="guimenu">not configured</span>, FCoE is enabled on
        the switch, but an FCoE interface has not been activated for the
        adapter. Select the adapter, then click <span class="guimenu">Create FCoE VLAN
        Interface</span> to activate the interface on the adapter.
       </p></dd><dt id="id-1.11.6.4.12.3.3.4.3"><span class="term">Not available:</span></dt><dd><p>
        If the status is <span class="guimenu">not available</span>, FCoE is not possible
        for the adapter because FCoE has not been enabled for that connection
        on the switch.
       </p></dd></dl></div></li><li class="step"><p>
     To set up an FCoE-enabled adapter that has not yet been configured, select
     it and click <span class="guimenu">Create FCoE VLAN Interface</span>. Confirm the
     query with <span class="guimenu">Yes</span>.
    </p><p>
     The adapter is now listed with an interface name in the <span class="guimenu">FCoE VLAN
     Interface</span> column.
    </p></li><li class="step"><p>
     To change the settings for an adapter that is already configured, select
     it from the list, then click <span class="guimenu">Change Settings</span>.
    </p><p>
     The following options can be configured:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.4.12.3.5.3.1"><span class="term"><span class="guimenu">FCoE enable</span></span></dt><dd><p>
        Enable or disable the creation of FCoE instances for the adapter.
       </p></dd><dt id="id-1.11.6.4.12.3.5.3.2"><span class="term"><span class="guimenu">DCB required</span></span></dt><dd><p>
        Specifies whether Data Center Bridging is required for the adapter
        (usually this is the case).
       </p></dd><dt id="id-1.11.6.4.12.3.5.3.3"><span class="term"><span class="guimenu">Auto VLAN</span></span></dt><dd><p>
        Specifies whether the <code class="systemitem">fcoemon</code>
        daemon creates the VLAN interfaces automatically.
       </p></dd></dl></div><p>
     If you modify a setting, click <span class="guimenu">Next</span> to save and apply
     the change. The settings are written to the
     <code class="filename">/etc/fcoe/cfg-eth<em class="replaceable">X</em></code> file.
     The <code class="systemitem">fcoemon</code> daemon reads the
     configuration files for each FCoE interface when it is initialized.
    </p></li><li class="step"><p>
     To remove an interface that is already configured, select it from the
     list. Click <span class="guimenu">Remove Interface</span> and
     <span class="guimenu">Continue</span> to confirm. The FCoE Interface value changes
     to <span class="guimenu">not configured</span>.
    </p></li><li class="step"><p>
     On the <span class="guimenu">Configuration</span> tab, view or modify the general
     settings for the FCoE system service. You can enable or disable debugging
     messages from the FCoE service script and the
     <code class="systemitem">fcoemon</code> daemon and specify whether
     messages are sent to the system log.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/fcoe_configtab_a.png"><img src="images/fcoe_configtab_a.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to save and apply changes.
    </p></li></ol></div></div></section><section class="sect1" id="sec-fcoe-cli" data-id-title="Configuring FCoE with commands"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.4 </span><span class="title-name">Configuring FCoE with commands</span></span> <a title="Permalink" class="permalink" href="#sec-fcoe-cli">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following steps require using the <code class="command">fipvlan</code> command. If
   the command is not installed, install it by running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper in fcoe-utils</pre></div><p>
   To discover and configure all Ethernet interfaces, proceed as follows:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open the terminal.
    </p></li><li class="step"><p>
     To discover all available Ethernet interfaces, run the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> fipvlan -a</pre></div></li><li class="step"><p>
     For each Ethernet interface where FCoE offload is configured, run the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> fipvlan -c -s <em class="replaceable">ETHERNET_INTERFACE</em></pre></div><p>
     The command creates a network interface if it does not exist and starts
     the <code class="literal">Open-FCoE</code> initiator on the discovered FCoE VLAN.
    </p></li></ol></div></div></section><section class="sect1" id="sec-fcoe-admin" data-id-title="Managing FCoE instances with the FCoE administration tool"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.5 </span><span class="title-name">Managing FCoE instances with the FCoE administration tool</span></span> <a title="Permalink" class="permalink" href="#sec-fcoe-admin">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="command">fcoeadm</code> utility is the Fibre Channel over Ethernet
   (FCoE) management tool. It can be used to create, destroy, and reset an FCoE
   instance on a given network interface. The <code class="command">fcoeadm</code>
   utility sends commands to a running
   <code class="systemitem">fcoemon</code> process via a socket
   interface. For information about <code class="command">fcoemon</code>, see the
   <code class="command">man 8 fcoemon</code>.
  </p><p>
   The <code class="command">fcoeadm</code> utility allows you to query the FCoE
   instances about the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Interfaces
    </p></li><li class="listitem"><p>
     Target LUNs
    </p></li><li class="listitem"><p>
     Port statistics
    </p></li></ul></div><p>
   The <code class="command">fcoeadm</code> utility is part of the
   <code class="filename">fcoe-utils</code> package. The general syntax for the command
   looks like the following:
  </p><div class="verbatim-wrap"><pre class="screen">fcoeadm
  [-c|--create] [&lt;ethX&gt;]
  [-d|--destroy] [&lt;ethX&gt;]
  [-r|--reset] [&lt;ethX&gt;]
  [-S|--Scan] [&lt;ethX&gt;]
  [-i|--interface] [&lt;ethX&gt;]
  [-t|--target] [&lt;ethX&gt;]
  [-l|--lun] [&lt;ethX&gt;]
  [-s|--stats &lt;ethX&gt;] [&lt;interval&gt;]
  [-v|--version]
  [-h|--help]</pre></div><p>
   Refer to <code class="command">man 8 fcoeadm</code> for details.
  </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.11.6.4.14.8"><span class="name">Examples</span><a title="Permalink" class="permalink" href="#id-1.11.6.4.14.8">#</a></h2></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.4.14.9.1"><span class="term"><code class="command">fcoeadm -c eth2.101</code></span></dt><dd><p>
      Create an FCoE instance on eth2.101.
     </p></dd><dt id="id-1.11.6.4.14.9.2"><span class="term"><code class="command">fcoeadm -d eth2.101</code></span></dt><dd><p>
      Destroy an FCoE instance on eth2.101.
     </p></dd><dt id="id-1.11.6.4.14.9.3"><span class="term"><code class="command">fcoeadm -i eth3</code></span></dt><dd><p>
      Show information about all FCoE instances on interface
      <code class="literal">eth3</code>. If no interface is specified, information for
      all interfaces that have FCoE instances created will be shown. The
      following example shows information on connection eth0.201:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> fcoeadm -i eth0.201
  Description:      82599EB 10-Gigabit SFI/SFP+ Network Connection
  Revision:         01
  Manufacturer:     Intel Corporation
  Serial Number:    001B219B258C
  Driver:           ixgbe 3.3.8-k2
  Number of Ports:  1

      Symbolic Name:     fcoe v0.1 over eth0.201
      OS Device Name:    host8
      Node Name:         0x1000001B219B258E
      Port Name:         0x2000001B219B258E
      FabricName:        0x2001000573D38141
      Speed:             10 Gbit
      Supported Speed:   10 Gbit
      MaxFrameSize:      2112
      FC-ID (Port ID):   0x790003
      State:             Online</pre></div></dd><dt id="id-1.11.6.4.14.9.4"><span class="term"><code class="command">fcoeadm -l eth3.101</code></span></dt><dd><p>
      Show detailed information about all LUNs discovered on connection
      eth3.101. If no connection is specified, information about all
      LUNs discovered on all FCoE connections will be shown.
     </p></dd><dt id="id-1.11.6.4.14.9.5"><span class="term"><code class="command">fcoeadm -r eth2.101</code></span></dt><dd><p>
      Reset the FCoE instance on eth2.101.
     </p></dd><dt id="id-1.11.6.4.14.9.6"><span class="term"><code class="command">fcoeadm -s eth3 3</code></span></dt><dd><p>
      Show statistical information about a specific eth3 port that has FCoE
      instances, at an interval of three seconds. The statistics are displayed
      one line per time interval. If no interval is given, the default of one
      second is used.
     </p></dd><dt id="id-1.11.6.4.14.9.7"><span class="term"><code class="command">fcoeadm -t eth3</code></span></dt><dd><p>
      Show information about all discovered targets from a given eth3
      port having FCoE instances. After each discovered target, any associated
      LUNs are listed. If no instance is specified, targets from all ports
      that have FCoE instances are shown. The following example shows
      information of targets from the eth0.201 connection:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> fcoeadm -t eth0.201
  Interface:        eth0.201
  Roles:            FCP Target
  Node Name:        0x200000D0231B5C72
  Port Name:        0x210000D0231B5C72
  Target ID:        0
  MaxFrameSize:     2048
  OS Device Name:   rport-8:0-7
  FC-ID (Port ID):  0x79000C
  State:            Online

LUN ID  Device Name   Capacity   Block Size  Description
------  -----------  ----------  ----------  ----------------------------
    40  /dev/sdqi     792.84 GB      512     IFT DS S24F-R2840-4 (rev 386C)
    72  /dev/sdpk     650.00 GB      512     IFT DS S24F-R2840-4 (rev 386C)
   168  /dev/sdgy       1.30 TB      512     IFT DS S24F-R2840-4 (rev 386C)</pre></div></dd></dl></div></section><section class="sect1" id="sec-fcoe-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.6 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-fcoe-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_fcoe.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For information, see the follow documentation:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     For information about the Open-FCoE service daemon, see the
     <code class="command">fcoemon(8)</code> man page.
    </p></li><li class="listitem"><p>
     For information about the Open-FCoE Administration tool, see the
     <code class="command">fcoeadm(8)</code> man page.
    </p></li><li class="listitem"><p>
     For information about the Data Center Bridging Configuration tool, see the
     <code class="command">dcbtool(8)</code> man page.
    </p></li><li class="listitem"><p>
     For information about the Link Layer Discovery Protocol agent daemon, see
     the <code class="filename">lldpad(8)</code> man page.
    </p></li></ul></div></section></section><section xml:lang="en" class="chapter" id="cha-nvmeof" data-id-title="NVMe-oF"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">17 </span><span class="title-name">NVMe-oF</span></span> <a title="Permalink" class="permalink" href="#cha-nvmeof">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
        This chapter describes how to set up an NVMe over Fabrics host and target.
      </p></div></div></div></div><section class="sect1" id="sec-nvmeof-overview" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.1 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      <span class="emphasis"><em><span class="trademark">NVM
      Express</span>®</em></span>
      (<span class="emphasis"><em><span class="trademark">NVMe</span>®</em></span>)
      is an interface standard for accessing non-volatile storage, commonly SSD
      disks. NVMe supports much higher speeds and has a lower latency than
      SATA.
    </p><p>
      <span class="emphasis"><em><span class="trademark">NVMe-oF</span>™</em></span> is an
      architecture to access NVMe storage over different networking
      fabrics—for example, <span class="emphasis"><em>RDMA</em></span>,
      <span class="emphasis"><em>TCP</em></span>, or <span class="emphasis"><em>NVMe over Fibre
      Channel</em></span> (<span class="emphasis"><em>FC-NVMe</em></span>). The role of NVMe-oF
      is similar to iSCSI. To increase the fault-tolerance, NVMe-oF has a
      built-in support for multipathing. The NVMe-oF multipathing is not based
      on the traditional DM-Multipathing.
    </p><p>
      The <span class="emphasis"><em>NVMe host</em></span> is the machine that connects to an
      NVMe target. The <span class="emphasis"><em>NVMe target</em></span> is the machine that
      shares its NVMe block devices.
    </p><p>
      NVMe is supported on SUSE Linux Enterprise Server <span class="productnumber"><span class="phrase">15 SP7</span></span>. There are Kernel modules
      available for the NVMe block storage and NVMe-oF target and host.
    </p><p>
      To see if your hardware requires any special consideration, refer to
      <a class="xref" href="#sec-nvmeof-hardware" title="17.4. Special hardware configuration">Section 17.4, “Special hardware configuration”</a>.
    </p></section><section class="sect1" id="sec-nvmeof-host-configuration" data-id-title="Setting up an NVMe-oF host"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.2 </span><span class="title-name">Setting up an NVMe-oF host</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-host-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      To use NVMe-oF, a target must be available with one of the supported
      networking methods. Supported are NVMe over Fibre Channel, TCP, and
      RDMA. The following sections describe how to connect a host to an NVMe
      target.
    </p><section class="sect2" id="sec-nvmeof-host-configuration-cli" data-id-title="Installing command line client"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.1 </span><span class="title-name">Installing command line client</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-host-configuration-cli">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To use NVMe-oF, you need the <code class="command">nvme</code> command line
        tool. Install it with <code class="command">zypper</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">zypper in nvme-cli</code></pre></div><p>
        Use <code class="command">nvme --help</code> to list all available subcommands.
        Man pages are available for <code class="command">nvme</code> subcommands.
        Consult them by executing <code class="command">man
        nvme-<em class="replaceable">SUBCOMMAND</em></code>. For example, to
        view the man page for the <code class="option">discover</code> subcommand, execute
        <code class="command">man nvme-discover</code>.
      </p></section><section class="sect2" id="sec-nvmeof-host-configuration-target-discovery" data-id-title="Discovering NVMe-oF targets"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.2 </span><span class="title-name">Discovering NVMe-oF targets</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-host-configuration-target-discovery">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To list available NVMe subsystems on the NVMe-oF target, you need
        the discovery controller address and service ID.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">nvme discover -t <em class="replaceable">TRANSPORT</em> -a <em class="replaceable">DISCOVERY_CONTROLLER_ADDRESS</em> -s <em class="replaceable">SERVICE_ID</em></code></pre></div><p>
        Replace <em class="replaceable">TRANSPORT</em> with the underlying
        transport medium: <code class="option">loop</code>, <code class="option">rdma</code>,
        <code class="option">tcp</code>, or <code class="option">fc</code>. Replace
        <em class="replaceable">DISCOVERY_CONTROLLER_ADDRESS</em> with the
        address of the discovery controller. For RDMA and TCP, this should be
        an IPv4 address. Replace <em class="replaceable">SERVICE_ID</em> with the
        transport service ID. If the service is IP based, like RDMA or TCP,
        service ID specifies the port number. For Fibre Channel, the service ID
        is not required.
      </p><p>
        The NVMe hosts only see the subsystems they are allowed to connect
        to.
      </p><p>
        Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> nvme discover -t tcp -a 10.0.0.3 -s 4420</pre></div><p>
        For the FC, the example looks as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> nvme discover --transport=fc \
                --traddr=nn-0x201700a09890f5bf:pn-0x201900a09890f5bf \
                --host-traddr=nn-0x200000109b579ef6:pn-0x100000109b579ef6</pre></div><p>
        For more details, see <code class="command">man nvme-discover</code>.
      </p></section><section class="sect2" id="sec-nvmeof-host-configuration-connect-target" data-id-title="Connecting to NVMe-oF targets"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.3 </span><span class="title-name">Connecting to NVMe-oF targets</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-host-configuration-connect-target">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        After you have identified the NVMe subsystem, you can connect it with
        the <code class="command">nvme connect</code> command.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">nvme connect -t <em class="replaceable">transport</em> -a <em class="replaceable">DISCOVERY_CONTROLLER_ADDRESS</em> -s <em class="replaceable">SERVICE_ID</em> -n <em class="replaceable">SUBSYSTEM_NQN</em></code></pre></div><p>
        Replace <em class="replaceable">TRANSPORT</em> with the underlying
        transport medium: <code class="option">loop</code>, <code class="option">rdma</code>,
        <code class="option">tcp</code> or <code class="option">fc</code>. Replace
        <em class="replaceable">DISCOVERY_CONTROLLER_ADDRESS</em> with the
        address of the discovery controller. For RDMA and TCP this should be an
        IPv4 address. Replace <em class="replaceable">SERVICE_ID</em> with the
        transport service ID. If the service is IP based, like RDMA or TCP,
        this specifies the port number. Replace
        <em class="replaceable">SUBSYSTEM_NQN</em> with the NVMe qualified name
        of the desired subsystem as found by the discovery command.
        <span class="emphasis"><em>NQN</em></span> is the abbreviation for <span class="emphasis"><em> NVMe
        Qualified Name</em></span>. The NQN must be unique.
      </p><p>
        Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">nvme connect -t tcp -a 10.0.0.3 -s 4420 -n nqn.2014-08.com.example:nvme:nvm-subsystem-sn-d78432</code></pre></div><p>
        For the FC, the example looks as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> nvme connect --transport=fc \
             --traddr=nn-0x201700a09890f5bf:pn-0x201900a09890f5bf \
             --host-traddr=nn-0x200000109b579ef6:pn-0x100000109b579ef6 \
             --nqn=nqn.2014-08.org.nvmexpress:uuid:1a9e23dd-466e-45ca-9f43-a29aaf47cb21</pre></div><p>
        Alternatively, use <code class="command">nvme connect-all</code> to connect to
        all discovered namespaces. For advanced usage, see <code class="command">man
        nvme-connect</code> and <code class="command">man nvme-connect-all</code>.
      </p><p>
        In case of a path loss, the NVMe subsystem tries to reconnect for a
        time period, defined by the <code class="literal">ctrl-loss-tmo</code> option of
        the <code class="command">nvme connect</code> command. After this time (default
        value is 600s), the path is removed and the upper layers of the block
        layer (file system) are notified. By default, the file system is then
        mounted read-only, which usually is not the expected behavior.
        Therefore, it is recommended to set the
        <code class="literal">ctrl-loss-tmo</code> option so that the NVMe subsystem
        keeps trying to reconnect without a limit. To do so, run the following
        command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> nvme connect --ctrl-loss-tmo=-1</pre></div><p>
        To make an NVMe over Fabrics subsystem available at boot, create a
        <code class="filename">/etc/nvme/discovery.conf</code> file on the host with the
        parameters passed to the <code class="command">discover</code> command (as
        described in
        <a class="xref" href="#sec-nvmeof-host-configuration-target-discovery" title="17.2.2. Discovering NVMe-oF targets">Section 17.2.2, “Discovering NVMe-oF targets”</a>. For
        example, if you use the <code class="command">discover</code> command as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> nvme discover -t tcp -a 10.0.0.3 -s 4420</pre></div><p>
        Add the parameters of the <code class="command">discover</code> command to the
        <code class="filename">/etc/nvme/discovery.conf</code> file:
      </p><div class="verbatim-wrap"><pre class="screen">echo "-t tcp -a 10.0.0.3 -s 4420" | sudo tee -a /etc/nvme/discovery.conf</pre></div><p>
        Then enable the <span class="guimenu">nvmf-autoconnect</span> service:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl enable nvmf-autoconnect.service</pre></div></section><section class="sect2" id="sec-nvmeof-host-configuration-multipathing" data-id-title="Multipathing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.4 </span><span class="title-name">Multipathing</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-host-configuration-multipathing">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        NVMe native multipathing is enabled by default. If the
        <code class="option">CMIC</code> option in the controller identity settings is
        set, the NVMe stack recognizes an NVME drive as a multipathed device by
        default.
      </p><p>
        To manage the multipathing, you can use the following:
      </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Managing multipathing </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.5.4.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.11.6.5.4.6.4.2"><span class="term"><code class="command">nvme list-subsys</code></span></dt><dd><p>
              Prints the layout of the multipath devices.
            </p></dd><dt id="id-1.11.6.5.4.6.4.3"><span class="term"><code class="command">multipath -ll</code></span></dt><dd><p>
              The command has a compatibility mode and displays NVMe
              multipath devices. Bear in mind that you need to enable the
              <code class="literal">enable_foreign</code> option to use the command. For
              details, refer to <a class="xref" href="#sec-multipath-conf-misc" title="18.13. Miscellaneous options">Section 18.13, “Miscellaneous options”</a>.
            </p></dd><dt id="id-1.11.6.5.4.6.4.4"><span class="term"><code class="option">nvme-core.multipath=N</code></span></dt><dd><p>
              When the option is added as a boot parameter, the NVMe native
              multipathing will be disabled.
            </p></dd></dl></div></section></section><section class="sect1" id="sec-nvmeof-target-configuration" data-id-title="Setting up an NVMe-oF target"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.3 </span><span class="title-name">Setting up an NVMe-oF target</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-target-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-nvmeof-target-configuration-cli" data-id-title="Installing command line client"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.1 </span><span class="title-name">Installing command line client</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-target-configuration-cli">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To configure an NVMe-oF target, you need the
        <code class="command">nvmetcli</code> command line tool. Install it with
        <code class="command">zypper</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">zypper in nvmetcli</code></pre></div><p>
        The current documentation for <code class="command">nvmetcli</code> is available
        at
        <a class="link" href="https://git.infradead.org/users/hch/nvmetcli.git/blob_plain/HEAD:/Documentation/nvmetcli.txt" target="_blank">https://git.infradead.org/users/hch/nvmetcli.git/blob_plain/HEAD:/Documentation/nvmetcli.txt</a>.
      </p></section><section class="sect2" id="sec-nvmeof-target-configuration-steps" data-id-title="Configuration steps"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.2 </span><span class="title-name">Configuration steps</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-target-configuration-steps">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The following procedure provides an example of how to set up an
        NVMe-oF target.
      </p><p>
        The configuration is stored in a tree structure. Use the command
        <code class="command">cd</code> to navigate. Use <code class="command">ls</code> to list
        objects. You can create new objects with <code class="command">create</code>.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Start the <code class="command">nvmetcli</code> interactive shell:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">nvmetcli</code></pre></div></li><li class="step"><p>
            Create a new port:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd ports</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">create 1</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">ls 1/</code>
o- 1
  o- referrals
  o- subsystems</pre></div></li><li class="step"><p>
            Create an NVMe subsystem:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd /subsystems</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">ls</code>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces</pre></div></li><li class="step"><p>
            Create a new namespace and set an NVMe device to it:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd namespaces</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">create 1</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd 1</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">set device path=/dev/nvme0n1</code>
Parameter path is now '/dev/nvme0n1'.</pre></div></li><li class="step"><p>
            Enable the previously created namespace:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd ..</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">enable</code>
The Namespace has been enabled.</pre></div></li><li class="step"><p>
            Display the created namespace:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd ..</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">ls</code>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces
    o- 1</pre></div></li><li class="step"><p>
            Allow all hosts to use the subsystem. Only do this in secure
            environments.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">set attr allow_any_host=1</code>
Parameter allow_any_host is now '1'.</pre></div><p>
            Alternatively, you can allow only specific hosts to connect:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/allowed_hosts/</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">create hostnqn</code></pre></div></li><li class="step"><p>
            List all created objects:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd /</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">ls</code>
o- /
  o- hosts
  o- ports
  | o- 1
  |   o- referrals
  |   o- subsystems
  o- subsystems
    o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
      o- allowed_hosts
      o- namespaces
        o- 1</pre></div></li><li class="step"><p>
            Make the target available via TCP. Use
            <code class="literal">trtype=rdma</code> for RDMA:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd ports/1/</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">set addr adrfam=ipv4 trtype=tcp traddr=10.0.0.3 trsvcid=4420</code>
Parameter trtype is now 'tcp'.
Parameter adrfam is now 'ipv4'.
Parameter trsvcid is now '4420'.
Parameter traddr is now '10.0.0.3'.</pre></div><p>
            Alternatively, you can make it available with Fibre Channel:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd ports/1/</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">set addr adrfam=fc trtype=fc traddr=nn-0x1000000044001123:pn-0x2000000055001123 trsvcid=none</code></pre></div></li><li class="step"><p>
            Link the subsystem to the port:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">cd /ports/1/subsystems</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</code></pre></div><p>
            Now you can verify that the port is enabled using
            <code class="command">dmesg</code>:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>dmesg
        ...
[  257.872084] nvmet_tcp: enabling port 1 (10.0.0.3:4420)</pre></div></li></ol></div></div></section><section class="sect2" id="sec-nvmeof-target-configuration-backup-configuration" data-id-title="Back up and restore target configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.3 </span><span class="title-name">Back up and restore target configuration</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-target-configuration-backup-configuration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        You can save the target configuration in a JSON file with the following
        commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">nvmetcli</code>
<code class="prompt user">(nvmetcli)&gt; </code><code class="command">saveconfig nvme-target-backup.json</code></pre></div><p>
        To restore the configuration, use:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">restore nvme-target-backup.json</code></pre></div><p>
        You can also wipe the current configuration:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">(nvmetcli)&gt; </code><code class="command">clear</code></pre></div></section></section><section class="sect1" id="sec-nvmeof-hardware" data-id-title="Special hardware configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.4 </span><span class="title-name">Special hardware configuration</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-hardware">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-nvmeof-hardware-overview" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.4.1 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-hardware-overview">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Some hardware needs special configuration to work correctly. Skim the
        titles of the following sections to see if you are using any of the
        mentioned devices or vendors.
      </p></section><section class="sect2" id="sec-nvmeof-hardware-broadcom" data-id-title="Broadcom"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.4.2 </span><span class="title-name">Broadcom</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-hardware-broadcom">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        If you are using the <span class="emphasis"><em>Broadcom Emulex LightPulse Fibre Channel
        SCSI</em></span> driver, add a Kernel configuration parameter on the
        target and host for the <code class="literal">lpfc</code> module:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">echo "options lpfc lpfc_enable_fc4_type=3" &gt; /etc/modprobe.d/lpfc.conf</code></pre></div><p>
        Make sure that the Broadcom adapter firmware has at least version
        11.4.204.33. Also make sure that you have the current versions of
        <span class="package">nvme-cli</span>, <span class="package">nvmetcli</span> and the Kernel
        installed.
      </p><p>
        To enable a Fibre Channel port as an NVMe target, an additional
        module parameter needs to be configured:
        <code class="option">lpfc_enable_nvmet=<em class="replaceable">
        COMMA_SEPARATED_WWPNS</em></code>. Enter the WWPN with a
        leading <code class="literal">0x</code>, for example
        <code class="option">lpfc_enable_nvmet=0x2000000055001122,0x2000000055003344</code>.
        Only listed WWPNs will be configured for target mode. A Fibre Channel
        port can either be configured as target <span class="emphasis"><em>or</em></span> as
        initiator.
      </p></section><section class="sect2" id="sec-nvmeof-hardware-marvell" data-id-title="Marvell"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.4.3 </span><span class="title-name">Marvell</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-hardware-marvell">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        FC-NVMe is supported on QLE269x and QLE27xx adapters. FC-NVMe
        support is enabled by default in the Marvell® QLogic® QLA2xxx Fibre
        Channel driver.
      </p><p>
        To confirm NVMe is enabled, run the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/module/qla2xxx/parameters/ql2xnvmeenable</pre></div><p>
        A resulting <code class="literal">1</code> means NVMe is enabled, a
        <code class="literal">0</code> indicates it is disabled.
      </p><p>
        Next, ensure that the Marvell adapter firmware is at least version
        8.08.204 by checking the output of the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /sys/class/scsi_host/host0/fw_version</pre></div><p>
        Last, ensure that the latest versions available for <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> of
        <span class="package">nvme-cli</span>, <span class="package">QConvergeConsoleCLI</span>,
        and the Kernel are installed. You may, for example, run
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper lu &amp;&amp; zypper pchk</pre></div><p>
        to check for updates and patches.
      </p><p>
        For more details on installation, please refer to the FC-NVMe
        sections in the following Marvell user guides:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            <a class="link" href="https://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32769&amp;docid=96728&amp;ProductCategory=39&amp;Product=1259&amp;Os=126" target="_blank">https://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32769&amp;docid=96728&amp;ProductCategory=39&amp;Product=1259&amp;Os=126</a>
          </p></li><li class="listitem"><p>
            <a class="link" href="https://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32761&amp;docid=96726&amp;ProductCategory=39&amp;Product=1261&amp;Os=126" target="_blank">https://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32761&amp;docid=96726&amp;ProductCategory=39&amp;Product=1261&amp;Os=126</a>
          </p></li></ul></div></section></section><section class="sect1" id="sec-nvmeof-boot" data-id-title="Booting from NVMe-oF over TCP"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.5 </span><span class="title-name">Booting from NVMe-oF over TCP</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-boot">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      <span class="productname"><span class="phrase">SLES</span></span> supports booting from NVMe-oF over TCP according to
      the
      <a class="link" href="https://nvmexpress.org/wp-content/uploads/NVM-Express-Boot-Specification-2022.11.15-Ratified.pdf" target="_blank">NVM
      Express® Boot Specification 1.0</a>.
    </p><p>
      The UEFI pre-boot environment can be configured to attempt NVMe-oF over
      TCP connections to remote storage servers and use these for booting. The
      pre-boot environment creates an ACPI table—NVMe Boot Firmware Table
      (NBFT) to store information about the NVMe-oF configuration used for
      booting. The operating system uses this table at a later boot stage to
      set up networking and NVMe-oF connections to access the root file
      system.
    </p><section class="sect2" id="sec-nvmeof-tcp-requirements" data-id-title="System requirements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.5.1 </span><span class="title-name">System requirements</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-tcp-requirements">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To boot the system from NVMe-oF over TCP, the following requirements
        must be met:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            <span class="productname"><span class="phrase">SLES</span></span><span class="productnumber"><span class="phrase">15 SP7</span></span> or later.
          </p></li><li class="listitem"><p>
            A SAN storage array supporting NVMe-oF over TCP
          </p></li><li class="listitem"><p>
            A host system with a BIOS that supports booting from NVMe-oF over
            TCP. Contact your hardware vendor for information about support for
            this feature. Booting from NVMe-oF over TCP is currently only
            supported on UEFI platforms.
          </p></li></ul></div></section><section class="sect2" id="sec-install-nvme-tcp" data-id-title="Installation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.5.2 </span><span class="title-name">Installation</span></span> <a title="Permalink" class="permalink" href="#sec-install-nvme-tcp">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To install <span class="productname"><span class="phrase">SLES</span></span> from NVMe-oF over TCP, proceed as
        follows:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Use the host system's UEFI setup menus to configure NVMe-oF
            connections to be established at boot time. Typically, you need to
            configure both networking (local IP addresses, gateways, etc.) and
            NVMe-oF targets (remote IP address, subsystem NQN or discovery
            NQN). Refer to the hardware documentaion for the configuration
            description. Your hardware vendor may provide means to manage the
            BIOS configuration centrally and remotely. Please contact your
            hardware vendor for additional information.
          </p></li><li class="step"><p>
            Prepare the installation as described in
            <span class="intraxref">Book “Deployment Guide”</span>.
          </p></li><li class="step"><p>
            Start the system installation using any supported installation
            method. You do not need to use any specific boot parameters to
            enable installation on NVMe-oF over TCP.
          </p></li><li class="step"><p>
            If the BIOS has been configured correctly, the disk partitioning
            dialog in YaST will show NVMe namespaces exported by the
            subsystems configured in the BIOS. They will be displayed as NVMe
            devices, where the <code class="literal">tcp</code> string indicates that the
            devices are connected via the TCP transport. Install the operating
            system (in particular the EFI boot partition and the root file
            system) on these namespaces.
          </p></li><li class="step"><p>
            Complete the installation.
          </p></li></ol></div></div><p>
        After installation, the system should boot from NVMe-oF over TCP
        automatically. If it does not, check if the boot priority is set
        correctly in the BIOS setup.
      </p><p>
        The network interfaces used for booting are named
        <code class="literal">nbft0</code>, <code class="literal">nbft1</code> and so on. To get
        information about the NVMe-oF boot, run the command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">nvme nbft show</code></pre></div></section></section><section class="sect1" id="sec-nvmeof-more-information" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.6 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-nvmeof-more-information">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nvmeof.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      For more details about the abilities of the <code class="command">nvme</code>
      command, refer to <code class="command">nvme nvme-help</code>.
    </p><p>
      The following links provide a basic introduction to NVMe and NVMe-oF:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <a class="link" href="https://nvmexpress.org/" target="_blank">https://nvmexpress.org/</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://www.nvmexpress.org/wp-content/uploads/NVMe_Over_Fabrics.pdf" target="_blank">https://www.nvmexpress.org/wp-content/uploads/NVMe_Over_Fabrics.pdf</a>
        </p></li><li class="listitem"><p>
          <a class="link" href="https://storpool.com/blog/demystifying-what-is-nvmeof" target="_blank">https://storpool.com/blog/demystifying-what-is-nvmeof</a>
        </p></li></ul></div></section></section><section xml:lang="en" class="chapter" id="cha-multipath" data-id-title="Managing multipath I/O for devices"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">18 </span><span class="title-name">Managing multipath I/O for devices</span></span> <a title="Permalink" class="permalink" href="#cha-multipath">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes how to manage failover and path load balancing for
  multiple paths between the servers and block storage devices by using
  Multipath I/O (MPIO).
 </p><section class="sect1" id="sec-multipath-intro" data-id-title="Understanding multipath I/O"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.1 </span><span class="title-name">Understanding multipath I/O</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-intro">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Multipathing is the ability of a server to communicate with the same
   physical or logical block storage device across multiple physical paths
   between the host bus adapters in the server and the storage controllers for
   the device, typically in Fibre Channel (FC) or iSCSI SAN environments.
  </p><p>
   Linux multipathing provides connection fault tolerance and can provide load
   balancing across the active connections. When multipathing is configured and
   running, it automatically isolates and identifies device connection
   failures, and reroutes I/O to alternate connections.
  </p><p>
   Multipathing provides fault tolerance against connection failures, but not
   against failures of the storage device itself. The latter is achieved with
   complementary techniques like mirroring.
  </p><section class="sect2" id="id-1.11.6.6.4.5" data-id-title="Multipath terminology"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.1.1 </span><span class="title-name">Multipath terminology</span></span> <a title="Permalink" class="permalink" href="#id-1.11.6.6.4.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.4.5.2.1"><span class="term">Storage array</span></dt><dd><p>
       A hardware device with many disks and multiple fabrics connections
       (controllers) that provides SAN storage to clients. Storage arrays
       typically have RAID and failover features and support multipathing.
       Historically, active/passive (failover) and active/active
       (load-balancing) storage array configurations were distinguished. These
       concepts still exist but they are merely special cases of the concepts
       of path groups and access states supported by modern hardware.
      </p></dd><dt id="id-1.11.6.6.4.5.2.2"><span class="term">Host, host system</span></dt><dd><p>
       The computer running <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> which acts as a client system for a
       <span class="emphasis"><em>storage array</em></span>.
      </p></dd><dt id="id-1.11.6.6.4.5.2.3"><span class="term">Multipath map, multipath device</span></dt><dd><p>
       A set of <span class="emphasis"><em>path devices</em></span>. It represents a storage
       volume on a storage array and is seen as a single block device by the
       host system.
      </p></dd><dt id="id-1.11.6.6.4.5.2.4"><span class="term">Path device</span></dt><dd><p>
       A member of a multipath map, typically a SCSI device. Each path device
       represents a unique connection between the host computer and the actual
       storage volume, for example, a logical unit from an iSCSI session.
      </p></dd><dt id="id-1.11.6.6.4.5.2.5"><span class="term">WWID</span></dt><dd><p>
       “World Wide Identifier”.
       <code class="systemitem">multipath-tools</code> uses the WWID to determine
       which low-level devices should be assembled into a multipath map. The
       WWID must be distinguished from the configurable <span class="emphasis"><em>map
       name</em></span> (see <a class="xref" href="#sec-multipath-names" title="18.12. Multipath device names and WWIDs">Section 18.12, “Multipath device names and WWIDs”</a>).
      </p></dd><dt id="id-1.11.6.6.4.5.2.6"><span class="term">uevent, udev event</span></dt><dd><p>
       An event sent by the kernel to user space and processed by the
       <code class="command">udev</code> subsystem. Uevents are generated when devices
       are added or removed, or when they change their properties.
      </p></dd><dt id="id-1.11.6.6.4.5.2.7"><span class="term">Device mapper</span></dt><dd><p>
       A framework in the Linux kernel for creating virtual block devices. I/O
       operations to mapped devices are redirected to the underlying block
       devices. Device mappings may be stacked. The device mapper implements
       its own event signaling, also known as “device mapper
       events” or “dm events”.
      </p></dd><dt id="id-1.11.6.6.4.5.2.8"><span class="term">initramfs</span></dt><dd><p>
       The initial RAM file system, also referred to as “initial RAM
       disk” (initrd) for historical reasons (see
       <span class="intraxref">Book “Administration Guide”, Chapter 16 “Introduction to the boot process”, Section 16.1 “Terminology”</span>).
      </p></dd><dt id="id-1.11.6.6.4.5.2.9"><span class="term">ALUA</span></dt><dd><p>
       “Asymmetric Logical Unit Access”, a concept introduced with
       the SCSI standard SCSI-3. Storage volumes can be accessed via multiple
       ports, which are organized in port groups with different states (active,
       standby, etc.). ALUA defines SCSI commands to query the port groups and
       their states and change the state of a port group. Modern storage arrays
       that support SCSI usually support ALUA, too.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-multipath-hardware" data-id-title="Hardware support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.2 </span><span class="title-name">Hardware support</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-hardware">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The multipathing drivers and tools are available on all architectures
   supported by <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>. The generic, protocol-agnostic driver works with
   most multipath-capable storage hardware on the market. Some storage array
   vendors provide their own multipathing management tools. Consult the
   vendor’s hardware documentation to determine what settings are required.
  </p><section class="sect2" id="sec-multipath-hardware-implementations" data-id-title="Multipath implementations: device mapper and NVMe"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.2.1 </span><span class="title-name">Multipath implementations: device mapper and NVMe</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-hardware-implementations">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The traditional, generic implementation of multipathing under Linux uses
    the device mapper framework. For most device types like SCSI devices,
    device mapper multipathing is the only available implementation. Device
    mapper multipath is highly configurable and flexible.
   </p><p>
    The Linux <span class="emphasis"><em>NVM Express</em></span> (NVMe) kernel subsystem
    implements multipathing natively in the kernel. This implementation creates
    less computational overhead for NVMe devices, which are typically fast
    devices with very low latencies. Native NVMe multipathing requires no user
    space component. Since SLE 15, native multipathing has been the default
    for NVMe multipath devices. For details, refer to
    <a class="xref" href="#sec-nvmeof-host-configuration-multipathing" title="17.2.4. Multipathing">Section 17.2.4, “Multipathing”</a>.
   </p><p>
    This chapter documents device mapper multipath and its user-space
    component, <code class="systemitem">multipath-tools</code>.
    <code class="systemitem">multipath-tools</code> also has limited support for
    native NVMe multipathing (see
    <a class="xref" href="#sec-multipath-conf-misc" title="18.13. Miscellaneous options">Section 18.13, “Miscellaneous options”</a>).
   </p></section><section class="sect2" id="sec-multipath-hardware-autodetect" data-id-title="Storage array autodetection for multipathing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.2.2 </span><span class="title-name">Storage array autodetection for multipathing</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-hardware-autodetect">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Device mapper multipath is a generic technology. Multipath device detection
    requires only that the low-level (for example, SCSI) devices are detected
    by the kernel, and that device properties reliably identify multiple
    low-level devices as being different “paths” to the same volume
    rather than actually different devices.
   </p><p>
    The <code class="filename">multipath-tools</code> package detects storage arrays by
    their vendor and product names. It provides built-in configuration defaults
    for a large variety of storage products. Consult the hardware documentation
    of your storage array: some vendors provide specific recommendations for
    Linux multipathing configuration.
   </p><p>
    If you need to apply changes to the built-in configuration for your storage
    array, read <a class="xref" href="#sec-multipath-conf-file" title="18.8. Multipath configuration">Section 18.8, “Multipath configuration”</a>.
   </p><div id="id-1.11.6.6.5.4.5" data-id-title="Disclaimer about built-in hardware properties" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Disclaimer about built-in hardware properties</div><p>
     <code class="filename">multipath-tools</code> has built-in presets for many storage
     arrays. The existence of such presets for a given storage product
     <span class="emphasis"><em>does not imply</em></span> that the vendor of the storage product
     has tested the product with <code class="systemitem">dm-multipath</code>, nor
     that the vendor endorses or supports the use of
     <code class="systemitem">dm-multipath</code> with the product. Always consult the
     original vendor documentation for support-related questions.
    </p></div></section><section class="sect2" id="sec-multipath-hardware-handlers" data-id-title="Storage arrays that require specific hardware handlers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.2.3 </span><span class="title-name">Storage arrays that require specific hardware handlers</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-hardware-handlers">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Some storage arrays require special commands for failover from one path to
    the other, or non-standard error-handling methods. These special commands
    and methods are implemented by hardware handlers in the Linux kernel.
    Modern SCSI storage arrays support the “Asymmetric Logical Unit
    Access” (ALUA) hardware handler defined in the SCSI standard. Besides
    ALUA, the SLE kernel contains hardware handlers for Netapp E-Series
    (RDAC), the Dell/EMC CLARiiON CX family of arrays, and legacy arrays from
    HP.
   </p><p>
    Since Linux kernel 4.4, the Linux kernel has automatically detected
    hardware handlers for most arrays, including all arrays supporting ALUA.
    The only requirement is that the device handler modules are loaded at the
    time the respective devices are probed. The
    <code class="systemitem">multipath-tools</code> package ensures this by installing
    appropriate configuration files. Once a device handler is attached to a
    given device, it cannot be changed anymore.
   </p></section></section><section class="sect1" id="sec-multipath-planning" data-id-title="Planning for multipathing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.3 </span><span class="title-name">Planning for multipathing</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Use the guidelines in this section when planning your multipath I/O
   solution.
  </p><section class="sect2" id="sec-multipath-planning-prereq" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning-prereq">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The storage array you use for the multipathed device must support
      multipathing. For more information, see
      <a class="xref" href="#sec-multipath-hardware" title="18.2. Hardware support">Section 18.2, “Hardware support”</a>.
     </p></li><li class="listitem"><p>
      You need to configure multipathing only if multiple physical paths exist
      between host bus adapters in the server and host bus controllers for the
      block storage device.
     </p></li><li class="listitem"><p>
      For some storage arrays, the vendor provides its own multipathing
      software to manage multipathing for the array’s physical and logical
      devices. In this case, you should follow the vendor’s instructions for
      configuring multipathing for those devices.
     </p></li><li class="listitem"><p>
      When using multipathing in a virtualization environment, the multipathing
      is controlled in the host server environment. Configure multipathing for
      the device before you assign it to a virtual guest machine.
     </p></li></ul></div></section><section class="sect2" id="sec-multipath-planning-types" data-id-title="Multipath installation types"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.2 </span><span class="title-name">Multipath installation types</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning-types">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    We distinguish installation types by the way the root device is handled.
    <a class="xref" href="#sec-multipath-installing" title="18.4. Installing SUSE Linux Enterprise Server on multipath systems">Section 18.4, “Installing <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> on multipath systems”</a> describes how the different
    setups are created during and after installation.
   </p><section class="sect3" id="sec-multipath-planning-type-root" data-id-title="Root file system on multipath (SAN-boot)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">18.3.2.1 </span><span class="title-name">Root file system on multipath (SAN-boot)</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning-type-root">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The root file system is on a multipath device. This is typically the case
     for diskless servers that use SAN storage exclusively. On such systems,
     multipath support is required for booting, and multipathing must be
     enabled in the initramfs.
    </p></section><section class="sect3" id="sec-multipath-planning-type-noroot" data-id-title="Root file system on a local disk"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">18.3.2.2 </span><span class="title-name">Root file system on a local disk</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning-type-noroot">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The root file system (and possibly some other file systems) is on local
     storage, for example, on a directly attached SATA disk or local RAID, but
     the system additionally uses file systems in the multipath SAN storage.
     This system type can be configured in three different ways:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.6.4.4.3.1"><span class="term">Multipath setup for local disk</span></dt><dd><p>
        All block devices are part of multipath maps, including the local disk.
        The root device appears as a degraded multipath map with just one path.
        This configuration is created if multipathing was enabled during the
        initial system installation with YaST.
       </p></dd><dt id="vl-multipath-planning-type-noroot-ignore"><span class="term">Local disk is excluded from multipath</span></dt><dd><p>
        In this configuration, multipathing is enabled in the initramfs, but
        the root device is explicitly excluded from multipath (see
        <a class="xref" href="#sec-multipath-blacklist" title="18.11.1. The blacklist section in multipath.conf">Section 18.11.1, “The <code class="literal">blacklist</code> section in <code class="filename">multipath.conf</code>”</a>).
        <a class="xref" href="#pr-multipath-disable-root" title="Disabling multipathing for the root disk after installation">Procedure 18.1, “Disabling multipathing for the root disk after installation”</a> describes how to set up
        this configuration.
       </p></dd><dt id="vl-multipath-planning-type-noroot-noinitrd"><span class="term">Multipath disabled in the initramfs</span></dt><dd><p>
        This setup is created if multipathing was not enabled during the
        initial system installation with YaST. This configuration is rather
        fragile; consider using one of the other options instead.
       </p></dd></dl></div></section></section><section class="sect2" id="sec-multipath-planning-disks" data-id-title="Disk management tasks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.3 </span><span class="title-name">Disk management tasks</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning-disks">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Use third-party SAN array management tools or the user interface of your
    storage array to create logical devices and assign them to hosts. Make sure
    to configure the host credentials correctly on both sides.
   </p><p>
    You can add or remove volumes to a running host, but detecting the changes
    may require rescanning SCSI targets and reconfiguring multipathing on the
    host. See <a class="xref" href="#sec-multipath-best-practice-scandev" title="18.14.6. Scanning for new devices without rebooting">Section 18.14.6, “Scanning for new devices without rebooting”</a>.
   </p><div id="id-1.11.6.6.6.5.4" data-id-title="Storage processors" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Storage processors</div><p>
     On some disk arrays, the storage array manages the traffic through storage
     processors. One processor is active and the other one is passive until
     there is a failure. If you are connected to the passive storage processor,
     you might not see the expected LUNs, or you might see the LUNs but
     encounter I/O errors when you try to access them.
    </p><p>
     If a disk array has more than one storage processor, ensure that the SAN
     switch has a connection to the active storage processor that owns the LUNs
     you want to access.
    </p></div></section><section class="sect2" id="sec-multipath-planning-raid" data-id-title="Software RAID and complex storage stacks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.4 </span><span class="title-name">Software RAID and complex storage stacks</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning-raid">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Multipathing is set up on top of basic storage devices such as SCSI disks.
    In a multi-layered storage stack, multipathing is always the bottom layer.
    Other layers such as software RAID, Logical Volume Management, block device
    encryption, etc. are layered on top of it. Therefore, for each device that
    has multiple I/O paths and that you plan to use in a software RAID, you
    must configure the device for multipathing before you attempt to create the
    software RAID device.
   </p></section><section class="sect2" id="sec-multipath-planning-ha" data-id-title="High-availability solutions"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.5 </span><span class="title-name">High-availability solutions</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-planning-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    High-availability solutions for clustering storage resources run on top of
    the multipathing service on each node. Make sure that the configuration
    settings in the <code class="filename">/etc/multipath.conf</code> file on each node
    are consistent across the cluster.
   </p><p>
    Make sure that multipath devices have the same name across all devices.
    Refer to <a class="xref" href="#sec-multipath-names" title="18.12. Multipath device names and WWIDs">Section 18.12, “Multipath device names and WWIDs”</a> for details.
   </p><p>
    The Distributed Replicated Block Device (DRBD) high-availability solution
    for mirroring devices across a LAN runs on top of multipathing. For each
    device that has multiple I/O paths and that you plan to use in a DRDB
    solution, you must configure the device for multipathing before you
    configure DRBD.
   </p><p>
    Special care must be taken when using multipathing together with clustering
    software that relies on shared storage for fencing, such as
    <code class="command">pacemaker</code> with <code class="command">sbd</code>. See
    <a class="xref" href="#sec-multipath-policies-cluster" title="18.9.2. Queuing policy on clustered servers">Section 18.9.2, “Queuing policy on clustered servers”</a> for details.
   </p></section></section><section class="sect1" id="sec-multipath-installing" data-id-title="Installing SUSE Linux Enterprise Server on multipath systems"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.4 </span><span class="title-name">Installing <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> on multipath systems</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-installing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   No special installation parameters are required for the installation of
   <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> on systems with multipath hardware.
  </p><section class="sect2" id="sec-multipath-installing-nomp" data-id-title="Installing without connected multipath devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.4.1 </span><span class="title-name">Installing without connected multipath devices</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-installing-nomp">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You may want to perform installation on a local disk, without configuring
    the fabric and the storage first, with the intention to add multipath SAN
    devices to the system later. In this case, the installation will proceed
    like on a non-multipath system. After installation,
    <code class="systemitem">multipath-tools</code> will be installed, but the
    <code class="systemitem">systemd</code> service <code class="filename">multipathd.service</code> will be disabled.
    The system will be configured as described in
    <a class="xref" href="#vl-multipath-planning-type-noroot-noinitrd">Multipath disabled in the initramfs</a> in
    <a class="xref" href="#sec-multipath-planning-type-noroot" title="18.3.2.2. Root file system on a local disk">Section 18.3.2.2, “Root file system on a local disk”</a>. Before adding SAN
    hardware, you will need to enable and start
    <code class="filename">multipathd.service</code>. We recommend creating a
    <code class="literal">blacklist</code> entry in the
    <code class="filename">/etc/multipath.conf</code> for the root device (see
    <a class="xref" href="#sec-multipath-blacklist" title="18.11.1. The blacklist section in multipath.conf">Section 18.11.1, “The <code class="literal">blacklist</code> section in <code class="filename">multipath.conf</code>”</a>).
   </p></section><section class="sect2" id="sec-multipath-installing-mp" data-id-title="Installing with connected multipath devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.4.2 </span><span class="title-name">Installing with connected multipath devices</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-installing-mp">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If multipath devices are connected to the system at installation time,
    YaST will detect them and display a pop-up window asking you whether
    multipath should be enabled before entering the partitioning stage.
   </p><div class="informalfigure"><div class="mediaobject"><a href="images/multipath-popup.png"><img src="images/multipath-popup.png" width="50%" alt="YaST multipath dialog" title="YaST multipath dialog"/></a></div></div><p>
    If you select “No” at this prompt (not recommended), the
    installation will proceed as in
    <a class="xref" href="#sec-multipath-installing-nomp" title="18.4.1. Installing without connected multipath devices">Section 18.4.1, “Installing without connected multipath devices”</a>. In the partitioning stage,
    do not use/edit devices that will later be part of a multipath map.
   </p><p>
    If you select “Yes” at the multipath prompt,
    <code class="command">multipathd</code> will run during the installation. No device
    will be added to the <code class="literal">blacklist</code> section of
    <code class="filename">/etc/multipath.conf</code>, thus all SCSI and DASD devices,
    including local disks, will appear as multipath devices in the partitioning
    dialogs. After installation, all SCSI and DASD devices will be multipath
    devices, as described in
    <a class="xref" href="#sec-multipath-planning-type-root" title="18.3.2.1. Root file system on multipath (SAN-boot)">Section 18.3.2.1, “Root file system on multipath (SAN-boot)”</a>.
   </p><div class="procedure" id="pr-multipath-disable-root" data-id-title="Disabling multipathing for the root disk after installation"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 18.1: </span><span class="title-name">Disabling multipathing for the root disk after installation </span></span><a title="Permalink" class="permalink" href="#pr-multipath-disable-root">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     This procedure assumes that you installed on a local disk and enabled
     multipathing during installation, so that the root device is on multipath
     now, but you prefer to set up the system as described in
     <a class="xref" href="#vl-multipath-planning-type-noroot-ignore">Local disk is excluded from multipath</a> in
     <a class="xref" href="#sec-multipath-planning-type-noroot" title="18.3.2.2. Root file system on a local disk">Section 18.3.2.2, “Root file system on a local disk”</a>.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Check your system for <code class="filename">/dev/mapper/...</code> references to
      your local root device, and replace them with references that will still
      work if the device is not a multipath map anymore (see
      <a class="xref" href="#sec-multipath-referring" title="18.12.4. Referring to multipath maps">Section 18.12.4, “Referring to multipath maps”</a>). If the following command
      finds no references, you do not need to apply changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> grep -rl /dev/mapper/ /etc</pre></div></li><li class="step"><p>
      Switch to <code class="literal">by-uuid</code> persistent device policy for
      <code class="command">dracut</code> (see
      <a class="xref" href="#sec-multipath-initrd-persistent" title="18.7.4.2. Persistent device names in the initramfs">Section 18.7.4.2, “Persistent device names in the initramfs”</a>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>echo 'persistent_policy="by-uuid"' | \
      sudo tee /etc/dracut.conf.d/10-persistent-policy.conf</pre></div></li><li class="step"><p>
      Determine the WWID of the root device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>multipathd show paths format "%i %d %w %s"
0:2:0:0 sda 3600605b009e7ed501f0e45370aaeb77f IBM,ServeRAID M5210
...</pre></div><p>
      This command prints all paths devices with their WWIDs and vendor/product
      information. You will be able to identify the root device (here, the
      ServeRAID device) and note the WWID.
     </p></li><li class="step"><p>
      Create a blacklist entry in <code class="filename">/etc/multipath.conf</code> (see
      <a class="xref" href="#sec-multipath-blacklist" title="18.11.1. The blacklist section in multipath.conf">Section 18.11.1, “The <code class="literal">blacklist</code> section in <code class="filename">multipath.conf</code>”</a>) with the WWID you just
      determined (do <span class="emphasis"><em>not</em></span> apply these settings just yet):
     </p><div class="verbatim-wrap"><pre class="screen">blacklist {
    wwid 3600605b009e7ed501f0e45370aaeb77f
}</pre></div></li><li class="step"><p>
      Rebuild the initramfs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> dracut -f</pre></div></li><li class="step"><p>
      Reboot. Your system should boot with a non-multipath root disk.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-multipath-offline-update" data-id-title="Updating SLE on multipath systems"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.5 </span><span class="title-name">Updating SLE on multipath systems</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-offline-update">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When updating your system online, you can proceed as described in
   <span class="intraxref">Book “Upgrade Guide”, Chapter 5 “Upgrading online”</span>.
  </p><p>
   The offline update of your system is similar to the fresh installation as
   described in <a class="xref" href="#sec-multipath-installing" title="18.4. Installing SUSE Linux Enterprise Server on multipath systems">Section 18.4, “Installing <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> on multipath systems”</a>. There is no
   <code class="literal">blacklist</code>, so if the user selects to enable multipath,
   the root device will appear as a multipath device, even if it is normally
   not one. When <code class="command">dracut</code> builds the initramfs during the update
   procedure, it sees a different storage stack than it would see on the booted
   system. See <a class="xref" href="#sec-multipath-initrd-persistent" title="18.7.4.2. Persistent device names in the initramfs">Section 18.7.4.2, “Persistent device names in the initramfs”</a> and
   <a class="xref" href="#sec-multipath-referring" title="18.12.4. Referring to multipath maps">Section 18.12.4, “Referring to multipath maps”</a>.
  </p></section><section class="sect1" id="sec-multipath-mpiotools" data-id-title="Multipath management tools"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.6 </span><span class="title-name">Multipath management tools</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-mpiotools">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The multipathing support in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> is based on the Device Mapper
   Multipath module of the Linux kernel and the
   <code class="systemitem">multipath-tools</code> user space package.
  </p><p>
   The generic multipathing capability is handled by the Device Mapper
   Multipath (DM-MP) module. For details, refer to
   <a class="xref" href="#sec-multipath-mpiotools-dm" title="18.6.1. Device mapper multipath module">Section 18.6.1, “Device mapper multipath module”</a>.
  </p><p>
   The packages <code class="systemitem">multipath-tools</code> and
   <code class="systemitem">kpartx</code> provide tools that handle
   automatic path discovery and grouping. The tools are the following:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.9.5.1"><span class="term"><code class="command">multipathd</code></span></dt><dd><p>
      The daemon to set up and monitor multipath maps, and a command-line
      client to communicate with the daemon process. See
      <a class="xref" href="#sec-multipath-mpiotools-multipathd" title="18.6.2. The multipathd daemon">Section 18.6.2, “The <code class="command">multipathd</code> daemon”</a>.
     </p></dd><dt id="id-1.11.6.6.9.5.2"><span class="term"><code class="command">multipath</code></span></dt><dd><p>
      The command-line tool for multipath operations. See
      <a class="xref" href="#sec-multipath-mpiotools-multipath" title="18.6.3. The multipath command">Section 18.6.3, “The <code class="command">multipath</code> command”</a>.
     </p></dd><dt id="id-1.11.6.6.9.5.3"><span class="term"><code class="command">kpartx</code></span></dt><dd><p>
      The command-line tool for managing “partitions” on multipath
      devices. See <a class="xref" href="#sec-multipath-configuration-partitioning" title="18.7.3. Partitions on multipath devices and kpartx">Section 18.7.3, “Partitions on multipath devices and <code class="command">kpartx</code>”</a>.
     </p></dd><dt id="id-1.11.6.6.9.5.4"><span class="term"><code class="command">mpathpersist</code></span></dt><dd><p>
      The command-line tool for managing SCSI persistent reservations. See
      <a class="xref" href="#sec-multipath-mpiotools-mpathpersist" title="18.6.4. SCSI persistent reservations and mpathpersist">Section 18.6.4, “SCSI persistent reservations and <code class="command">mpathpersist</code>”</a>.
     </p></dd></dl></div><section class="sect2" id="sec-multipath-mpiotools-dm" data-id-title="Device mapper multipath module"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.6.1 </span><span class="title-name">Device mapper multipath module</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-mpiotools-dm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The Device Mapper Multipath (DM-MP) module
    <code class="filename">dm-multipath.ko</code> provides the generic multipathing
    capability for Linux. DM-MPIO is the preferred solution for multipathing on
    <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> for SCSI and DASD devices, and can be used for NVMe devices
    as well.
   </p><div id="id-1.11.6.6.9.6.3" data-id-title="Using DM-MP for NVMe devices" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Using DM-MP for NVMe devices</div><p>
     Since <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 15, native NVMe multipathing (see
     <a class="xref" href="#sec-multipath-hardware-implementations" title="18.2.1. Multipath implementations: device mapper and NVMe">Section 18.2.1, “Multipath implementations: device mapper and NVMe”</a>) has been
     recommended for NVMe and used by default. To disable native NVMe
     multipathing and use device mapper multipath instead (<span class="emphasis"><em>not
     recommended</em></span>), boot with the kernel parameter
     <code class="literal">nvme-core.multipath=0</code>.
    </p></div><p>
    The Device Mapper Multipath module handles the following tasks:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Distributing load over multiple paths inside the active path group.
     </p></li><li class="listitem"><p>
      Noticing I/O errors on path devices, and marking these as failed, so that
      no I/O will be sent to them.
     </p></li><li class="listitem"><p>
      Switching path groups when all paths in the active path group have
      failed.
     </p></li><li class="listitem"><p>
      Either failing or queuing I/O on the multipath device if all paths have
      failed, depending on configuration.
     </p></li></ul></div><p>
    The following tasks are handled by the user-space components in the
    <code class="systemitem">multipath-tools</code> package, not by the Device Mapper
    Multipath module:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Discovering devices representing different paths to the same storage
      device and assembling multipath maps from them.
     </p></li><li class="listitem"><p>
      Collecting path devices with similar properties into path groups.
     </p></li><li class="listitem"><p>
      Monitoring path devices actively for failure or reinstantiation.
     </p></li><li class="listitem"><p>
      Monitoring of addition and removal of path devices.
     </p></li><li class="listitem"><p>
      The Device Mapper Multipath module does not provide an easy-to-use user
      interface for setup and configuration.
     </p></li></ul></div><p>
    For details about the components from the
    <code class="systemitem">multipath-tools</code> package, refer to
    <a class="xref" href="#sec-multipath-mpiotools-multipathd" title="18.6.2. The multipathd daemon">Section 18.6.2, “The <code class="command">multipathd</code> daemon”</a>.
   </p><div id="id-1.11.6.6.9.6.9" data-id-title="Failures that multipath prevents" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Failures that multipath prevents</div><p>
     DM-MPIO protects against failures in the paths to the device, and not
     failures in the device itself, such as media errors. The latter kind of
     errors must be prevented by other means, such as replication.
    </p></div></section><section class="sect2" id="sec-multipath-mpiotools-multipathd" data-id-title="The multipathd daemon"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.6.2 </span><span class="title-name">The <code class="command">multipathd</code> daemon</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-mpiotools-multipathd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <code class="command">multipathd</code> is the most important part of a modern Linux
    device mapper multipath setup. It is normally started through the systemd
    service <code class="filename">multipathd.service</code> (see
    <a class="xref" href="#sec-multipath-configuration-start" title="18.7.1. Enabling, starting, and stopping multipath services">Section 18.7.1, “Enabling, starting, and stopping multipath services”</a>).
   </p><p>
    <code class="command">multipathd</code> serves the following tasks (some of them
    depend on the configuration):
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      On startup, detects path devices and sets up multipath maps from detected
      devices.
     </p></li><li class="listitem"><p>
      Monitors uevents and device mapper events, adding or removing path
      mappings to multipath maps as necessary and initiating failover or
      failback operations.
     </p></li><li class="listitem"><p>
      Sets up new maps on the fly when new path devices are discovered.
     </p></li><li class="listitem"><p>
      Checks path devices at regular intervals to detect failure, and tests
      failed paths to reinstate them if they become operational again.
     </p></li><li class="listitem"><p>
      When all paths fail, <code class="command">multipathd</code> either fails the map,
      or switches the map device to queuing mode for a given time interval.
     </p></li><li class="listitem"><p>
      Handles path state changes and switches path groups or regroups paths, as
      necessary.
     </p></li><li class="listitem"><p>
      Tests paths for “marginal” state, i.e. shaky fabrics
      conditions that cause path state flipping between operational and
      non-operational.
     </p></li><li class="listitem"><p>
      Handles SCSI persistent reservation keys for path devices if configured.
      See <a class="xref" href="#sec-multipath-mpiotools-mpathpersist" title="18.6.4. SCSI persistent reservations and mpathpersist">Section 18.6.4, “SCSI persistent reservations and <code class="command">mpathpersist</code>”</a>.
     </p></li></ul></div><p>
    <code class="command">multipathd</code> also serves as a command-line client to
    process interactive commands by sending them to the running daemon. The
    general syntax to send commands to the daemon is as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd <em class="replaceable">COMMAND</em></pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd -k'<em class="replaceable">COMMAND</em>'</pre></div><p>
    There is also an interactive mode that allows sending multiple subsequent
    commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd -k</pre></div><div id="id-1.11.6.6.9.7.11" data-id-title="How multipath and multipathd work together" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: How multipath and multipathd work together</div><p>
     Many <code class="command">multipathd</code> commands have
     <code class="command">multipath</code> equivalents. For example, <code class="command">multipathd
     show topology</code> does the same thing as <code class="command">multipath
     -ll</code>. The notable difference is that the multipathd command
     inquires the internal state of the running <code class="command">multipathd</code>
     daemon, whereas multipath obtains information directly from the kernel and
     I/O operations.
    </p><p>
     If the multipath daemon is running, we recommend making modifications to
     the system by using the <code class="command">multipathd</code> commands. Otherwise,
     the daemon may notice configuration changes and react to them. In some
     situations, the daemon might even try to undo the applied changes.
     <code class="command">multipath</code> automatically delegates certain possibly
     dangerous commands, like destroying and flushing maps, to
     <code class="command">multipathd</code> if a running daemon is detected.
    </p></div><p>
    The list below describes frequently used <code class="command">multipathd</code>
    commands:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.9.7.13.1"><span class="term">show topology</span></dt><dd><p>
       Shows the current map topology and properties. See
       <a class="xref" href="#sec-multipath-best-practice-status" title="18.14.2. Interpreting multipath I/O status">Section 18.14.2, “Interpreting multipath I/O status”</a>.
      </p></dd><dt id="id-1.11.6.6.9.7.13.2"><span class="term">show paths</span></dt><dd><p>
       Shows the currently known path devices.
      </p></dd><dt id="id-1.11.6.6.9.7.13.3"><span class="term">show paths format "<em class="replaceable">FORMAT STRING</em>"</span></dt><dd><p>
       Shows the currently known path devices using a format string. Use
       <code class="command">show wildcards</code> to see a list of supported format
       specifiers.
      </p></dd><dt id="id-1.11.6.6.9.7.13.4"><span class="term">show maps</span></dt><dd><p>
       Shows the currently configured map devices.
      </p></dd><dt id="id-1.11.6.6.9.7.13.5"><span class="term">show maps format <em class="replaceable">FORMAT STRING</em></span></dt><dd><p>
       Shows the currently configured map devices using a format string. Use
       <code class="command">show wildcards</code> to see a list of supported format
       specifiers.
      </p></dd><dt id="id-1.11.6.6.9.7.13.6"><span class="term">show config local</span></dt><dd><p>
       Shows the current configuration that multipathd is using.
      </p></dd><dt id="id-1.11.6.6.9.7.13.7"><span class="term">reconfigure</span></dt><dd><p>
       Rereads configuration files, rescans devices, and sets up maps again.
       This is basically equivalent to a restart of
       <code class="command">multipathd</code>. A few options cannot be modified without
       a restart. They are mentioned in the man page
       <code class="systemitem">multipath.conf(5)</code>. The
       <code class="option">reconfigure</code> command reloads only map devices that have
       changed in some way. To force the reloading of every map device, use
       <code class="command">reconfigure all</code> (available since SUSE Linux Enterprise Server 15 SP4;
       on previous versions, <code class="option">reconfigure</code> reloaded every map).
      </p></dd><dt id="id-1.11.6.6.9.7.13.8"><span class="term">del map <em class="replaceable">MAP DEVICE NAME</em></span></dt><dd><p>
       Unconfigure and delete the given map device and its partitions.
       <em class="replaceable">MAP DEVICE NAME</em> can be a device node name
       like <code class="filename">dm-0</code>, a WWID, or a map name. The command fails
       if the device is in use.
      </p></dd><dt id="id-1.11.6.6.9.7.13.9"><span class="term">switchgroup map <em class="replaceable">MAP DEVICE NAME</em> group <em class="replaceable">N</em></span></dt><dd><p>
       Switch to the path group with the given numeric index (starting at 1).
       This is useful for maps with manual failback (see
       <a class="xref" href="#sec-multipath-policies-default" title="18.9. Configuring policies for failover, queuing, and failback">Section 18.9, “Configuring policies for failover, queuing, and failback”</a>).
      </p></dd></dl></div><p>
    Additional commands are available to modify path states, enable or disable
    queuing, and more. See <code class="systemitem">multipathd(8)</code> for details.
   </p></section><section class="sect2" id="sec-multipath-mpiotools-multipath" data-id-title="The multipath command"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.6.3 </span><span class="title-name">The <code class="command">multipath</code> command</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-mpiotools-multipath">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Even though multipath setup is mostly automatic and handled by
    <code class="command">multipathd</code>, <code class="command">multipath</code> is still useful
    for some administration tasks. Several examples of the command usage
    follow:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.9.8.3.1"><span class="term">multipath</span></dt><dd><p>
       Detects path devices and configures all multipath maps that it finds.
      </p></dd><dt id="id-1.11.6.6.9.8.3.2"><span class="term">multipath -d</span></dt><dd><p>
       Similar to <code class="command">multipath</code>, but does not set up any maps
       (“dry run”).
      </p></dd><dt id="id-1.11.6.6.9.8.3.3"><span class="term">multipath <em class="replaceable">DEVICENAME</em></span></dt><dd><p>
       Configures a specific multipath device.
       <em class="replaceable">DEVICENAME</em> can denote a member path device by
       its device node name (<code class="filename">/dev/sdb</code>) or device number in
       <code class="literal">major:minor</code> format. Alternatively, it can be the WWID
       or name of a multipath map.
      </p></dd><dt id="id-1.11.6.6.9.8.3.4"><span class="term">multipath -f <em class="replaceable">DEVICENAME</em></span></dt><dd><p>
       Unconfigures ("flushes") a multipath map and its partition mappings. The
       command will fail if the map or one of its partitions is in use. See
       above for possible values of <em class="replaceable">DEVICENAME</em>.
      </p></dd><dt id="id-1.11.6.6.9.8.3.5"><span class="term">multipath -F</span></dt><dd><p>
       Unconfigures ("flushes") all multipath maps and their partition
       mappings. The command will fail for maps in use.
      </p></dd><dt id="id-1.11.6.6.9.8.3.6"><span class="term">multipath -ll</span></dt><dd><p>
       Displays the status and topology of all currently configured multipath
       devices. See <a class="xref" href="#sec-multipath-best-practice-status" title="18.14.2. Interpreting multipath I/O status">Section 18.14.2, “Interpreting multipath I/O status”</a>.
      </p></dd><dt id="id-1.11.6.6.9.8.3.7"><span class="term">multipath -ll <em class="replaceable">DEVICENAME</em></span></dt><dd><p>
       Displays the status of a specified multipath device. See above for
       possible values of <em class="replaceable">DEVICENAME</em>.
      </p></dd><dt id="id-1.11.6.6.9.8.3.8"><span class="term">multipath -t</span></dt><dd><p>
       Shows the internal hardware table and the active configuration of
       multipath. Refer to <code class="systemitem">multipath.conf(5)</code> for
       details about the configuration parameters.
      </p></dd><dt id="id-1.11.6.6.9.8.3.9"><span class="term">multipath -T</span></dt><dd><p>
       Has a similar function to the <code class="command">multipath -t</code> command
       but shows only hardware entries matching the hardware detected on the
       host.
      </p></dd></dl></div><p>
    The option <code class="option">-v</code> controls the verbosity of the output. The
    provided value overrides the <code class="option">verbosity</code> option in
    <code class="filename">/etc/multipath.conf</code>. See
    <a class="xref" href="#sec-multipath-conf-misc" title="18.13. Miscellaneous options">Section 18.13, “Miscellaneous options”</a>.
   </p></section><section class="sect2" id="sec-multipath-mpiotools-mpathpersist" data-id-title="SCSI persistent reservations and mpathpersist"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.6.4 </span><span class="title-name">SCSI persistent reservations and <code class="command">mpathpersist</code></span></span> <a title="Permalink" class="permalink" href="#sec-multipath-mpiotools-mpathpersist">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="command">mpathpersist</code> utility is used to manage SCSI
    persistent reservations on Device Mapper Multipath devices. Persistent
    reservations serve to restrict access to SCSI Logical Units to certain SCSI
    initiators. In multipath configurations, it is important to use the same
    reservation keys for all I_T nexuses (paths) for a given volume; otherwise,
    creating a reservation on one path device would cause I/O errors on other
    paths.
   </p><p>
    Use this utility with the <code class="literal">reservation_key</code> attribute in
    the <code class="filename">/etc/multipath.conf</code> file to set persistent
    reservations for SCSI devices. If (and only if) this option is set, the
    <code class="command">multipathd</code> daemon checks persistent reservations for
    newly discovered paths or reinstated paths.
   </p><p>
    You can add the attribute to the <code class="literal">defaults</code> section or the
    <code class="literal">multipaths</code> section of
    <code class="filename">multipath.conf</code>. For example:
   </p><div class="verbatim-wrap"><pre class="screen">multipaths {
    multipath {
        wwid             3600140508dbcf02acb448188d73ec97d
        alias            yellow
        reservation_key  0x123abc
    }
}</pre></div><p>
    After setting the <code class="literal">reservation_key</code> parameter for all
    mpath devices applicable for persistent management, reload the
    configuration using <code class="command">multipathd reconfigure</code>.
   </p><div id="id-1.11.6.6.9.9.7" data-id-title="Using “reservation_key file”" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Using “<code class="option">reservation_key file</code>”</div><p>
     If the special value <code class="literal">reservation_key file</code> is used in
     the <code class="literal">defaults</code> section of
     <code class="filename">multipath.conf</code>, reservation keys can be managed
     dynamically in the file <code class="filename">/etc/multipath/prkeys</code> using
     <code class="command">mpathpersist</code>.
    </p><p>
     This is the recommended way to handle persistent reservations with
     multipath maps. It is available from <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12 SP4.
    </p></div><p>
    Use the command <code class="command">mpathpersist</code> to query and set persistent
    reservations for multipath maps consisting of SCSI devices. Refer to the
    manual page <code class="systemitem">mpathpersist(8)</code> for details. The
    command-line options are the same as those of the
    <code class="command">sg_persist</code> from the <code class="systemitem">sg3_utils</code>
    package. The <code class="systemitem">sg_persist(8)</code> manual page explains
    the semantics of the options in detail.
   </p><p>
    In the following examples, <em class="replaceable">DEVICE</em> denotes a
    device mapper multipath device like
    <code class="filename">/dev/mapper/mpatha</code>. The commands below are listed with
    long options for better readability. All options have single-letter
    replacements, like in <code class="command">mpathpersist -oGS 123abc
    <em class="replaceable">DEVICE</em></code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.9.9.10.1"><span class="term">mpathpersist --in --read-keys <em class="replaceable">DEVICE</em></span></dt><dd><p>
       Read the registered reservation keys for the device.
      </p></dd><dt id="id-1.11.6.6.9.9.10.2"><span class="term">mpathpersist --in --read-reservation <em class="replaceable">DEVICE</em></span></dt><dd><p>
       Show existing reservations for the device.
      </p></dd><dt id="id-1.11.6.6.9.9.10.3"><span class="term">mpathpersist --out --register --param-sark=123abc <em class="replaceable">DEVICE</em></span></dt><dd><p>
       Register a reservation key for the device. This will add the reservation
       key for all I_T nexuses (path devices) on the host.
      </p></dd><dt id="id-1.11.6.6.9.9.10.4"><span class="term">mpathpersist --out --reserve --param-rk=123abc --prout-type=5 <em class="replaceable">DEVICE</em></span></dt><dd><p>
       Create a reservation of type 5 (“write exclusive - registrants
       only”) for the device, using the previously registered key.
      </p></dd><dt id="id-1.11.6.6.9.9.10.5"><span class="term">mpathpersist --out --release --param-rk=123abc --prout-type=5 <em class="replaceable">DEVICE</em></span></dt><dd><p>
       Release a reservation of type 5 for the device.
      </p></dd><dt id="id-1.11.6.6.9.9.10.6"><span class="term">mpathpersist --out --register-ignore --param-sark=0 <em class="replaceable">DEVICE</em></span></dt><dd><p>
       Delete a previously existing reservation key from the device.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-multipath-config" data-id-title="Configuring the system for multipathing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.7 </span><span class="title-name">Configuring the system for multipathing</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-multipath-configuration-start" data-id-title="Enabling, starting, and stopping multipath services"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.7.1 </span><span class="title-name">Enabling, starting, and stopping multipath services</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-configuration-start">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To enable multipath services to start at boot time, run the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl enable multipathd</pre></div><p>
    To manually start the service in the running system, enter:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl start multipathd</pre></div><p>
    To restart the service, enter:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart multipathd</pre></div><p>
    In most situations, restarting the service is not necessary. To simply have
    <code class="command">multipathd</code> reload its configuration, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl reload multipathd</pre></div><p>
    To check the status of the service, enter:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl status multipathd</pre></div><p>
    To stop the multipath services in the current session, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl stop multipathd multipathd.socket</pre></div><p>
    Stopping the service does not remove existing multipath maps. To remove
    unused maps, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipath -F</pre></div><div id="id-1.11.6.6.10.3.16" data-id-title="Keep multipathd.service enabled" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Keep <code class="filename">multipathd.service</code> enabled</div><p>
     We strongly recommend keeping <code class="filename">multipathd.service</code>
     always enabled and running on systems with multipath hardware. The service
     does support <code class="command">systemd</code>'s socket activation mechanism, but
     we do not recommend that you rely on that. Multipath maps will not be set up
     during boot if the service is disabled.
    </p></div><div id="ann-multipath-configuration-disable" data-id-title="Disabling multipath" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Disabling multipath</div><p>
     If you need to disable multipath despite the warning above, for example,
     because a third-party multipathing software is going to be deployed,
     proceed as follows. Be sure that the system uses no hard-coded references
     to multipath devices (see <a class="xref" href="#sec-multipath-trouble-ref" title="18.15.2. Understanding device referencing issues">Section 18.15.2, “Understanding device referencing issues”</a>).
    </p><p>
     To disable multipathing <span class="emphasis"><em>just for a single system
     boot</em></span>, use the kernel parameter
     <code class="literal">multipath=off</code>. This affects both the booted system and
     the initramfs, which does not need to be rebuilt in this case.
    </p><p>
     To disable multipathd services <span class="emphasis"><em>permanently</em></span>, so that
     they will not be started on future system boots, run the following
     commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl disable multipathd multipathd.socket
<code class="prompt user">&gt; </code><code class="command">sudo</code> dracut --force --omit multipath</pre></div><p>
     (Whenever you disable or enable the multipath services, rebuild the
     <code class="systemitem">initramfs</code>. See
     <a class="xref" href="#sec-multipath-initrd-sync" title="18.7.4. Keeping the initramfs synchronized">Section 18.7.4, “Keeping the initramfs synchronized”</a>.)
    </p><p>
     If you want to make sure multipath devices do not get set up,
     <span class="emphasis"><em>even when running <code class="command">multipath</code>
     manually</em></span>, add the following lines at the end of
     <code class="filename">/etc/multipath.conf</code> before rebuilding the initramfs:
    </p><div class="verbatim-wrap"><pre class="screen">blacklist {
    wwid .*
}</pre></div></div></section><section class="sect2" id="sec-multipath-configuration-sandevs" data-id-title="Preparing SAN devices for multipathing"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.7.2 </span><span class="title-name">Preparing SAN devices for multipathing</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-configuration-sandevs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before configuring multipath I/O for your SAN devices, prepare the SAN
    devices, as necessary, by doing the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Configure and zone the SAN with the vendor’s tools.
     </p></li><li class="listitem"><p>
      Configure permissions for host LUNs on the storage arrays with the
      vendor’s tools.
     </p></li><li class="listitem"><p>
      If <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> ships no driver for the host bus adapter (HBA), install
      a Linux driver from the HBA vendor. See the vendor’s specific
      instructions for more details.
     </p></li></ul></div><p>
    If multipath devices are detected and
    <code class="filename">multipathd.service</code> is enabled, multipath maps should
    be created automatically. If this does not happen,
    <a class="xref" href="#sec-multipath-trouble-steps" title="18.15.3. Troubleshooting steps in emergency mode">Section 18.15.3, “Troubleshooting steps in emergency mode”</a>
    lists some shell commands that can be used to examine the situation. When
    the LUNs are not seen by the HBA driver, check the zoning setup in the SAN.
    In particular, check whether LUN masking is active and whether the LUNs are
    correctly assigned to the server.
   </p><p>
    If the HBA driver can see LUNs, but no corresponding block devices are
    created, additional kernel parameters may be needed. See <em class="citetitle">TID
    3955167: Troubleshooting SCSI (LUN) Scanning Issues</em> in the
    SUSE Knowledge base at
    <a class="link" href="https://www.suse.com/support/kb/doc.php?id=3955167" target="_blank">https://www.suse.com/support/kb/doc.php?id=3955167</a>.
   </p></section><section class="sect2" id="sec-multipath-configuration-partitioning" data-id-title="Partitions on multipath devices and kpartx"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.7.3 </span><span class="title-name">Partitions on multipath devices and <code class="command">kpartx</code></span></span> <a title="Permalink" class="permalink" href="#sec-multipath-configuration-partitioning">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Multipath maps can have partitions like their path devices. Partition table
    scanning and device node creation for partitions is done in user space by
    the <code class="command">kpartx</code> tool. <code class="command">kpartx</code> is
    automatically invoked by udev rules; there is usually no need to run it
    manually. See <a class="xref" href="#sec-multipath-referring" title="18.12.4. Referring to multipath maps">Section 18.12.4, “Referring to multipath maps”</a> for ways to refer
    to multipath partitions.
   </p><div id="id-1.11.6.6.10.5.3" data-id-title="Disabling invocation of kpartx" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Disabling invocation of <code class="command">kpartx</code></div><p>
     The <code class="literal">skip_kpartx</code> option in
     <code class="filename">/etc/multipath.conf</code> can be used to disable invocation
     of <code class="command">kpartx</code> on selected multipath maps. This may be
     useful on virtualization hosts, for example.
    </p></div><p>
    Partition tables and partitions on multipath devices can be manipulated as
    usual, using YaST or tools like <code class="command">fdisk</code> or
    <code class="command">parted</code>. Changes applied to the partition table will be
    noted by the system when the partitioning tool exits. If this does not work
    (usually because a device is busy), try <code class="command">multipathd
    reconfigure</code>, or reboot the system.
   </p></section><section class="sect2" id="sec-multipath-initrd-sync" data-id-title="Keeping the initramfs synchronized"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.7.4 </span><span class="title-name">Keeping the initramfs synchronized</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-initrd-sync">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.11.6.6.10.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     Make sure that the initial RAM file system (initramfs) and the booted
     system behave consistently regarding the use of multipathing for all block
     devices. Rebuild the initramfs after applying multipath configuration
     changes.
    </p></div><p>
    If multipathing is enabled in the system, it also needs to be enabled in
    the <code class="filename">initramfs</code> and vice versa. The only exception to
    this rule is the option
    <a class="xref" href="#vl-multipath-planning-type-noroot-noinitrd">Multipath disabled in the initramfs</a> in
    <a class="xref" href="#sec-multipath-planning-type-noroot" title="18.3.2.2. Root file system on a local disk">Section 18.3.2.2, “Root file system on a local disk”</a>.
   </p><p>
    The multipath configuration must be synchronized between the booted system
    and the initramfs. Therefore, if you change any of the files:
    <code class="filename">/etc/multipath.conf</code>,
    <code class="filename">/etc/multipath/wwids</code>,
    <code class="filename">/etc/multipath/bindings</code>, or other configuration files,
    or udev rules related to device identification, rebuild initramfs using the
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> dracut -f</pre></div><p>
    If the <code class="filename">initramfs</code> and the system are not synchronized,
    the system will not boot properly, and the start-up procedure may result in
    an emergency shell. See <a class="xref" href="#sec-multipath-trouble" title="18.15. Troubleshooting MPIO">Section 18.15, “Troubleshooting MPIO”</a> for
    instructions on how to avoid or repair such a scenario.
   </p><section class="sect3" id="sec-multipath-initrd-disable" data-id-title="Enabling or disabling multipathing in the initramfs"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">18.7.4.1 </span><span class="title-name">Enabling or disabling multipathing in the initramfs</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-initrd-disable">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Special care must be taken if the initramfs is rebuilt in non-standard
     situations, for example, from a rescue system or after booting with the
     kernel parameter <code class="literal">multipath=off</code>.
     <code class="command">dracut</code> will automatically include multipathing support
     in the initramfs if and only if it detects that the root file system is on
     a multipath device while the initramfs is being built. In such cases, it
     is necessary to enable or disable multipathing explicitly.
    </p><p>
     To enable multipath support in the <code class="filename">initramfs</code>, run the
     command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> dracut --force --add multipath</pre></div><p>
     To disable multipath support in <code class="filename">initramfs</code>, run the
     command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> dracut --force --omit multipath</pre></div></section><section class="sect3" id="sec-multipath-initrd-persistent" data-id-title="Persistent device names in the initramfs"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">18.7.4.2 </span><span class="title-name">Persistent device names in the initramfs</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-initrd-persistent">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     When <code class="command">dracut</code> generates the initramfs, it must refer to
     disks and partitions to be mounted in a persistent manner, to make sure
     the system will boot correctly. When <code class="command">dracut</code> detects
     multipath devices, it will use the DM-MP device names such as
    </p><div class="verbatim-wrap"><pre class="screen">/dev/mapper/3600a098000aad73f00000a3f5a275dc8-part1</pre></div><p>
     for this purpose by default. This is good if the system
     <span class="emphasis"><em>always</em></span> runs in multipath mode. But if the system is
     started without multipathing, as described in
     <a class="xref" href="#sec-multipath-initrd-disable" title="18.7.4.1. Enabling or disabling multipathing in the initramfs">Section 18.7.4.1, “Enabling or disabling multipathing in the initramfs”</a>, booting with such an
     initramfs will fail, because the <code class="filename">/dev/mapper</code> devices
     will not exist. See <a class="xref" href="#sec-multipath-referring" title="18.12.4. Referring to multipath maps">Section 18.12.4, “Referring to multipath maps”</a> for another
     possible problem scenario, and some background information.
    </p><p>
     To prevent this from happening, change <code class="command">dracut</code>'s
     persistent device naming policy by using the
     <code class="option">--persistent-policy</code> option. We recommend setting the
     <code class="literal">by-uuid</code> use policy:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> dracut --force --omit multipath --persistent-policy=by-uuid</pre></div><p>
     See also <a class="xref" href="#pr-multipath-disable-root" title="Disabling multipathing for the root disk after installation">Procedure 18.1, “Disabling multipathing for the root disk after installation”</a> and
     <a class="xref" href="#sec-multipath-trouble-ref" title="18.15.2. Understanding device referencing issues">Section 18.15.2, “Understanding device referencing issues”</a>.
    </p></section></section></section><section class="sect1" id="sec-multipath-conf-file" data-id-title="Multipath configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.8 </span><span class="title-name">Multipath configuration</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-file">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The built-in <code class="systemitem">multipath-tools</code> defaults work well for
   most setups. If customizations are needed, a configuration file needs to be
   created. The main configuration file is
   <code class="filename">/etc/multipath.conf</code>. In addition, files in
   <code class="filename">/etc/multipath/conf.d/</code> are taken into account. See
   <a class="xref" href="#sec-multipath-conf-file-precedence" title="18.8.2.1. Additional configuration files and precedence rules">Section 18.8.2.1, “Additional configuration files and precedence rules”</a> for additional
   information.
  </p><div id="id-1.11.6.6.11.3" data-id-title="Vendor recommendations and built-in hardware defaults" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Vendor recommendations and built-in hardware defaults</div><p>
    Some storage vendors publish recommended values for multipath options in
    their documentation. These values often represent what the vendor has
    tested in their environment and found most suitable for the storage product.
    See the disclaimer in <a class="xref" href="#sec-multipath-hardware-autodetect" title="18.2.2. Storage array autodetection for multipathing">Section 18.2.2, “Storage array autodetection for multipathing”</a>.
   </p><p>
    <code class="systemitem">multipath-tools</code> has built-in defaults for many
    storage arrays that are derived from the published vendor recommendations.
    Run <code class="command">multipath -T</code> to see the current settings for your
    devices and compare them to vendor recommendations.
   </p></div><section class="sect2" id="sec-multipath-conf-file-create" data-id-title="Creating /etc/multipath.conf"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.8.1 </span><span class="title-name">Creating <code class="filename">/etc/multipath.conf</code></span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-file-create">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    It is recommended that you create a minimal
    <code class="filename">/etc/multipath.conf</code> that just contains those settings
    you want to change. In many cases, you do not need to create
    <code class="filename">/etc/multipath.conf</code> at all.
   </p><p>
    If you prefer working with a configuration template that contains all
    possible configuration directives, run:
   </p><div class="verbatim-wrap"><pre class="screen">multipath -T &gt;/etc/multipath.conf</pre></div><p>
    See also
    <a class="xref" href="#sec-multipath-best-practice-config" title="18.14.1. Best practices for configuration">Section 18.14.1, “Best practices for configuration”</a>.
   </p></section><section class="sect2" id="sec-multipath-conf-file-syntax" data-id-title="multipath.conf syntax"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.8.2 </span><span class="title-name"><code class="filename">multipath.conf</code> syntax</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-file-syntax">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="filename">/etc/multipath.conf</code> file uses a hierarchy of
    sections, subsections and option/value pairs.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      White space separates tokens. Consecutive white space characters are
      collapsed into a single space, unless quoted (see below).
     </p></li><li class="listitem"><p>
      The hash (<code class="literal">#</code>) and exclamation mark
      (<code class="literal">!</code>) characters cause the rest of the line to be
      discarded as a comment.
     </p></li><li class="listitem"><p>
      Sections and subsections are started with a section name and an opening
      brace (<code class="literal">{</code>) on the same line, and end with a closing
      brace (<code class="literal">}</code>) on a line on its own.
     </p></li><li class="listitem"><p>
      Options and values are written on one line. Line continuations are
      unsupported.
     </p></li><li class="listitem"><p>
      Options and section names must be keywords. The allowed keywords are
      documented in <code class="systemitem">multipath.conf(5)</code>.
     </p></li><li class="listitem"><p>
      Values may be enclosed in double quotes (<code class="literal">"</code>). They must
      be enclosed in quotes if they contain white space or comment characters.
      A double quote character inside a value is represented by a pair of
      double quotes (<code class="literal">""</code>).
     </p></li><li class="listitem"><p>
      The values of some options are POSIX regular expressions (see
      <code class="systemitem">regex(7)</code>). They are case sensitive and not
      anchored, so “<code class="literal">bar</code>” matches
      “<code class="literal">rhabarber</code>”, but not “Barbie”.
     </p></li></ul></div><p>
    The following example illustrates the syntax:
   </p><div class="verbatim-wrap"><pre class="screen">section {
    subsection {
        option1 value
        option2      "complex value!"
        option3    "value with ""quoted"" word"
    } ! subsection end
} # section end</pre></div><section class="sect3" id="sec-multipath-conf-file-precedence" data-id-title="Additional configuration files and precedence rules"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">18.8.2.1 </span><span class="title-name">Additional configuration files and precedence rules</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-file-precedence">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     After <code class="filename">/etc/multipath.conf</code>, the tools read files
     matching the pattern <code class="filename">/etc/multipath.conf.d/*.conf</code>.
     The additional files follow the same syntax rules as
     <code class="filename">/etc/multipath.conf</code>. Sections and options can occur
     multiple times. If <span class="emphasis"><em>the same option in the same
     section</em></span> is set in multiple files, or on multiple lines in the
     same file, the last value takes precedence. Separate precedence rules
     apply between <code class="filename">multipath.conf</code> sections. See below.
    </p></section></section><section class="sect2" id="sec-multipath-conf-file-sections" data-id-title="multipath.conf sections"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.8.3 </span><span class="title-name"><code class="filename">multipath.conf</code> sections</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-file-sections">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="filename">/etc/multipath.conf</code> file is organized into the
    following sections. Some options can occur in more than one section. See
    <code class="systemitem">multipath.conf(5)</code> for details.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.11.6.3.1"><span class="term">defaults</span></dt><dd><p>
       General default settings.
      </p><div id="id-1.11.6.6.11.6.3.1.2.2" data-id-title="Overriding built-in device properties" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Overriding built-in device properties</div><p>
        Built-in hardware-specific device properties take precedence over the
        settings in the <code class="literal">defaults</code> section. Changes must
        therefore be made in the <code class="literal">devices</code> section or in the
        <code class="literal">overrides</code> section.
       </p></div></dd><dt id="id-1.11.6.6.11.6.3.2"><span class="term">blacklist</span></dt><dd><p>
       Lists devices to ignore. See
       <a class="xref" href="#sec-multipath-blacklist" title="18.11.1. The blacklist section in multipath.conf">Section 18.11.1, “The <code class="literal">blacklist</code> section in <code class="filename">multipath.conf</code>”</a>.
      </p></dd><dt id="id-1.11.6.6.11.6.3.3"><span class="term">blacklist_exceptions</span></dt><dd><p>
       Lists devices to be multipathed even though they are matched by the
       blacklist. See
       <a class="xref" href="#sec-multipath-blacklist" title="18.11.1. The blacklist section in multipath.conf">Section 18.11.1, “The <code class="literal">blacklist</code> section in <code class="filename">multipath.conf</code>”</a>.
      </p></dd><dt id="id-1.11.6.6.11.6.3.4"><span class="term">devices</span></dt><dd><p>
       Settings specific to the storage controller. This section is a
       collection of <code class="literal">device</code> subsections. Values in this
       section override values for the same options in the
       <code class="filename">defaults</code> section, and the built-in settings of
       <code class="systemitem">multipath-tools</code>.
      </p><p>
       <code class="literal">device</code> entries in the <code class="literal">devices</code>
       section are matched against the vendor and product of a device using
       regular expressions. These entries will be “merged”, setting
       all options from matching sections for the device. If the same option is
       set in multiple matching <code class="literal">device</code> sections, the last
       device entry takes precedence, even if it is less “specific”
       than preceding entries. This applies also if the matching entries appear
       in different configuration files (see
       <a class="xref" href="#sec-multipath-conf-file-precedence" title="18.8.2.1. Additional configuration files and precedence rules">Section 18.8.2.1, “Additional configuration files and precedence rules”</a>). In the following
       example, a device <code class="literal">SOMECORP STORAGE</code> will use
       <code class="literal">fast_io_fail_tmo 15</code>.
      </p><div class="verbatim-wrap"><pre class="screen">devices {
    device {
        vendor SOMECORP
        product STOR
        fast_io_fail_tmo 10
    }
    device {
        vendor SOMECORP
        product .*
        fast_io_fail_tmo 15
    }
}</pre></div></dd><dt id="id-1.11.6.6.11.6.3.5"><span class="term">multipaths</span></dt><dd><p>
       Settings for individual multipath devices. This section is a list of
       <code class="literal">multipath</code> subsections. Values override the
       <code class="literal">defaults</code> and <code class="literal">devices</code> sections.
      </p></dd><dt id="id-1.11.6.6.11.6.3.6"><span class="term">overrides</span></dt><dd><p>
       Settings that override values from all other sections.
      </p></dd></dl></div></section><section class="sect2" id="sec-multipath-conf-file-apply" data-id-title="Applying multipath.conf modifications"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.8.4 </span><span class="title-name">Applying <code class="filename">multipath.conf</code> modifications</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-file-apply">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To apply the configuration changes, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd reconfigure</pre></div><p>
    Do not forget to synchronize with the configuration in the initramfs. See
    <a class="xref" href="#sec-multipath-initrd-sync" title="18.7.4. Keeping the initramfs synchronized">Section 18.7.4, “Keeping the initramfs synchronized”</a>.
   </p><div id="id-1.11.6.6.11.7.5" data-id-title="Do not apply settings using multipath" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Do not apply settings using <code class="command">multipath</code></div><p>
     Do not apply new settings with the <code class="command">multipath</code> command
     while <code class="command">multipathd</code> is running. This may result in an
     inconsistent and possibly broken setup.
    </p></div><div id="sec-multipath-conf-file-verify" data-id-title="Verifying a modified setup" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Verifying a modified setup</div><p>
     It is possible to test modified settings first before they are applied, by
     running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipath -d -v2</pre></div><p>
     This command shows new maps to be created with the proposed topology, but
     not whether maps will be removed/flushed. To obtain more information, run
     with increased verbosity:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipath -d -v3 2&gt;&amp;1 | less</pre></div></div></section></section><section class="sect1" id="sec-multipath-policies-default" data-id-title="Configuring policies for failover, queuing, and failback"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.9 </span><span class="title-name">Configuring policies for failover, queuing, and failback</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-policies-default">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The goal of multipath I/O is to provide connectivity fault tolerance between
   the storage system and the server. The desired default behavior depends on
   whether the server is a stand-alone server or a node in a high-availability
   cluster.
  </p><p>
   This section discusses the most important
   <code class="systemitem">multipath-tools</code> configuration parameters for
   achieving fault tolerance.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.12.4.1"><span class="term">polling_interval</span></dt><dd><p>
      The time interval (in seconds) between health checks for path devices.
      The default is 5 seconds. Failed devices are checked at this time
      interval. For healthy devices, the time interval may be increased up to
      <code class="literal">max_polling_interval</code> seconds.
     </p></dd><dt id="id-1.11.6.6.12.4.2"><span class="term">detect_checker</span></dt><dd><p>
      If this is set to <code class="literal">yes</code> (default, recommended),
      <code class="command">multipathd</code> automatically detects the best path
      checking algorithm.
     </p></dd><dt id="id-1.11.6.6.12.4.3"><span class="term">path_checker</span></dt><dd><p>
      The algorithm used to check path state. If you need to enable the
      checker, disable <code class="literal">detect_checker</code> as follows:
     </p><div class="verbatim-wrap"><pre class="screen">defaults {
          detect_checker no
}</pre></div><p>
      The following list contains only the most important algorithms. See
      <code class="systemitem">multipath.conf(5)</code> for the full list.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.12.4.3.2.4.1"><span class="term">tur</span></dt><dd><p>
         Send TEST UNIT READY command. This is the default for SCSI devices
         with ALUA support.
        </p></dd><dt id="id-1.11.6.6.12.4.3.2.4.2"><span class="term">directio</span></dt><dd><p>
         Read a device sector using asynchronous I/O (aio).
        </p></dd><dt id="id-1.11.6.6.12.4.3.2.4.3"><span class="term">rdac</span></dt><dd><p>
         Device-specific checker for NetAPP E-Series and similar arrays.
        </p></dd><dt id="id-1.11.6.6.12.4.3.2.4.4"><span class="term">none</span></dt><dd><p>
         No path checking is performed.
        </p></dd></dl></div></dd><dt id="id-1.11.6.6.12.4.4"><span class="term">checker_timeout</span></dt><dd><p>
      If a device does not respond to a path checker command in the given time,
      it is considered failed. The default is the kernel's SCSI command timeout
      for the device (usually 30 seconds).
     </p></dd><dt id="id-1.11.6.6.12.4.5"><span class="term">fast_io_fail_tmo</span></dt><dd><p>
      If an error on the SCSI transport layer is detected (for example on a
      Fibre Channel remote port), the kernel transport layer waits for this
      amount of time (in seconds) for the transport to recover. After that, the
      path device fails with “transport offline” state. This is
      very useful for multipath, because it allows a quick path failover for a
      frequently occurring class of errors. The value must match typical time
      scale for reconfiguration in the fabric. The default value of 5 seconds
      works well for Fibre Channel. Other transports, like iSCSI, may require
      longer timeouts.
     </p></dd><dt id="id-1.11.6.6.12.4.6"><span class="term">dev_loss_tmo</span></dt><dd><p>
      If a SCSI transport endpoint (for example a Fibre Channel remote port) is
      not reachable any more, the kernel waits for this amount of time (in
      seconds) for the port to reappear until it removes the SCSI device node
      for good. Device node removal is a complex operation which is prone to
      race conditions or deadlocks and should best be avoided. We therefore
      recommend setting this to a high value. The special value
      <code class="literal">infinity</code> is supported. The default is 10 minutes. To
      avoid deadlock situations, <code class="command">multipathd</code> ensures that I/O
      queuing (see <code class="literal">no_path_retry</code>) is stopped before
      <code class="literal">dev_loss_tmo</code> expires.
     </p></dd><dt id="id-1.11.6.6.12.4.7"><span class="term">no_path_retry</span></dt><dd><p>
      Determine what happens if all paths of a given multipath map have failed.
      The possible values are:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.12.4.7.2.2.1"><span class="term">fail</span></dt><dd><p>
         Fail I/O on the multipath map. This will cause I/O errors in upper
         layers such as mounted file systems. The affected file systems, and
         possibly the entire host, will enter degraded mode.
        </p></dd><dt id="id-1.11.6.6.12.4.7.2.2.2"><span class="term">queue</span></dt><dd><p>
         I/O on the multipath map is queued in the device mapper layer and sent
         to the device when path devices become available again. This is the
         safest option to avoid losing data, but it can have negative effects
         if the path devices do not get reinstated for a long time. Processes
         reading from the device will hang in uninterruptible sleep
         (<code class="literal">D</code>) state. Queued data occupies memory, which
         becomes unavailable for processes. Eventually, memory will be
         exhausted.
        </p></dd><dt id="id-1.11.6.6.12.4.7.2.2.3"><span class="term"><em class="replaceable">N</em></span></dt><dd><p>
         <em class="replaceable">N</em> is a positive integer. Keep the map
         device in queuing mode for <em class="replaceable">N</em> polling
         intervals. When the time elapses, <code class="command">multipathd</code> fails
         the map device. If <code class="literal">polling_interval</code> is 5 seconds
         and <code class="literal">no_path_retry</code> is 6,
         <code class="command">multipathd</code> will queue I/O for approximately 6 * 5s
         = 30s before failing I/O on the map device.
        </p></dd></dl></div></dd><dt id="id-1.11.6.6.12.4.8"><span class="term">flush_on_last_del</span></dt><dd><p>
      If set to <code class="literal">yes</code> and all path devices of a map are
      deleted (as opposed to just failed), fail all I/O in the map before
      removing it. The default is <code class="literal">no</code>.
     </p></dd><dt id="id-1.11.6.6.12.4.9"><span class="term">deferred_remove</span></dt><dd><p>
      If set to <code class="literal">yes</code> and all path devices of a map are
      deleted, wait for holders to close the file descriptors for the map
      device before flushing and removing it. If paths reappear before the last
      holder closed the map, the deferred remove operation will be cancelled.
      The default is <code class="literal">no</code>.
     </p></dd><dt id="vle-failback"><span class="term">failback</span></dt><dd><p>
      If a failed path device in an inactive path group recovers,
      <code class="command">multipathd</code> reevaluates the path group priorities of
      all path groups (see <a class="xref" href="#sec-multipath-grouping" title="18.10. Configuring path grouping and priorities">Section 18.10, “Configuring path grouping and priorities”</a>). After the
      reevaluation, the highest-priority path group may be one of the currently
      inactive path groups. This parameter determines what happens in this
      situation.
     </p><div id="id-1.11.6.6.12.4.10.2.2" data-id-title="Observe vendor recommendations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Observe vendor recommendations</div><p>
       The optimal failback policy depends on the property of the storage
       device. It is therefore strongly encouraged to verify
       <code class="option">failback</code> settings with the storage vendor.
      </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.12.4.10.2.3.1"><span class="term">manual</span></dt><dd><p>
         Nothing happens unless the administrator runs a <code class="command">multipathd
         switchgroup</code> (see
         <a class="xref" href="#sec-multipath-mpiotools-multipathd" title="18.6.2. The multipathd daemon">Section 18.6.2, “The <code class="command">multipathd</code> daemon”</a>).
        </p></dd><dt id="id-1.11.6.6.12.4.10.2.3.2"><span class="term">immediate</span></dt><dd><p>
         The highest-priority path group is activated immediately. This is
         often beneficial for performance, especially on stand-alone servers,
         but it should not be used for arrays on which the change of the path
         group is a costly operation.
        </p></dd><dt id="id-1.11.6.6.12.4.10.2.3.3"><span class="term">followover</span></dt><dd><p>
         Like <code class="literal">immediate</code>, but only perform failback when the
         path that has just become active is the only healthy path in its path
         group. This is useful for cluster configurations: It keeps a node from
         automatically failing back when another node requested a failover
         before.
        </p></dd><dt id="id-1.11.6.6.12.4.10.2.3.4"><span class="term"><em class="replaceable">N</em></span></dt><dd><p>
         <em class="replaceable">N</em> is a positive integer. Wait for
         <em class="replaceable">N</em> polling intervals before activating the
         highest priority path group. If the priorities change again during
         this time, the wait period starts anew.
        </p></dd></dl></div></dd><dt id="id-1.11.6.6.12.4.11"><span class="term">eh_deadline</span></dt><dd><p>
      Set an approximate upper limit for the time (in seconds) spent in SCSI
      error handling if devices are unresponsive and SCSI commands time out
      without error response. When the deadline has elapsed, the kernel will
      perform a full HBA reset.
     </p></dd></dl></div><p>
   After modifying the <code class="filename">/etc/multipath.conf</code> file, apply
   your settings as described in
   <a class="xref" href="#sec-multipath-conf-file-apply" title="18.8.4. Applying multipath.conf modifications">Section 18.8.4, “Applying <code class="filename">multipath.conf</code> modifications”</a>.
  </p><section class="sect2" id="sec-multipath-policies-standalone" data-id-title="Queuing policy on stand-alone servers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.9.1 </span><span class="title-name">Queuing policy on stand-alone servers</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-policies-standalone">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When you configure multipath I/O for a stand-alone server, a
    <code class="literal">no_path_retry</code> setting with value
    <code class="literal">queue</code> protects the server operating system from
    receiving I/O errors as long as possible. It queues messages until a
    multipath failover occurs. If “infinite” queuing is not desired
    (see above), select a numeric value that is deemed high enough for the
    storage paths to recover under ordinary circumstances (see above).
   </p></section><section class="sect2" id="sec-multipath-policies-cluster" data-id-title="Queuing policy on clustered servers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.9.2 </span><span class="title-name">Queuing policy on clustered servers</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-policies-cluster">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When you configure multipath I/O for a node in a high-availability cluster,
    you want multipath to report the I/O failure to trigger the resource
    failover instead of waiting for a multipath failover to be resolved. In
    cluster environments, you must modify the <code class="literal">no_path_retry
    </code>setting so that the cluster node receives an I/O error in
    relation to the cluster verification process (recommended to be 50% of the
    heartbeat tolerance) if the connection is lost to the storage system. In
    addition, you want the multipath <code class="literal">failback</code> to be set to
    <code class="literal">manual</code> or <code class="literal">followover</code> to avoid a
    ping-pong of resources because of path failures.
   </p></section></section><section class="sect1" id="sec-multipath-grouping" data-id-title="Configuring path grouping and priorities"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.10 </span><span class="title-name">Configuring path grouping and priorities</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-grouping">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Path devices in multipath maps are grouped in <span class="emphasis"><em>path
   groups</em></span>, also called <span class="emphasis"><em>priority groups</em></span>. Only
   one path group receives I/O at any given time. <code class="command">multipathd</code>
   assigns <span class="emphasis"><em>priorities</em></span> to path groups. Out of the path
   groups with active paths, the group with the highest priority is activated
   according to the configured failback policy for the map (see
   <a class="xref" href="#sec-multipath-policies-default" title="18.9. Configuring policies for failover, queuing, and failback">Section 18.9, “Configuring policies for failover, queuing, and failback”</a>).
   The priority of a path group is the average of the priorities of the active
   path devices in the path group. The priority of a path device is an integer
   value calculated from the device properties (see the description of the
   <code class="literal">prio</code> option below).
  </p><p>
   This section describes the <code class="filename">multipath.conf</code> configuration
   parameters relevant for priority determination and path grouping.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.13.4.1"><span class="term">path_grouping_policy</span></dt><dd><p>
      Specifies the method used to combine paths into groups. Only the most
      important policies are listed here; see
      <code class="systemitem">multipath.conf(5)</code> for other less frequently used
      values.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.13.4.1.2.2.1"><span class="term">failover</span></dt><dd><p>
         One path per path group. This setting is useful for traditional
         “active/passive” storage arrays.
        </p></dd><dt id="id-1.11.6.6.13.4.1.2.2.2"><span class="term">multibus</span></dt><dd><p>
         All paths in one path group. This is useful for traditional
         “active/active” arrays.
        </p></dd><dt id="id-1.11.6.6.13.4.1.2.2.3"><span class="term">group_by_prio</span></dt><dd><p>
         Path devices with the same path priority are grouped together. This
         option is useful for modern arrays that support asymmetric access
         states, like ALUA. Combined with the <code class="literal">alua</code> or
         <code class="literal">sysfs</code> priority algorithms, the priority groups set
         up by <code class="command">multipathd</code> will match the primary target port
         groups that the storage array reports through ALUA-related SCSI
         commands.
        </p></dd></dl></div><p>
      Using the same policy names, the path grouping policy for a multipath map
      can be changed temporarily with the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipath -p <em class="replaceable">POLICY_NAME</em> <em class="replaceable">MAP_NAME</em></pre></div></dd><dt id="id-1.11.6.6.13.4.2"><span class="term">marginal_pathgroups</span></dt><dd><p>
      If set to <code class="literal">on</code> or <code class="literal">fpin</code>,
      “marginal” path devices are sorted into a separate path
      group. This is independent of the path grouping algorithm in use. See
      <a class="xref" href="#sec-multipath-marginal" title="18.13.1. Handling unreliable (“marginal”) path devices">Section 18.13.1, “Handling unreliable (“marginal”) path devices”</a>.
     </p></dd><dt id="id-1.11.6.6.13.4.3"><span class="term">detect_prio</span></dt><dd><p>
      If this is set to <code class="literal">yes</code> (default, recommended),
      <code class="command">multipathd</code> automatically detects the best algorithm to
      set the priority for a storage device and ignores the
      <code class="literal">prio</code> setting. In practice, this means using the
      <code class="literal">sysfs</code> prio algorithm if ALUA support is detected.
     </p></dd><dt id="id-1.11.6.6.13.4.4"><span class="term">prio</span></dt><dd><p>
      Determines the method to derive priorities for path devices. If you
      override this, disable <code class="literal">detect_prio</code> as follows:
     </p><div class="verbatim-wrap"><pre class="screen">defaults {
          detect_prio no
}</pre></div><p>
      The following list contains only the most important methods. Several
      other methods are available, mainly to support legacy hardware. See
      <code class="systemitem">multipath.conf(5)</code> for the full list.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.13.4.4.2.4.1"><span class="term">alua</span></dt><dd><p>
         Uses SCSI-3 ALUA access states to derive path priority values. The
         optional <code class="literal">exclusive_pref_bit</code> argument can be used to
         change the behavior for devices that have the ALUA “preferred
         primary target port group” (PREF) bit set:
        </p><div class="verbatim-wrap"><pre class="screen">        prio alua
        prio_args exclusive_pref_bit</pre></div><p>
         If this option is set, “preferred” paths get a priority
         bonus over other active/optimized paths. Otherwise, all
         active/optimized paths are assigned the same priority.
        </p></dd><dt id="id-1.11.6.6.13.4.4.2.4.2"><span class="term">sysfs</span></dt><dd><p>
         Like <code class="literal">alua</code>, but instead of sending SCSI commands to
         the device, it obtains the access states from
         <code class="systemitem">sysfs</code>. This causes less I/O load than
         <code class="literal">alua</code>, but is not suitable for all storage arrays
         with ALUA support.
        </p></dd><dt id="id-1.11.6.6.13.4.4.2.4.3"><span class="term">const</span></dt><dd><p>
         Uses a constant value for all paths.
        </p></dd><dt id="id-1.11.6.6.13.4.4.2.4.4"><span class="term">path_latency</span></dt><dd><p>
         Measures I/O latency (time from I/O submission to completion) on path
         devices, and assigns higher priority to devices with lower latency.
         See <code class="systemitem">multipath.conf(5)</code> for details. This
         algorithm is still experimental.
        </p></dd><dt id="id-1.11.6.6.13.4.4.2.4.5"><span class="term">weightedpath</span></dt><dd><p>
         Assigns a priority to paths based on their name, serial number,
         Host:Bus:Target:Lun ID (HBTL), or Fibre Channel WWN. The priority
         value does not change over time. The method requires a
         <code class="literal">prio_args</code> argument, see
         <code class="systemitem">multipath.conf(5)</code> for details. For example:
        </p><div class="verbatim-wrap"><pre class="screen">        prio weightedpath
        prio_args "hbtl 2:.*:.*:.* 10 hbtl 3:.*:.*:.* 20 hbtl .* 1"</pre></div><p>
         This assigns devices on SCSI host 3 a higher priority than devices on
         SCSI host 2, and all others a lower priority.
        </p></dd></dl></div></dd><dt id="id-1.11.6.6.13.4.5"><span class="term">prio_args</span></dt><dd><p>
      Some <code class="literal">prio</code> algorithms require extra arguments. These
      are specified in this option, with syntax depending on the algorithm. See
      above.
     </p></dd><dt id="id-1.11.6.6.13.4.6"><span class="term">hardware_handler</span></dt><dd><p>
      The name of a kernel module that the kernel uses to activate path devices
      when switching path groups. This option has no effect with recent kernels
      because hardware handlers are autodetected. See
      <a class="xref" href="#sec-multipath-hardware-handlers" title="18.2.3. Storage arrays that require specific hardware handlers">Section 18.2.3, “Storage arrays that require specific hardware handlers”</a>.
     </p></dd><dt id="id-1.11.6.6.13.4.7"><span class="term">path_selector</span></dt><dd><p>
      The name of a kernel module that is used for load balancing between the
      paths of the active path group. The available choices depend on the
      kernel configuration. For historical reasons, the name must always be
      enclosed in quotes and followed by a “0” in
      <code class="filename">multipath.conf</code>, like this:
     </p><div class="verbatim-wrap"><pre class="screen">    path_selector "queue-length 0"</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.13.4.7.2.3.1"><span class="term">service-time</span></dt><dd><p>
         Estimates the time pending I/O will need to complete on all paths, and
         selects the path with the lowest value. This is the default.
        </p></dd><dt id="id-1.11.6.6.13.4.7.2.3.2"><span class="term">historical-service-time</span></dt><dd><p>
         Estimates future service time based on the historical service time
         (about which it keeps a moving average) and the number of outstanding
         requests. Estimates the time pending I/O will need to complete on all
         paths, and selects the path with the lowest value.
        </p></dd><dt id="id-1.11.6.6.13.4.7.2.3.3"><span class="term">queue-length</span></dt><dd><p>
         Selects the path with the lowest number of currently pending I/O
         requests.
        </p></dd><dt id="id-1.11.6.6.13.4.7.2.3.4"><span class="term">round-robin</span></dt><dd><p>
         Switches paths in round-robin fashion. The number of requests
         submitted to a path before switching to the next one can be adjusted
         with the options <code class="literal">rr_min_io_rq</code> and
         <code class="literal">rr_weight</code>.
        </p></dd><dt id="id-1.11.6.6.13.4.7.2.3.5"><span class="term">io-affinity</span></dt><dd><p>
         This path selector currently does not work with
         <code class="systemitem">multipath-tools</code>.
        </p></dd></dl></div></dd></dl></div><p>
   After modifying the <code class="filename">/etc/multipath.conf</code> file, apply
   your settings as described in
   <a class="xref" href="#sec-multipath-conf-file-apply" title="18.8.4. Applying multipath.conf modifications">Section 18.8.4, “Applying <code class="filename">multipath.conf</code> modifications”</a>.
  </p></section><section class="sect1" id="sec-multipath-select-devices" data-id-title="Selecting devices for multipathing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.11 </span><span class="title-name">Selecting devices for multipathing</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-select-devices">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   On systems with multipath devices, you might want to avoid setting up
   multipath maps on some devices (typically local disks).
   <code class="systemitem">multipath-tools</code> offers various means for
   configuring which devices should be considered multipath path devices.
  </p><div id="id-1.11.6.6.14.3" data-id-title="multipath on local disks" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: multipath on local disks</div><p>
    In general, there is nothing wrong with setting up “degraded”
    multipath maps with just a single device on top of local disks. It works
    fine and requires no extra configuration. However, some administrators find
    this confusing or generally oppose this sort of unnecessary multipathing.
    Also, the multipath layer causes a slight performance overhead. See also
    <a class="xref" href="#sec-multipath-planning-type-noroot" title="18.3.2.2. Root file system on a local disk">Section 18.3.2.2, “Root file system on a local disk”</a>.
   </p></div><p>
   After modifying the <code class="filename">/etc/multipath.conf</code> file, apply
   your settings as described in
   <a class="xref" href="#sec-multipath-conf-file-apply" title="18.8.4. Applying multipath.conf modifications">Section 18.8.4, “Applying <code class="filename">multipath.conf</code> modifications”</a>.
  </p><section class="sect2" id="sec-multipath-blacklist" data-id-title="The blacklist section in multipath.conf"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.11.1 </span><span class="title-name">The <code class="literal">blacklist</code> section in <code class="filename">multipath.conf</code></span></span> <a title="Permalink" class="permalink" href="#sec-multipath-blacklist">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="filename">/etc/multipath.conf</code> file can contain a
    <code class="literal">blacklist</code> section that lists all devices that should be
    ignored by <code class="command">multipathd</code> and <code class="command">multipath</code>.
    The following example illustrates possible ways of excluding devices:
   </p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">blacklist {
    wwid 3600605b009e7ed501f0e45370aaeb77f <span class="callout" id="mp-co-bl-wwid">1</span>
    device {  <span class="callout" id="mp-co-bl-device">2</span>
        vendor ATA
        product .*
    }
    protocol scsi:sas <span class="callout" id="mp-co-bl-prot">3</span>
    property SCSI_IDENT_LUN_T10 <span class="callout" id="mp-co-bl-prop">4</span>
    devnode "!^dasd[a-z]*" <span class="callout" id="mp-co-bl-devnode">5</span>
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-bl-wwid"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       <code class="literal">wwid</code> entries are ideal for excluding specific
       devices, for example, the root disk.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-bl-device"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This <code class="literal">device</code> section excludes all ATA devices (the
       regular expression for <code class="literal">product</code> matches anything).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-bl-prot"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Excluding by <code class="literal">protocol</code> allows excluding devices using
       certain bus types, here SAS. Other common protocol values are
       <code class="literal">scsi:fcp</code>, <code class="literal">scsi:iscsi</code>, and
       <code class="literal">ccw</code>. See <code class="systemitem">multipath.conf(5)</code>
       for more. To see the protocols that paths in your systems are using,
       run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd show paths format "%d %P"</pre></div><p>
       This form is supported since <span class="productname"><span class="phrase">SLES</span></span> 15 SP1 and
       <span class="productname"><span class="phrase">SLES</span></span> 12 SP5.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-bl-prop"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This <code class="literal">property</code> entry excludes devices that have a
       certain udev property (no matter what the value of the property is).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-bl-devnode"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Excluding devices by <code class="literal">devnode</code> is only recommended for
       classes of devices using regular expressions, like in the example, which
       excludes everything but DASD devices. Using this for individual devices
       like <code class="filename">sda</code> is discouraged because device node names
       are not persistent.
      </p><p>
       The example illustrates special syntax that is only supported in the
       <code class="literal">blacklist</code> and <code class="literal">blacklist_exceptions</code>
       sections: Prefixing the regular expression with an exclamation mark
       (<code class="literal">!</code>) negates the match. Note that the exclamation mark
       must appear within double quotes.
      </p></td></tr></table></div></div><p>
    By default, <code class="systemitem">multipath-tools</code> ignores all devices
    except SCSI, DASD or NVMe. Technically, the built-in devnode exclude list
    is this negated regular expression:
   </p><div class="verbatim-wrap"><pre class="screen">    devnode !^(sd[a-z]|dasd[a-z]|nvme[0-9])</pre></div></section><section class="sect2" id="sec-multipath-blacklist-exceptions" data-id-title="The blacklist exceptions section in multipath.conf"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.11.2 </span><span class="title-name">The <code class="literal">blacklist exceptions</code> section in <code class="filename">multipath.conf</code></span></span> <a title="Permalink" class="permalink" href="#sec-multipath-blacklist-exceptions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Sometimes, it is desired to configure only very specific devices for
    multipathing. In this case, devices are excluded by default, and exceptions
    are defined for devices that should be part of a multipath map. The
    <code class="literal">blacklist_exceptions</code> section exists for this purpose. It
    is typically used like in the following example, which excludes everything
    except storage with product string “NETAPP”:
   </p><div class="verbatim-wrap"><pre class="screen">blacklist {
     wwid .*
}
blacklist_exceptions {
     device {
         vendor ^NETAPP$
         product .*
     }
}</pre></div><p>
    The <code class="literal">blacklist_exceptions</code> section supports all methods
    described for the <code class="literal">blacklist</code> section above.
   </p><p>
    The <code class="literal">property</code> directive in
    <code class="literal">blacklist_exceptions</code> is mandatory because every device
    <span class="emphasis"><em>must</em></span> have at least one of the “allowed”
    udev properties to be considered a path device for multipath (the value of
    the property does not matter). The built-in default for
    <code class="literal">property</code> is
   </p><div class="verbatim-wrap"><pre class="screen">    property (SCSI_IDENT_|ID_WWN)</pre></div><p>
    Only devices that have at least one udev property matching this regular
    expression will be included.
   </p></section><section class="sect2" id="sec-multipath-device-select-other" data-id-title="Other options affecting device selection"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.11.3 </span><span class="title-name">Other options affecting device selection</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-device-select-other">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Besides the <code class="literal">blacklist</code> options, there are several other
    settings in <code class="filename">/etc/multipath.conf</code> that affect which
    devices are considered multipath path devices.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.14.7.3.1"><span class="term">find_multipaths</span></dt><dd><p>
       This option controls the behavior of <code class="command">multipath</code> and
       <code class="command">multipathd</code> when a device that is not excluded is
       first encountered. The possible values are:
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.14.7.3.1.2.2.1"><span class="term">greedy</span></dt><dd><p>
          Every device non-excluded by <code class="literal">blacklist</code> in
          <code class="filename">/etc/multipath.conf</code> is included. This is the
          default on SUSE Linux Enterprise. If this setting is active, the only way to prevent
          devices from being added to multipath maps is setting them as
          excluded.
         </p></dd><dt id="id-1.11.6.6.14.7.3.1.2.2.2"><span class="term">strict</span></dt><dd><p>
          Every device is excluded, even if it is not present in the
          <code class="literal">blacklist</code> section of
          <code class="filename">/etc/multipath.conf</code>, unless its WWID is listed
          in the file <code class="filename">/etc/multipath/wwids</code>. It requires
          manual maintenance of the WWIDs file (see note below).
         </p></dd><dt id="id-1.11.6.6.14.7.3.1.2.2.3"><span class="term">yes</span></dt><dd><p>
          Devices are included if they meet the conditions for
          <code class="literal">strict</code>, or if at least one other device with the
          same WWID exists in the system.
         </p></dd><dt id="id-1.11.6.6.14.7.3.1.2.2.4"><span class="term">smart</span></dt><dd><p>
          If a new WWID is first encountered, it is temporarily marked as
          multipath path device. <code class="command">multipathd</code> waits for some
          time for additional paths with the same WWID to appear. If this
          happens, the multipath map is set up as usual. Otherwise, when the
          timeout expires, the single device is released to the system as
          a non-multipath device. The timeout is configurable with the option
          <code class="option">find_multipaths_timeout</code>.
         </p><p>
          This option depends on <code class="command">systemd</code> features which are
          only available on SUSE Linux Enterprise Server 15.
         </p></dd></dl></div><div id="id-1.11.6.6.14.7.3.1.2.3" data-id-title="Maintaining /etc/multipath/wwids" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Maintaining <code class="filename">/etc/multipath/wwids</code></div><p>
        <code class="systemitem">multipath-tools</code> keeps a record of previously
        setup multipath maps in the file
        <code class="filename">/etc/multipath/wwids</code> (the “WWIDs
        file”). Devices with WWIDs listed in this file are considered
        multipath path devices. The file is important for multipath device
        selection for all values of <code class="option">find_multipaths</code> except
        <code class="literal">greedy</code>.
       </p><p>
        If <code class="option">find_multipaths</code> is set to <code class="literal">yes</code> or
        <code class="literal">smart</code>, <code class="command">multipathd</code> adds WWIDs to
        <code class="filename">/etc/multipath/wwids</code> after setting up new maps, so
        that these maps will be detected more quickly in the future.
       </p><p>
        The WWIDs file can be manually modified:
       </p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipath -a 3600a098000aad1e3000064e45f2c2355 <span class="callout" id="mp-co-wwid-a">1</span>
<code class="prompt user">&gt; </code><code class="command">sudo</code> multipath -w /dev/sdf <span class="callout" id="mp-co-wwid-w">2</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-wwid-a"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
           This command adds the given WWID to
           <code class="filename">/etc/multipath/wwids</code>.
          </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-wwid-w"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
           This command removes the WWID of the given device.
          </p></td></tr></table></div></div><p>
        In the <code class="literal">strict</code> mode, this is the only way to add new
        multipath devices. After modifying the WWIDs file, run
        <code class="command">multipathd reconfigure</code> to apply the changes. We
        recommend rebuilding the initramfs after applying changes to the WWIDs
        file (see <a class="xref" href="#sec-multipath-initrd-sync" title="18.7.4. Keeping the initramfs synchronized">Section 18.7.4, “Keeping the initramfs synchronized”</a>).
       </p></div></dd><dt id="id-1.11.6.6.14.7.3.2"><span class="term">allow_usb_devices</span></dt><dd><p>
       If this option is set to <code class="literal">yes</code>, USB storage devices are
       considered for multipathing. The default is <code class="literal">no</code>.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-multipath-names" data-id-title="Multipath device names and WWIDs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.12 </span><span class="title-name">Multipath device names and WWIDs</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-names">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="command">multipathd</code> and <code class="command">multipath</code> internally
   use WWIDs to identify devices. WWIDs are also used as map names by default.
   For convenience, <code class="systemitem">multipath-tools</code> supports assigning
   simpler, more easily memorizable names to multipath devices.
  </p><section class="sect2" id="sec-multipath-conf-file-wwid" data-id-title="WWIDs and device Identification"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.12.1 </span><span class="title-name">WWIDs and device Identification</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-file-wwid">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    It is crucial for multipath operation to reliably detect devices that
    represent paths to the same storage volume.
    <code class="systemitem">multipath-tools</code> uses the device's World Wide
    Identification (WWID) for this purpose (sometimes also referred to as
    Universally Unique ID (UUID) or Unique ID (UID—do not confuse with
    “User ID”)). The WWID of a map device is always the same as the
    WWID of its path devices.
   </p><p>
    By default, WWIDs of path devices are inferred from udev properties of the
    devices, which are determined in udev rules, either by reading device
    attributes from the sysfs file system or by using specific I/O commands. To
    see the udev properties of a device, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>udevadm info /dev/<em class="replaceable">sdx</em></pre></div><p>
    The udev properties used by <code class="systemitem">multipath-tools</code> to
    derive WWIDs are:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">ID_SERIAL</code> for SCSI devices (do not confuse this with
      the device's “serial number”)
     </p></li><li class="listitem"><p>
      <code class="literal">ID_UID</code> for DASD devices
     </p></li><li class="listitem"><p>
      <code class="literal">ID_WWN</code> for NVMe devices
     </p></li></ul></div><div id="id-1.11.6.6.15.3.7" data-id-title="Avoid changing WWIDs" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Avoid changing WWIDs</div><p>
     It is impossible to change the WWID of a multipath map which is in use. If
     the WWID of mapped path devices changes because of a configuration change,
     the map needs to be destroyed, and a new map needs to be set up with the
     new WWID. This cannot be done while the old map is in use. In extreme
     cases, data corruption may result from WWID changes. It must therefore be
     <span class="emphasis"><em>strictly avoided</em></span> to apply configuration changes that
     would cause map WWIDs to change.
    </p><p>
     It is allowed to enable the <code class="literal">uid_attrs</code> option in
     <code class="filename">/etc/multipath.conf</code>, see
     <a class="xref" href="#sec-multipath-conf-misc" title="18.13. Miscellaneous options">Section 18.13, “Miscellaneous options”</a>.
    </p></div></section><section class="sect2" id="sec-multipath-alias" data-id-title="Setting aliases for multipath maps"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.12.2 </span><span class="title-name">Setting aliases for multipath maps</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-alias">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Arbitrary map names can be set in the <code class="literal">multipaths</code> section
    of <code class="filename">/etc/multipath.conf</code> as follows:
   </p><div class="verbatim-wrap"><pre class="screen">multipaths {
    multipath {
        wwid 3600a098000aad1e3000064e45f2c2355
        alias postgres
    }
}</pre></div><p>
    Aliases are expressive, but they need to be assigned to each map
    individually, which may be cumbersome on large systems.
   </p></section><section class="sect2" id="sec-multipath-user-friendly-names" data-id-title="Using autogenerated user-friendly names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.12.3 </span><span class="title-name">Using autogenerated user-friendly names</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-user-friendly-names">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <code class="systemitem">multipath-tools</code> also supports autogenerated
    aliases, so-called “user-friendly names”. The naming scheme of
    the aliases follows the pattern: mpath<em class="replaceable">INDEX</em>,
    where <em class="replaceable">INDEX</em> is a lower case letter (starting
    with <code class="literal">a</code>). So the first autogenerated alias is
    <code class="literal">mpatha</code>, the next one is <code class="literal">mpathb</code>,
    <code class="literal">mpathc</code> to <code class="literal">mpathz</code>. After that follows
    <code class="literal">mpathaa</code>, <code class="literal">mpathab</code>, and so on.
   </p><p>
    Map names are only useful if they are persistent.
    <code class="systemitem">multipath-tools</code> keeps track of the assigned names
    in the file <code class="filename">/etc/multipath/bindings</code> (the
    “bindings file”). When a new map is created, the WWID is first
    looked up in this file. If it is not found, the lowest available
    user-friendly name is assigned to it.
   </p><p>
    Explicit aliases as discussed in <a class="xref" href="#sec-multipath-alias" title="18.12.2. Setting aliases for multipath maps">Section 18.12.2, “Setting aliases for multipath maps”</a> take
    precedence over user-friendly names.
   </p><p>
    The following options in <code class="filename">/etc/multipath.conf</code> affect
    user-friendly names:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.15.5.6.1"><span class="term">user_friendly_names</span></dt><dd><p>
       If set to <code class="literal">yes</code>, user-friendly names are assigned and
       used. Otherwise, the WWID is used as a map name unless an alias is
       configured.
      </p></dd><dt id="id-1.11.6.6.15.5.6.2"><span class="term">alias_prefix</span></dt><dd><p>
       The prefix used to create user-friendly names, <code class="literal">mpath</code>
       by default.
      </p></dd></dl></div><div id="id-1.11.6.6.15.5.7" data-id-title="Map names in high-availability clusters" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Map names in high-availability clusters</div><p>
     For cluster operations, device names must be identical across all nodes in
     the cluster. The <code class="systemitem">multipath-tools</code> configuration
     must be kept synchronized between nodes. If
     <code class="literal">user_friendly_names</code> is used,
     <code class="command">multipathd</code> may modify the
     <code class="filename">/etc/multipath/bindings</code> file at runtime. Such
     modifications must be replicated dynamically to all nodes. The same
     applies to <code class="filename">/etc/multipath/wwids</code> (see
     <a class="xref" href="#sec-multipath-device-select-other" title="18.11.3. Other options affecting device selection">Section 18.11.3, “Other options affecting device selection”</a>).
    </p></div><div id="id-1.11.6.6.15.5.8" data-id-title="Changing map names at runtime" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Changing map names at runtime</div><p>
     It is possible to change map names at runtime. Use any of the methods
     described in this section and run <code class="command">multipathd
     reconfigure</code>, and the map names will change without disrupting
     the system operation.
    </p></div></section><section class="sect2" id="sec-multipath-referring" data-id-title="Referring to multipath maps"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.12.4 </span><span class="title-name">Referring to multipath maps</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-referring">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Technically, multipath maps are Device Mapper devices, which have generic
    names of the form <code class="filename">/dev/dm-<em class="replaceable">n</em></code>
    with an integer number <em class="replaceable">n</em>. These names are not
    persistent. They should <span class="emphasis"><em>never</em></span> be used to refer to the
    multipath maps. <code class="command">udev</code> creates various symbolic links to
    these devices, which are more suitable as persistent references. These
    links differ with respect to their invariance against certain configuration
    changes. The following typical example shows various symlinks all pointing
    to the same device.
   </p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">/dev/disk/by-id/dm-name-mpathb<span class="callout" id="mp-ref-dm-name">1</span> -&gt; ../../dm-1
/dev/disk/by-id/dm-uuid-mpath-3600a098000aad73f00000a3f5a275dc8<span class="callout" id="mp-ref-dm-uuid">2</span> -&gt; ../../dm-1
/dev/disk/by-id/scsi-3600a098000aad73f00000a3f5a275dc8<span class="callout" id="mp-ref-scsi">3</span> -&gt; ../../dm-1
/dev/disk/by-id/wwn-0x600a098000aad73f00000a3f5a275dc8<span class="callout" id="mp-ref-wwn">4</span> -&gt; ../../dm-1
/dev/mapper/mpathb<span class="callout" id="mp-ref-mapper">5</span> -&gt; ../dm-1</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-ref-dm-name"><span class="callout">1</span></a> <a href="#mp-ref-mapper"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       These two links use the map name to refer to the map. Thus, the links
       will change if the map name changes, for example, if you enable or
       disable user-friendly names.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-ref-dm-uuid"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This link uses the device mapper UUID, which is the WWID used by
       <code class="systemitem">multipath-tools</code> prefixed by the string
       <code class="literal">dm-uuid-mpath-</code>. It is independent of the map name.
      </p><p>
       The device mapper UUID is the preferred form to ensure that
       <span class="emphasis"><em>only multipath devices</em></span> are referenced. For example,
       the following line in <code class="filename">/etc/lvm/lvm.conf</code> rejects all
       devices except multipath maps:
      </p><div class="verbatim-wrap"><pre class="screen">filter = [ "a|/dev/disk/by-id/dm-uuid-mpath-.*|", "r|.*|" ]</pre></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-ref-scsi"><span class="callout">3</span></a> <a href="#mp-ref-wwn"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       These are links that would normally point to path devices. The multipath
       device took them over, because it has a higher udev link priority (see
       <code class="systemitem">udev(7)</code>). When the map is destroyed or
       multipathing is turned off, they will still exist and point to one of
       the path devices instead. This provides a means to reference a device by
       its WWID, whether or not multipathing is active.
      </p></td></tr></table></div></div><p>
    For <span class="bold"><strong>partitions</strong></span> on multipath maps created
    by the <code class="command">kpartx</code> tool, there are similar symbolic links,
    derived from the parent device name or WWID and the partition number:
   </p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">/dev/disk/by-id/dm-name-mpatha-part2 -&gt; ../../dm-5
/dev/disk/by-id/dm-uuid-part2-mpath-3600a098000aad1e300000b4b5a275d45 -&gt; ../../dm-5
/dev/disk/by-id/scsi-3600a098000aad1e300000b4b5a275d45-part2 -&gt; ../../dm-5
/dev/disk/by-id/wwn-0x600a098000aad1e300000b4b5a275d45-part2 -&gt; ../../dm-5
/dev/disk/by-partuuid/1c2f70e0-fb91-49f5-8260-38eacaf7992b -&gt; ../../dm-5
/dev/disk/by-uuid/f67c49e9-3cf2-4bb7-8991-63568cb840a4 -&gt; ../../dm-5
/dev/mapper/mpatha-part2 -&gt; ../dm-5</pre></div></div><p>
    Note that partitions often have <code class="filename">by-uuid</code> links, too,
    referring not to the device itself but to the file system it contains.
    These links are often preferable. They are invariant even if the file
    system is copied to a different device or partition.
   </p><div id="id-1.11.6.6.15.6.7" data-id-title="Map names in the initramfs" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Map names in the initramfs</div><p>
     When <code class="command">dracut</code> builds an initramfs, it creates hard-coded
     references to devices in the initramfs, using
     <code class="filename">/dev/mapper/$MAP_NAME</code> references by default. These
     hard-coded references will not be found during boot if the map names used
     in the initramfs do not match the names used during building the
     initramfs, causing boot failure. Normally this will not happen, because
     <code class="command">dracut</code> will add all multipath configuration files to
     the initramfs. But problems can occur if the initramfs is built from a
     different environment, for example, in the rescue system or during an
     offline update. To prevent this boot failure, change
     <code class="command">dracut</code>'s <code class="literal">persistent_policy</code> setting,
     as explained in <a class="xref" href="#sec-multipath-initrd-persistent" title="18.7.4.2. Persistent device names in the initramfs">Section 18.7.4.2, “Persistent device names in the initramfs”</a>.
    </p></div></section></section><section class="sect1" id="sec-multipath-conf-misc" data-id-title="Miscellaneous options"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.13 </span><span class="title-name">Miscellaneous options</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-conf-misc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section lists some useful <code class="filename">multipath.conf</code> options
   that were not mentioned so far. See
   <code class="systemitem">multipath.conf(5)</code> for a full list.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.16.3.1"><span class="term">verbosity</span></dt><dd><p>
      Controls the log verbosity of both <code class="command">multipath</code> and
      <code class="command">multipathd</code>. The command-line option
      <code class="option">-v</code> overrides this setting for both commands. The value
      can be between 0 (only fatal errors) and 4 (verbose logging). The default
      is 2.
     </p></dd><dt id="id-1.11.6.6.16.3.2"><span class="term">uid_attrs</span></dt><dd><p>
      This option enables an optimization for processing udev events, so-called
      “uevent merging”. It is useful in environments in which
      hundreds of path devices may fail or reappear simultaneously. In order to
      make sure that path WWIDs do not change (see
      <a class="xref" href="#sec-multipath-conf-file-wwid" title="18.12.1. WWIDs and device Identification">Section 18.12.1, “WWIDs and device Identification”</a>),
      the value should be set exactly like this:
     </p><div class="verbatim-wrap"><pre class="screen">defaults {
     uid_attrs "sd:ID_SERIAL dasd:ID_UID nvme:ID_WWN"
}</pre></div></dd><dt id="id-1.11.6.6.16.3.3"><span class="term">skip_kpartx</span></dt><dd><p>
      If set to <code class="literal">yes</code> for a multipath device (default is
      <code class="literal">no</code>), do not create partition devices on top of the
      given device (see
      <a class="xref" href="#sec-multipath-configuration-partitioning" title="18.7.3. Partitions on multipath devices and kpartx">Section 18.7.3, “Partitions on multipath devices and <code class="command">kpartx</code>”</a>).
      Useful for multipath devices used by virtual machines. Previous
      <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> releases achieved the same effect with the parameter
      “<code class="literal">features 1 no_partitions</code>”.
     </p></dd><dt id="id-1.11.6.6.16.3.4"><span class="term">max_sectors_kb</span></dt><dd><p>
      Limits the maximum amount of data sent in a single I/O request for all
      path devices of the multipath map.
     </p></dd><dt id="id-1.11.6.6.16.3.5"><span class="term">ghost_delay</span></dt><dd><p>
      On active/passive arrays, it can happen that passive paths (in
      “ghost” state) are probed before active paths. If the map was
      activated immediately and I/O was sent, this would cause a possibly
      costly path activation. This parameter specifies the time (in seconds) to
      wait for active paths of the map to appear before activating the map. The
      default is <code class="literal">no</code> (no ghost delay).
     </p></dd><dt id="id-1.11.6.6.16.3.6"><span class="term">recheck_wwid</span></dt><dd><p>
      If set to <code class="literal">yes</code> (default is <code class="literal">no</code>),
      double-checks the WWID of restored paths after failure, and removes them
      if the WWID has changed. This is a safety measure against data
      corruption.
     </p></dd><dt id="id-1.11.6.6.16.3.7"><span class="term">enable_foreign</span></dt><dd><p>
      <code class="systemitem">multipath-tools</code> provides a plugin API for other
      multipathing backends than Device Mapper multipath. The API supports
      monitoring and displaying information about the multipath topology using
      standard commands like <code class="command">multipath -ll</code>. Modifying the
      topology is unsupported.
     </p><p>
      The value of <code class="literal">enable_foreign</code> is a regular expression to
      match against foreign library names. The default value is
      “<code class="literal">NONE</code>”.
     </p><p>
      <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> ships the <code class="literal">nvme</code> plugin, which adds
      support for the native NVMe multipathing (see
      <a class="xref" href="#sec-multipath-hardware-implementations" title="18.2.1. Multipath implementations: device mapper and NVMe">Section 18.2.1, “Multipath implementations: device mapper and NVMe”</a>).
      To enable the <code class="literal">nvme</code> plugin, set
     </p><div class="verbatim-wrap"><pre class="screen">defaults {
    enable_foreign nvme
}</pre></div></dd></dl></div><section class="sect2" id="sec-multipath-marginal" data-id-title="Handling unreliable (“marginal”) path devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.13.1 </span><span class="title-name">Handling unreliable (“marginal”) path devices</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-marginal">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Unstable conditions in the fabric can cause path devices to behave
    erratically. They exhibit frequent I/O errors, recover, and fail again.
    Such path devices are also denoted “marginal” or
    “shaky” paths. This section summarizes the options that
    <code class="systemitem">multipath-tools</code> provides to deal with this
    problem.
   </p><div id="id-1.11.6.6.16.4.3" data-id-title="multipathds marginal path checking algorithm" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: multipathd's marginal path checking algorithm</div><p>
     If a path device exhibits a second failure (good → bad transition) before
     <code class="literal">marginal_path_double_failed_time</code> elapses after the
     first failure, <code class="command">multipathd</code> starts monitoring the path at
     a rate of 10 requests per second, for a monitoring period of
     <code class="literal">marginal_path_err_sample_time</code>. If the error rate during
     the monitoring period exceeds
     <code class="literal">marginal_path_err_rate_threshold</code>, the path is
     classified as marginal. After
     <code class="literal">marginal_path_err_recheck_gap_time</code>, the path
     transitions to normal state again.
    </p><p>
     This algorithm is used if all four numeric
     <code class="literal">marginal_path_</code> parameters are set to a positive value,
     and <code class="literal">marginal_pathgroups</code> is not set to
     <code class="literal">fpin</code>. It is available since SUSE Linux Enterprise Server 15 SP1 and
     SUSE Linux Enterprise Server 12 SP5.
    </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.16.4.4.1"><span class="term">marginal_path_double_failed_time</span></dt><dd><p>
       Maximum time (in seconds) between two path failures that triggers path
       monitoring.
      </p></dd><dt id="id-1.11.6.6.16.4.4.2"><span class="term">marginal_path_err_sample_time</span></dt><dd><p>
       Length (in seconds) of the path monitoring interval.
      </p></dd><dt id="id-1.11.6.6.16.4.4.3"><span class="term">marginal_path_err_rate_threshold</span></dt><dd><p>
       Minimum error rate (per thousand I/Os).
      </p></dd><dt id="id-1.11.6.6.16.4.4.4"><span class="term">marginal_path_err_recheck_gap_time</span></dt><dd><p>
       Time (in seconds) to keep the path in marginal state.
      </p></dd><dt id="id-1.11.6.6.16.4.4.5"><span class="term">marginal_pathgroups</span></dt><dd><p>
       This option is available since <span class="productname"><span class="phrase">SLES</span></span> 15SP3. Possible
       values are:
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.16.4.4.5.2.2.1"><span class="term">off</span></dt><dd><p>
          Marginal state is determined by <code class="command">multipathd</code> (see
          above). Marginal paths are not reinstated as long as they remain in
          marginal state. This is the default, and the behavior in SUSE Linux Enterprise Server
          releases before SP3, where the <code class="literal">marginal_pathgroups</code>
          option was unavailable.
         </p></dd><dt id="id-1.11.6.6.16.4.4.5.2.2.2"><span class="term">on</span></dt><dd><p>
          Similar to the <code class="literal">off</code> option, but instead of keeping
          them in the failed state, marginal paths are moved to a separate path
          group, which will be assigned a lower priority than all other path
          groups (see <a class="xref" href="#sec-multipath-grouping" title="18.10. Configuring path grouping and priorities">Section 18.10, “Configuring path grouping and priorities”</a>).
          Paths in this path group will only be used for I/O if all paths in
          other path groups have failed.
         </p></dd><dt id="id-1.11.6.6.16.4.4.5.2.2.3"><span class="term">fpin</span></dt><dd><p>
          This setting is available from <span class="productname"><span class="phrase">SLES</span></span> 15SP4.
          Marginal path state is derived from FPIN events (see below). Marginal
          paths are moved into a separate path group, as described
          for<code class="literal">off</code>. This setting requires no further host-side
          configuration. It is the recommended way to handle marginal paths on
          Fibre Channel fabrics that support FPIN.
         </p><div id="id-1.11.6.6.16.4.4.5.2.2.3.2.2" data-id-title="FPIN-based marginal path detection" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: FPIN-based marginal path detection</div><p>
           <code class="command">multipathd</code> listens for Fibre Channel Performance
           Impact Notifications (FPIN). If an FPIN-LI (Link Integrity) event is
           received for a path device, the path will enter marginal state. This
           state will last until a RSCN or Link up event is received on the
           Fibre channel adapter the device is connected to.
          </p></div></dd></dl></div></dd></dl></div><p>
    A simpler algorithm using the parameters
    <code class="literal">san_path_err_threshold</code>,
    <code class="literal">san_path_err_forget_rate</code>, and
    <code class="literal">san_path_err_recovery time</code> is also available and
    recommended for SUSE Linux Enterprise Server 15 (GA). See the “Shaky paths
    detection” section in <code class="systemitem">multipath.conf(5)</code>.
   </p></section></section><section class="sect1" id="sec-multipath-best-practice" data-id-title="Best practice"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.14 </span><span class="title-name">Best practice</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-best-practice">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-multipath-best-practice-config" data-id-title="Best practices for configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.14.1 </span><span class="title-name">Best practices for configuration</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-best-practice-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The large number of configuration directives is daunting at first. Usually,
    you can get good results with an empty configuration, unless you are in a
    clustering environment.
   </p><p>
    Here are some general recommendations for stand-alone servers. They are
    <span class="emphasis"><em>not mandatory</em></span>. See the documentation of the respective
    parameters in the previous sections for background information.
   </p><div class="verbatim-wrap"><pre class="screen">defaults {
    deferred_remove      yes
    find_multipaths      smart
    enable_foreign       nvme
    marginal_pathgroups  fpin    # 15.4 only, if supported by fabric
}
devices {
    # A catch-all device entry.
    device {
        vendor                .*
        product               .*
        dev_loss_tmo          infinity
        no_path_retry         60            # 5 minutes
        path_grouping_policy  group_by_prio
        path_selector         "historical-service-time 0"
        reservation_key       file          # if using SCSI persistent reservations
    }
    # Follow up with specific device entries below, they will take precedence.
}</pre></div><p>
    After modifying the <code class="filename">/etc/multipath.conf</code> file, apply
    your settings as described in
    <a class="xref" href="#sec-multipath-conf-file-apply" title="18.8.4. Applying multipath.conf modifications">Section 18.8.4, “Applying <code class="filename">multipath.conf</code> modifications”</a>.
   </p></section><section class="sect2" id="sec-multipath-best-practice-status" data-id-title="Interpreting multipath I/O status"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.14.2 </span><span class="title-name">Interpreting multipath I/O status</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-best-practice-status">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For a quick overview of the multipath subsystem, use <code class="command">multipath
    -ll</code> or <code class="command">multipathd show topology</code>. The output of
    these commands has the same format. The former command reads the kernel
    state, while the latter prints the status of the multipath daemon. Normally
    both states are equal. Here is an example of the output:
   </p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd show topology
mpatha<span class="callout" id="mp-co-top-name">1</span> (3600a098000aad1e300000b4b5a275d45<span class="callout" id="mp-co-top-wwid">2</span>) dm-0<span class="callout" id="mp-co-top-dev">3</span> NETAPP,INF-01-00<span class="callout" id="mp-co-top-prod">4</span>
size=64G features='3 queue_if_no_path pg_init_retries 50'<span class="callout" id="mp-co-top-feat">5</span> hwhandler='1 alua'<span class="callout" id="mp-co-top-hwh">6</span> wp=rw<span class="callout" id="mp-co-top-wp">7</span>
|-+- <span class="callout" id="mp-co-top-pg">8</span>policy='historical-service-time 2'<span class="callout" id="mp-co-top-ps">9</span> prio=50<span class="callout" id="mp-co-top-prio">10</span> status=active<span class="callout" id="mp-co-top-pgst">11</span>
| |-<span class="callout" id="mp-co-top-pp">12</span> 3:0:0:1<span class="callout" id="mp-co-top-hbtl">13</span> sdb 8:16<span class="callout" id="mp-co-top-sd">14</span> active<span class="callout" id="mp-co-top-dmst">15</span> ready<span class="callout" id="mp-co-top-st">16</span> running<span class="callout" id="mp-co-top-devst">17</span>
| `- 4:0:0:1 sdf 8:80  active ready running
`-+- policy='historical-service-time 2' prio=10 status=enabled
  `- 4:0:1:1 sdj 8:144 active ready running</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-name"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The map name.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-wwid"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The map WWID (if different from the map name).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-dev"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The device node name of the map device.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-prod"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The vendor and product name.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-pg"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       A path group. The indented lines below the path group list the path
       devices that belong to it.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-ps"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The path selector algorithm used by the path group. The "2" can be
       ignored.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-prio"><span class="callout">10</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The priority of the path group.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-pgst"><span class="callout">11</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The status of the path group (<code class="literal">active</code>,
       <code class="literal">enabled</code> or <code class="literal">disabled</code>). The active
       path group is the one that I/O is currently sent to.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-pp"><span class="callout">12</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       A path device.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-hbtl"><span class="callout">13</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The bus ID of the device (here, a SCSI Host:Bus:Target:Lun ID).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-sd"><span class="callout">14</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The device node name and major/minor number of the path device.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-dmst"><span class="callout">15</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The kernel device mapper state of the path (<code class="literal">active</code> or
       <code class="literal">failed</code>).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-st"><span class="callout">16</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The multipath path device state (see below).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-top-devst"><span class="callout">17</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The state of the path device in the kernel. This is a device-type
       specific value. For SCSI, it is either <code class="literal">running</code> or
       <code class="literal">offline</code>.
      </p></td></tr></table></div></div><p>
    The multipath path device states are:
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">ready</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         The path is healthy and up
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">ghost</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         A passive path in an active/passive array
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">faulty</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         The path is down or unreachable
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">i/o timeout</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         A checker command timed out
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">i/o pending</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Waiting for the completion of a path checker command
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">delayed</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Path reinstantiation is delayed to avoid "flapping"
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         <code class="literal">shaky</code>
        </p>
       </td><td>
        <p>
         An unreliable path (emc path checker only)
        </p>
       </td></tr></tbody></table></div></section><section class="sect2" id="sec-multipath-lvm" data-id-title="Using LVM2 on multipath devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.14.3 </span><span class="title-name">Using LVM2 on multipath devices</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-lvm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    LVM2 has built-in support for detecting multipath devices. It is activated
    by default in <code class="filename">/etc/lvm/lvm.conf</code>:
   </p><div class="verbatim-wrap"><pre class="screen">    multipath_component_detection=1</pre></div><p>
    This works reliably only if LVM2 is also configured to obtain information
    about device properties from udev:
   </p><div class="verbatim-wrap"><pre class="screen">    external_device_info_source="udev"</pre></div><p>
    This is the default in SUSE Linux Enterprise 15 SP4, but not in earlier releases. It
    is also possible (although normally not necessary) to create a filter
    expression for LVM2 to ignore all devices except multipath devices. See
    <a class="xref" href="#sec-multipath-referring" title="18.12.4. Referring to multipath maps">Section 18.12.4, “Referring to multipath maps”</a>.
   </p></section><section class="sect2" id="sec-multipath-best-practice-io-stalled" data-id-title="Resolving stalled I/O"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.14.4 </span><span class="title-name">Resolving stalled I/O</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-best-practice-io-stalled">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If all paths fail concurrently and I/O is queued, applications may stall
    for a long time. To resolve this, you can use the following procedure:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Enter the following command at a terminal prompt:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd disablequeueing map <em class="replaceable">MAPNAME</em></pre></div><p>
      Replace <code class="literal"><em class="replaceable">MAPNAME</em></code> with the
      correct WWID or mapped alias name for the device.
     </p><p>
      This command immediately causes all queued I/O to fail and propagates the
      error to the calling application. File systems will observe I/O errors
      and switch to read-only mode.
     </p></li><li class="step"><p>
      Reactivate queuing by entering the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> multipathd restorequeueing <em class="replaceable">MAPNAME</em></pre></div></li></ol></div></div></section><section class="sect2" id="sec-multipath-mpiotools-mdadm" data-id-title="MD RAID on multipath devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.14.5 </span><span class="title-name">MD RAID on multipath devices</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-mpiotools-mdadm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    MD RAID arrays on top of multipathing are set up automatically by the
    system's udev rules. No special configuration in
    <code class="filename">/etc/mdadm.conf</code> is necessary.
   </p></section><section class="sect2" id="sec-multipath-best-practice-scandev" data-id-title="Scanning for new devices without rebooting"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.14.6 </span><span class="title-name">Scanning for new devices without rebooting</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-best-practice-scandev">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If your system has already been configured for multipathing and you need to
    add storage to the SAN, you can use the
    <code class="command">rescan-scsi-bus.sh</code> script to scan for the new devices.
    The general syntax for the command follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> rescan-scsi-bus.sh [-a] [-r] --hosts=2-3,5</pre></div><p>
    Where the options have the following meaning:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.6.17.7.5.1"><span class="term">-a</span></dt><dd><p>
       the option ensures that all SCSI targets are scanned, otherwise only
       already existing targets will be scanned for new LUNs.
      </p></dd><dt id="id-1.11.6.6.17.7.5.2"><span class="term">-r</span></dt><dd><p>
       the option enables the removal of devices which have been removed on the
       storage side.
      </p></dd><dt id="id-1.11.6.6.17.7.5.3"><span class="term">--hosts</span></dt><dd><p>
       the option specifies the list of host bus adapters to scan (the default
       is to scan all).
      </p></dd></dl></div><p>
    Run <code class="command">rescan-scsi-bus.sh --help</code> for help on additional
    options.
   </p><p>
    If <code class="command">multipathd</code> is running and new SAN devices are
    discovered, they should be automatically set up as multipath maps according
    to the configuration described in
    <a class="xref" href="#sec-multipath-select-devices" title="18.11. Selecting devices for multipathing">Section 18.11, “Selecting devices for multipathing”</a>.
   </p><div id="id-1.11.6.6.17.7.8" data-id-title="Dell/EMC PowerPath environments" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Dell/EMC PowerPath environments</div><p>
     In EMC PowerPath environments, do not use the
     <code class="filename">rescan-scsi-bus.sh</code> utility provided with the
     operating system or the HBA vendor scripts for scanning the SCSI buses. To
     avoid potential file system corruption, EMC requires that you follow the
     procedure provided in the vendor documentation for EMC PowerPath for
     Linux.
    </p></div></section></section><section class="sect1" id="sec-multipath-trouble" data-id-title="Troubleshooting MPIO"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.15 </span><span class="title-name">Troubleshooting MPIO</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-trouble">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If a system runs into emergency mode on a system with multipath, printing
   messages about missing devices, the reason is almost always one of these:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Inconsistent configuration of multipath device selection
    </p></li><li class="listitem"><p>
     Use of non-existing device references
    </p></li></ul></div><section class="sect2" id="sec-multipath-trouble-select" data-id-title="Understanding device selection issues"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.15.1 </span><span class="title-name">Understanding device selection issues</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-trouble-select">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A block device can only either be part of a multipath map or be used
    directly (mounted as file system, used as swap, LVM physical volume, or
    otherwise). If a device is already mounted, an attempt by multipathd to
    make it part of a multipath map will fail with a “Device or resource
    busy” error. Vice-versa, the same error results if
    <code class="command">systemd</code> attempts to mount a device that has already
    been made part of a multipath map.
   </p><p>
    Storage device activation during boot is handled by a complex interaction
    between <code class="command">systemd</code>, <code class="systemitem">udev</code>,
    <code class="command">multipathd</code> and some other tools.
    <code class="systemitem">udev</code> rules play a central role. They set device
    properties that indicate to other subsystems how a device should be used.
    The multipath-related udev rules set the following properties for devices
    that are selected for multipathing:
   </p><div class="verbatim-wrap"><pre class="screen">SYSTEMD_READY=0
DM_MULTIPATH_DEVICE_PATH=1</pre></div><p>
    Partition devices inherit these properties from their parents.
   </p><p>
    If these properties are not set correctly, if some tool does not respect
    them, or if they get set too late, a race condition between
    <code class="command">multipathd</code> and some other subsystem may result. Only one
    of the contenders can win the race; the other one will see a “Device
    or resource busy” error.
   </p><p>
    One problem in this context is that the tools of the LVM2 suite do not
    evaluate udev properties by default. They rely on their own logic for
    determining whether a device is a multipath component, which sometimes does
    not match the logic of the rest of the system. A workaround for this is
    described in <a class="xref" href="#sec-multipath-lvm" title="18.14.3. Using LVM2 on multipath devices">Section 18.14.3, “Using LVM2 on multipath devices”</a>.
   </p><div id="id-1.11.6.6.18.4.8" data-id-title="Example of boot deadlock" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Example of boot deadlock</div><p>
     Consider a system with multipathing where the root device is not
     multipathed, and no devices are excluded from multipath (see
     <a class="xref" href="#vl-multipath-planning-type-noroot-noinitrd">Multipath disabled in the initramfs</a> in
     <a class="xref" href="#sec-multipath-planning-type-noroot" title="18.3.2.2. Root file system on a local disk">Section 18.3.2.2, “Root file system on a local disk”</a>). The root file
     system is mounted in the initramfs. <code class="command">systemd</code> switches to
     the root file system and <code class="command">multipathd</code> starts up. Because
     the device is already mounted, <code class="command">multipathd</code> fails to set
     up the multipath map for it. Because the root device is not configured in
     <code class="literal">blacklist</code>, it is considered a multipath device, and
     <code class="literal">SYSTEMD_READY=0</code> is set for it.
    </p><p>
     Later in the boot process, the system attempts to mount additional file
     systems like <code class="filename">/var</code> and <code class="filename">/home</code>.
     Usually, these file systems will be on the same device as the root file
     system, by default as BTRFS subvolumes of the root file system itself. But
     systemd cannot mount them because of <code class="literal">SYSTEMD_READY=0</code>.
     <span class="emphasis"><em>We are in a deadlock</em></span>: The dm-multipath device cannot
     be created, and the underlying device is blocked for systemd. The
     additional file systems cannot be mounted, resulting in boot failure.
    </p><p>
     <span class="bold"><strong>A solution to this problem already
     exists.</strong></span> <code class="command">multipathd</code> detects this situation
     and releases the device to <code class="command">systemd</code> which can then
     proceed mounting the file system. However, it is important to understand
     the general problem, which can still occur in more subtle ways.
    </p></div></section><section class="sect2" id="sec-multipath-trouble-ref" data-id-title="Understanding device referencing issues"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.15.2 </span><span class="title-name">Understanding device referencing issues</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-trouble-ref">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    An example of a device referencing issue has been given in
    <a class="xref" href="#sec-multipath-initrd-persistent" title="18.7.4.2. Persistent device names in the initramfs">Section 18.7.4.2, “Persistent device names in the initramfs”</a>. Typically, there are
    multiple symbolic links pointing to a device node (see
    <a class="xref" href="#sec-multipath-referring" title="18.12.4. Referring to multipath maps">Section 18.12.4, “Referring to multipath maps”</a>). But these links do not always
    exist; <code class="command">udev</code> creates them according to the current udev
    rules. For example, if multipathing is off, symbolic links under
    <code class="filename">/dev/mapper/</code> for multipath devices will be missing.
    Thus, any reference to a <code class="filename">/dev/mapper/</code> device will
    fail.
   </p><p>
    Such references can appear in various places, notably in
    <code class="filename">/etc/fstab</code> and <code class="filename">/etc/crypttab</code>, in
    the initramfs, or even on the kernel command line.
   </p><p>
    The safest way to circumvent this problem is to avoid using the kind of
    device references that are not persistent between boots or depend on system
    configuration. We generally recommend referring to file systems (and
    similar entities like swap space) by properties of the file system itself
    (like UUID or label) rather than the containing device. If such references
    are not available and device references are required, for example, in
    <code class="filename">/etc/crypttab</code>, the options should be evaluated
    carefully. For example, in <a class="xref" href="#sec-multipath-referring" title="18.12.4. Referring to multipath maps">Section 18.12.4, “Referring to multipath maps”</a>, the
    best option might be the <code class="filename">/dev/disk/by-id/wwn-</code> link
    because it would also work with <code class="literal">multipath=off</code>.
   </p></section><section class="sect2" id="sec-multipath-trouble-steps" data-id-title="Troubleshooting steps in emergency mode"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.15.3 </span><span class="title-name">Troubleshooting steps in emergency mode</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-trouble-steps">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    As there are many error situations that differ in subtle ways, it is
    impossible to provide a step-by-step recovery guide. But with the
    background knowledge from the previous subsections, you should be able to
    figure out the problem if a system runs into emergency mode because of
    multipathing issues. Before you begin debugging, make sure you have checked
    the following questions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Is the multipath service enabled?
     </p></li><li class="listitem"><p>
      Is the multipath dracut module included in the initramfs?
     </p></li><li class="listitem"><p>
      Is my root device configured as a multipath device? If not, is the root
      device properly excluded from multipath as described in
      <a class="xref" href="#sec-multipath-blacklist" title="18.11.1. The blacklist section in multipath.conf">Section 18.11.1, “The <code class="literal">blacklist</code> section in <code class="filename">multipath.conf</code>”</a>, or are you relying on the
      absence of the multipath module in the initramfs (see
      <a class="xref" href="#sec-multipath-planning-type-noroot" title="18.3.2.2. Root file system on a local disk">Section 18.3.2.2, “Root file system on a local disk”</a>)?
     </p></li><li class="listitem"><p>
      Does the system enter emergency mode before or after switching to the
      real root file system?
     </p></li></ul></div><p>
    If you are unsure with respect to the last question, here is a sample
    dracut emergency prompt as it would be printed before switching root:
   </p><div class="verbatim-wrap"><pre class="screen">Generating "/run/initramfs/rdsosreport.txt"
Entering emergency mode. Exit the shell to continue.
Type "journalctl" to view system logs.

You might want to save "/run/initramfs/rdsosreport.txt" to a USB stick or /boot
after mounting them and attach it to a bug report.

Give root password for maintenance
(or press Control-D to continue):</pre></div><p>
    The mention of <code class="filename">rdsosreport.txt</code> is a clear indication
    that the system is still running from the initramfs. If you are still
    uncertain, log in and check for the existence of the file
    <code class="filename">/etc/initrd-release</code>. This file exists only in an
    initramfs environment.
   </p><p>
    If emergency mode is entered after switching root, the emergency prompt
    looks similar, but <code class="filename">rdsosreport.txt</code> is not mentioned:
   </p><div class="verbatim-wrap"><pre class="screen">Timed out waiting for device dev-disk-by\x2duuid-c4a...cfef77d.device.
[DEPEND] Dependency failed for Local File Systems.
[DEPEND] Dependency failed for Postfix Mail Transport Agent.
Welcome to emergency shell
Give root password for maintenance
(or press Control-D to continue):</pre></div><div class="procedure" id="id-1.11.6.6.18.6.9" data-id-title="Steps for analyzing the situation in emergency mode"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 18.2: </span><span class="title-name">Steps for analyzing the situation in emergency mode </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.6.18.6.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Try to figure out what failed by examining failed systemd units and the
      journal.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl --failed
<code class="prompt root"># </code>journalctl -b -o short-monotonic</pre></div><p>
      When looking at the journal, determine the <span class="emphasis"><em>first</em></span>
      failed unit. When you have found the first failure, examine the messages
      before and around that point in time very carefully. Are there any
      warnings or other suspicious messages?
     </p><p>
      Watch out for the root switch ("<code class="literal">Switching root.</code>") and
      for messages about SCSI devices, device mapper, multipath and LVM2. Look
      for <code class="command">systemd</code> messages about devices and file systems
      ("<code class="literal">Found device</code>…", "<code class="literal">Mounting</code>…",
      "<code class="literal">Mounted</code>…").
     </p></li><li class="step"><p>
      Examine the existing devices, both low-level devices and device mapper
      devices (note that some of the commands below may not be available in the
      initramfs):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>cat /proc/partitions
<code class="prompt root"># </code>ls -l /sys/class/block
<code class="prompt root"># </code>ls -l /dev/disk/by-id/* /dev/mapper/*
<code class="prompt root"># </code>dmsetup ls --tree
<code class="prompt root"># </code>lsblk
<code class="prompt root"># </code>lsscsi</pre></div><p>
      From the output of the commands above, you should get an idea whether the
      low-level devices were successfully probed, and whether any multipath
      maps and multipath partitions were set up.
     </p></li><li class="step"><p>
      If the device mapper multipath setup is not as you expect, examine the
      udev properties, in particular, <code class="literal">SYSTEMD_READY</code> (see
      above)
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>udevadm info -e</pre></div></li><li class="step"><p>
      If the previous step showed unexpected udev properties, something may
      have gone wrong during udev rule processing. Check other properties, in
      particular, those used for device identification (see
      <a class="xref" href="#sec-multipath-conf-file-wwid" title="18.12.1. WWIDs and device Identification">Section 18.12.1, “WWIDs and device Identification”</a>).
      If the udev properties are correct, check the journal for
      <code class="command">multipathd</code> messages again. Look for "<code class="literal">Device
      or resource busy</code>" messages.
     </p></li><li class="step"><p>
      If the system failed to mount or otherwise activate a device, it is often
      helpful to try activating this device manually:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount /var
<code class="prompt root"># </code>swapon -a
<code class="prompt root"># </code>vgchange -a y</pre></div><p>
      Mostly, the manual activation will succeed and allow to proceed with
      system boot (usually by simply logging out from the emergency shell) and
      examine the situation further in the booted system.
     </p><p>
      If manual activation fails, you will probably see error messages that
      provide clues about what is going wrong. You can also try the commands
      again with increased verbosity.
     </p></li><li class="step"><p>
      At this point, you should have some idea what went wrong (if not, contact
      SUSE support and be prepared to answer most of the questions raised
      above).
     </p><p>
      You should be able to correct the situation with a few shell commands,
      exit the emergency shell, and boot successfully. You will still need to
      adjust your configuration to make sure the same problem will not occur
      again in the future.
     </p><p>
      Otherwise, you will need to boot the rescue system, set up the devices
      manually to <code class="command">chroot</code> into the real root file system, and
      attempt to fix the problem based on the insight you got in the previous
      steps. Be aware that in this situation, the storage stack for the root
      file system may differ from normal. Depending on your setup, you may have
      force addition or omission of dracut modules when building a new
      initramfs. See also <a class="xref" href="#sec-multipath-initrd-disable" title="18.7.4.1. Enabling or disabling multipathing in the initramfs">Section 18.7.4.1, “Enabling or disabling multipathing in the initramfs”</a>.
     </p></li><li class="step"><p>
      If the problem occurs frequently or even on every boot attempt, try
      booting with increased verbosity in order to get more information about
      the failure. The following kernel parameters, or a combination of them,
      are often helpful:
     </p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">udev.log-priority=debug<span class="callout" id="mp-co-kparm-udev">1</span>
systemd.log_level=debug<span class="callout" id="mp-co-kparm-sl">2</span>
scsi_mod.scsi_logging_level=020400<span class="callout" id="mp-co-kparm-sc">3</span>
rd.debug<span class="callout" id="mp-co-kparm-rd">4</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-kparm-udev"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Increase the log level of <code class="command">systemd-udevd</code> and udev
         rule processing.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-kparm-sl"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Increase the log level of <code class="command">systemd</code>.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-kparm-sc"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Increase the logging level of the kernel's SCSI subsystem.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#mp-co-kparm-rd"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Trace the scripts in the initramfs.
        </p></td></tr></table></div></div><p>
      In addition, it may make sense to enable logging for certain drivers and
      configure a serial console to capture the output during boot.
     </p></li></ol></div></div></section><section class="sect2" id="sec-multipath-trouble-tids" data-id-title="Technical information documents"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.15.4 </span><span class="title-name">Technical information documents</span></span> <a title="Permalink" class="permalink" href="#sec-multipath-trouble-tids">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_multipath.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For more information about troubleshooting multipath I/O issues on SUSE
    Linux Enterprise Server, see the following Technical Information Documents
    (TIDs) in the SUSE Knowledgebase:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="link" href="https://www.suse.com/support/kb/doc/?id=000016331" target="_blank"><em class="citetitle">Using
      LVM on local and SAN attached devices</em></a>
     </p></li><li class="listitem"><p>
      <a class="link" href="https://www.suse.com/support/kb/doc/?id=000017521" target="_blank"><em class="citetitle">Using
      LVM on Multipath (DM MPIO) Devices</em></a>
     </p></li><li class="listitem"><p>
      <a class="link" href="https://www.suse.com/support/kb/doc/?id=000017762" target="_blank"><em class="citetitle">HOWTO:
      Add, Resize and Remove LUN without restarting SLES</em></a>
     </p></li></ul></div></section></section></section><section class="chapter" id="cha-nfs" data-id-title="Sharing file systems with NFS"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">19 </span><span class="title-name">Sharing file systems with NFS</span></span> <a title="Permalink" class="permalink" href="#cha-nfs">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
        The <span class="emphasis"><em>Network File System</em></span> (<span class="emphasis"><em>NFS</em></span>)
        is a protocol that allows access to files on a server in a manner
        similar to accessing local files.
      </p><p>
        <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> installs NFS v4.2, which introduces support for sparse
        files, file pre-allocation, server-side clone and copy, application
        data block (ADB), and labeled NFS for mandatory access control (MAC)
        (requires MAC on both client and server).
      </p></div></div></div></div><section class="sect1" id="sec-nfs-overview" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.1 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      The <span class="emphasis"><em>Network File System</em></span> (NFS) is a standardized,
      well-proven and widely supported network protocol that allows sharing
      files between separate hosts.
    </p><p>
      The <span class="emphasis"><em>Network Information Service</em></span> (NIS) can be used to
      have centralized user management in the network. Combining NFS and NIS
      allows using file and directory permissions for access control in the
      network. NFS with NIS makes a network transparent to the user.
    </p><p>
      In the default configuration, NFS completely trusts the network and thus
      any machine that is connected to a trusted network. Any user with
      administrator privileges on any computer with physical access to any
      network the NFS server trusts can access any files that the server makes
      available.
    </p><p>
      Often, this level of security is perfectly satisfactory, such as when the
      network that is trusted is truly private, often localized to a single
      cabinet or machine room, and no unauthorized access is possible. In other
      cases, the need to trust a whole subnet as a unit is restrictive, and
      there is a need for more fine-grained trust. To meet the need in these
      cases, NFS supports various security levels using the
      <span class="emphasis"><em>Kerberos</em></span> infrastructure. Kerberos requires NFSv4, which is
      used by default. For details, see
      <span class="intraxref">Book “Security and Hardening Guide”, Chapter 6 “Network authentication with Kerberos”</span>.
    </p><p>
      The following are terms used in the YaST module.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.7.3.7.1"><span class="term">Exports</span></dt><dd><p>
            A directory <span class="emphasis"><em>exported</em></span> by an NFS server, which
            clients can integrate into their systems.
          </p></dd><dt id="id-1.11.6.7.3.7.2"><span class="term">NFS client</span></dt><dd><p>
            The NFS client is a system that uses NFS services from an NFS
            server over the Network File System protocol. The TCP/IP protocol
            is already integrated into the Linux kernel; there is no need to
            install any additional software.
          </p></dd><dt id="id-1.11.6.7.3.7.3"><span class="term">NFS server</span></dt><dd><p>
            The NFS server provides NFS services to clients. A running server
            depends on the following daemons:
            <code class="systemitem">nfsd</code> (worker),
            <code class="systemitem">idmapd</code> (ID-to-name mapping
            for NFSv4, needed for certain scenarios only),
            <code class="systemitem">statd</code> (file locking),
            and <code class="systemitem">mountd</code> (mount
            requests).
          </p></dd><dt id="id-1.11.6.7.3.7.4"><span class="term">NFSv3</span></dt><dd><p>
            NFSv3 is the version 3 implementation, the <span class="quote">“<span class="quote">old</span>”</span>
            stateless NFS that supports client authentication.
          </p></dd><dt id="id-1.11.6.7.3.7.5"><span class="term">NFSv4</span></dt><dd><p>
            NFSv4 is the new version 4 implementation that supports secure user
            authentication via Kerberos. NFSv4 requires one single port only and
            thus is better suited for environments behind a firewall than
            NFSv3.
          </p><p>
            The protocol is specified as
            <a class="link" href="https://datatracker.ietf.org/doc/rfc7531/" target="_blank">https://datatracker.ietf.org/doc/rfc7531/</a>.
          </p></dd><dt id="id-1.11.6.7.3.7.6"><span class="term">pNFS</span></dt><dd><p>
            Parallel NFS, a protocol extension of NFSv4. Any pNFS clients can
            directly access the data on an NFS server.
          </p></dd></dl></div><div id="id-1.11.6.7.3.8" data-id-title="Need for DNS" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Need for DNS</div><p>
        In principle, all exports can be made using IP addresses only. To avoid
        timeouts, you need a working DNS system. DNS is necessary at least for
        logging purposes, because the
        <code class="systemitem">mountd</code> daemon does reverse
        lookups.
      </p></div></section><section class="sect1" id="sec-nfs-installation" data-id-title="Installing NFS server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.2 </span><span class="title-name">Installing NFS server</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      The NFS server is not part of the default installation. To install the
      NFS server using YaST, choose <span class="guimenu">Software</span> › <span class="guimenu">Software Management</span>, select
      <span class="guimenu">Patterns</span>, and enable the <span class="guimenu">File
      Server</span> option in the <span class="guimenu">Server Functions</span>
      section. Click <span class="guimenu">Accept</span> to install the required
      packages.
    </p><p>
      The pattern does not include the YaST module for the NFS Server.
      After the pattern installation is complete, install the module by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">zypper in yast2-nfs-server</code></pre></div><p>
      Like NIS, NFS is a client/server system. However, a machine can be
      both—it can supply file systems over the network (export) and mount
      file systems from other hosts (import).
    </p></section><section class="sect1" id="sec-nfs-configuring-nfs-server" data-id-title="Configuring NFS server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.3 </span><span class="title-name">Configuring NFS server</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-configuring-nfs-server">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Configuring an NFS server can be done either through YaST or manually.
      For authentication, NFS can also be combined with Kerberos.
    </p><section class="sect2" id="sec-nfs-export-yast2" data-id-title="Exporting file systems with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.3.1 </span><span class="title-name">Exporting file systems with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-export-yast2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        With YaST, turn a host in your network into an NFS server—a
        server that exports directories and files to all hosts granted access
        to it or to all members of a group. Thus, the server can also provide
        applications without installing the applications locally on every host.
      </p><p>
        To set up such a server, proceed as follows:
      </p><div class="procedure" id="pro-nfs-export-yast2-nfs" data-id-title="Setting up an NFS server"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.1: </span><span class="title-name">Setting up an NFS server </span></span><a title="Permalink" class="permalink" href="#pro-nfs-export-yast2-nfs">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Start YaST and select <span class="guimenu">Network
            Services</span> › <span class="guimenu">NFS Server</span>; see
            <a class="xref" href="#fig-inst-nfsserver1" title="NFS server configuration tool">Figure 19.1, “NFS server configuration tool”</a>. You may be prompted to
            install additional software.
          </p><div class="figure" id="fig-inst-nfsserver1"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_inst_nfsserver1.png"><img src="images/yast2_inst_nfsserver1.png" width="75%" alt="NFS server configuration tool" title="NFS server configuration tool"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 19.1: </span><span class="title-name">NFS server configuration tool </span></span><a title="Permalink" class="permalink" href="#fig-inst-nfsserver1">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
            Click the <span class="guimenu">Start</span> radio button.
          </p></li><li class="step"><p>
            If <code class="systemitem">firewalld</code> is active on your system, configure it separately
            for NFS (see <a class="xref" href="#sec-nfs-firewall" title="19.5. Operating an NFS server and clients behind a firewall">Section 19.5, “Operating an NFS server and clients behind a firewall”</a>).
            YaST does not yet have complete support for <code class="systemitem">firewalld</code>, so
            ignore the "Firewall not configurable" message and continue.
          </p></li><li class="step"><p>
            Check whether you want to <span class="guimenu">Enable NFSv4</span>. If you
            deactivate NFSv4, YaST will only support NFSv3. For information
            about enabling NFSv2, see
            <a class="xref" href="#sec-nfs-export-manual-nsfv2" title="Note: NFSv2">Note: NFSv2</a>.
          </p><ol type="a" class="substeps"><li class="step"><p>
                If NFSv4 is selected, additionally enter the appropriate NFSv4
                domain name. This parameter is used by the
                <code class="systemitem">idmapd</code> daemon
                that is required for Kerberos setups or if clients cannot work
                with numeric user names. Leave it as
                <code class="literal">localdomain</code> (the default) if you do not run
                <code class="systemitem">idmapd</code> or do not have
                any special requirements. For more information on the
                <code class="systemitem">idmapd</code> daemon,
                see <a class="xref" href="#var-nfs-export-manual-idmapd"><code class="filename">/etc/idmapd.conf</code></a>.
              </p><div id="id-1.11.6.7.5.3.4.5.2.1.2" data-id-title="NFSv4 Domain Name" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: NFSv4 Domain Name</div><p>
                  Note that the domain name needs to be configured on all NFSv4
                  clients as well. Only clients that share the same domain name
                  as the server can access the server. The default domain name
                  for server and clients is <code class="literal">localdomain</code>.
                 </p></div></li></ol></li><li class="step"><p>
            Click <span class="guimenu">Enable GSS Security</span> if you need secure
            access to the server. A prerequisite for this is to have Kerberos
            installed on your domain and to have both the server and the
            clients kerberized.
            
            Click <span class="guimenu">Next</span> to proceed with the next
            configuration dialog.
          </p></li><li class="step"><p>
            Click <span class="guimenu">Add Directory</span> in the upper half of the
            dialog to export your directory.
          </p></li><li class="step"><p>
            If you have not configured the allowed hosts already, another
            dialog for entering the client information and options pops up
            automatically. Enter the host wild card (usually you can leave the
            default settings as they are).
          </p><p>
            There are four possible types of host wild cards that can be set
            for each host: a single host (name or IP address), netgroups, wild
            cards (such as <code class="literal">*</code> indicating all machines can
            access the server), and IP networks.
          </p><p>
            For more information about these options, see the
            <code class="literal">exports</code> man page.
          </p></li><li class="step"><p>
            Click <span class="guimenu">Finish</span> to complete the configuration.
          </p></li></ol></div></div></section><section class="sect2" id="sec-nfs-export-manual" data-id-title="Exporting file systems manually"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.3.2 </span><span class="title-name">Exporting file systems manually</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-export-manual">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The configuration files for the NFS export service are
        <code class="filename">/etc/exports</code> and
        <code class="filename">/etc/sysconfig/nfs</code>. In addition to these files,
        <code class="filename">/etc/idmapd.conf</code> is needed for the NFSv4 server
        configuration with kerberized NFS or if the clients cannot work with
        numeric user names.
      </p><p>
        To start or restart the services, run the command <code class="command">systemctl
        restart nfs-server</code>. This also restarts the RPC port mapper
        that is required by the NFS server.
      </p><p>
        To make sure the NFS server always starts at boot time, run
        <code class="command">sudo systemctl enable nfs-server</code>.
      </p><div id="id-1.11.6.7.5.4.5" data-id-title="NFSv4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: NFSv4</div><p>
          NFSv4 is the latest version of the NFS protocol available on
          <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>. Configuring directories for export with NFSv4 is now
          the same as with NFSv3.
        </p><p>
          On
          <span class="phrase">SUSE Linux Enterprise Server 11</span>, the bind mount in
          <code class="filename">/etc/exports</code> was mandatory. It is still
          supported, but now deprecated.
        </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.7.5.4.6.1"><span class="term"><code class="filename">/etc/exports</code></span></dt><dd><p>
              The <code class="filename">/etc/exports</code> file contains a list of
              entries. Each entry indicates a directory that is shared and how
              it is shared. A typical entry in
              <code class="filename">/etc/exports</code> consists of:
            </p><div class="verbatim-wrap"><pre class="screen">/<em class="replaceable">SHARED</em>/<em class="replaceable">DIRECTORY</em>   <em class="replaceable">HOST</em>(<em class="replaceable">OPTION_LIST</em>)</pre></div><p>
              For example:
            </p><div class="verbatim-wrap"><pre class="screen">/nfs_exports/public *(rw,sync,root_squash,wdelay)
/nfs_exports/department1 *.department1.example.com(rw,sync,root_squash,wdelay)
/nfs_exports/team1 192.168.1.0/24(rw,sync,root_squash,wdelay)
/nfs_exports/tux 192.168.1.2(rw,sync,root_squash)</pre></div><p>
              In this example, the following values for
              <em class="replaceable">HOST</em> are used:
            </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                  <code class="literal">*</code>: exports to all clients on the network
                </p></li><li class="listitem"><p>
                  <code class="literal">*.department1.example.com</code>: only exports
                  to clients on the *.department1.example.com domain
                </p></li><li class="listitem"><p>
                  <code class="literal">192.168.1.0/24</code>: only exports
                  to clients with IP adresses in the range of 192.168.1.0/24
                </p></li><li class="listitem"><p>
                  <code class="literal">192.168.1.2</code>: only exports
                  to the machine with the IP address 192.168.1.2
                </p></li></ul></div><p>
              In addition to the examples above, you can also restrict exports
              to netgroups (<code class="literal">@my-hosts</code>) defined in
              <code class="filename">/etc/netgroup</code>.
              For a detailed explanation of all options and their meanings,
              refer to the <code class="literal">man</code> page of
              <code class="filename">/etc/exports</code>: (<code class="command">man
              exports</code>).
            </p><p>
              In case you have modified <code class="filename">/etc/exports</code> while
              the NFS server was running, you need to restart it for the
              changes to become active: <code class="command">sudo systemctl restart
              nfs-server</code>.
            </p></dd><dt id="id-1.11.6.7.5.4.6.2"><span class="term"><code class="filename">/etc/sysconfig/nfs</code></span></dt><dd><p>
              The <code class="filename">/etc/sysconfig/nfs</code> file contains a few
              parameters that determine NFSv4 server daemon behavior. It is
              important to set the parameter
              <code class="systemitem">NFS4_SUPPORT</code> to <code class="literal">yes</code>
              (default). <code class="systemitem">NFS4_SUPPORT</code> determines
              whether the NFS server supports NFSv4 exports and clients.
            </p><p>
              In case you have modified <code class="filename">/etc/sysconfig/nfs</code>
              while the NFS server was running, you need to restart it for the
              changes to become active: <code class="command">sudo systemctl restart
              nfs-server</code>.
            </p><div id="id-1.11.6.7.5.4.6.2.2.3" data-id-title="Mount options" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Mount options</div><p>
                On
                <span class="phrase">SUSE Linux Enterprise Server 11</span>, the <code class="option">--bind</code> mount in
                <code class="filename">/etc/exports</code> was mandatory. It is still
                supported, but now deprecated. Configuring directories for
                export with NFSv4 is now the same as with NFSv3.
              </p></div><div id="sec-nfs-export-manual-nsfv2" data-id-title="NFSv2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: NFSv2</div><p>
                If NFS clients still depend on NFSv2, enable it on the server
                in <code class="filename">/etc/sysconfig/nfs</code> by setting:
              </p><div class="verbatim-wrap"><pre class="screen">NFSD_OPTIONS="-V2"
MOUNTD_OPTIONS="-V2"</pre></div><p>
                After restarting the service, check whether version 2 is
                available with the command:
              </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /proc/fs/nfsd/versions
+2 +3 +4 +4.1 +4.2</pre></div></div></dd><dt id="var-nfs-export-manual-idmapd"><span class="term"><code class="filename">/etc/idmapd.conf</code></span></dt><dd><p>
              The <code class="systemitem">idmapd</code> daemon is only
              required if Kerberos authentication is used or if clients cannot
              work with numeric user names. Linux clients can work with numeric
              user names since Linux kernel 2.6.39. The
              <code class="systemitem">idmapd</code> daemon does
              the name-to-ID mapping for NFSv4 requests to the server and
              replies to the client.
            </p><p>
              If required, <code class="systemitem">idmapd</code> needs
              to run on the NFSv4 server. Name-to-ID mapping on the client will
              be done by <code class="command">nfsidmap</code> provided by the package
              <span class="package">nfs-client</span>.
            </p><p>
              Make sure that there is a uniform way in which user names and IDs
              (UIDs) are assigned to users across machines that might be
              sharing file systems using NFS. This can be achieved by using
              NIS, LDAP, or any uniform domain authentication mechanism in your
              domain.
            </p><p>
              The parameter <code class="literal">Domain</code> must be set in
              <code class="filename">/etc/idmapd.conf</code>. It must be the same for
              the server and all NFSv4 clients that access this server. Clients
              in a different NFSv4 domain cannot access the server. Sticking
              with the default domain <code class="literal">localdomain</code> is
              recommended. If you need to choose a different name, you may want
              to go with the FQDN of the host, minus the host name. A sample
              configuration file looks like the following:
            </p><div class="verbatim-wrap"><pre class="screen">[General]
Verbosity = 0
Pipefs-Directory = /var/lib/nfs/rpc_pipefs
Domain = localdomain

[Mapping]
Nobody-User = nobody
Nobody-Group = nobody</pre></div><p>
              To start the <code class="systemitem">idmapd</code>
              daemon, run <code class="command">systemctl start nfs-idmapd</code>. In
              case you have modified <code class="filename">/etc/idmapd.conf</code>
              while the daemon was running, you need to restart it for the
              changes to become active: <code class="command">systemctl restart
              nfs-idmapd</code>.
            </p><p>
              For more information, see the man pages of
              <code class="literal">idmapd</code> and <code class="literal">idmapd.conf</code>
              (<code class="literal">man idmapd</code> and <code class="literal">man
              idmapd.conf</code>).
            </p></dd></dl></div></section><section class="sect2" id="sec-nfs-kerberos" data-id-title="NFS with Kerberos"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.3.3 </span><span class="title-name">NFS with Kerberos</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-kerberos">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To use Kerberos authentication for NFS, Generic Security Services (GSS)
        must be enabled. Select <span class="guimenu">Enable GSS Security</span> in the
        initial YaST NFS Server dialog. You must have a working Kerberos server
        to use this feature. YaST does not set up the server but only uses
        the provided functionality. To use Kerberos authentication in addition to
        the YaST configuration, complete at least the following steps before
        running the NFS configuration:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Make sure that both the server and the client are in the same Kerberos
            domain. They must access the same KDC (Key Distribution Center)
            server and share their <code class="filename">krb5.keytab</code> file (the
            default location on any machine is
            <code class="filename">/etc/krb5.keytab</code>). For more information about
            Kerberos, see <span class="intraxref">Book “Security and Hardening Guide”, Chapter 6 “Network authentication with Kerberos”</span>.
          </p></li><li class="step"><p>
            Start the gssd service on the client with <code class="command">systemctl start
            rpc-gssd.service</code>.
          </p></li><li class="step"><p>
            Start the svcgssd service on the server with <code class="command">systemctl
            start rpc-svcgssd.service</code>.
          </p></li></ol></div></div><p>
        Kerberos authentication also requires the
        <code class="systemitem">idmapd</code> daemon to run on the
        server. For more information, refer to
        <a class="xref" href="#var-nfs-export-manual-idmapd"><code class="filename">/etc/idmapd.conf</code></a>.
      </p><p>
        For more information about configuring kerberized NFS, refer to the
        links in <a class="xref" href="#sec-nfs-info" title="19.7. More information">Section 19.7, “More information”</a>.
      </p></section></section><section class="sect1" id="sec-nfs-configuring-nfs-clients" data-id-title="Configuring clients"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.4 </span><span class="title-name">Configuring clients</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-configuring-nfs-clients">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      To configure your host as an NFS client, you do not need to install
      additional software. All needed packages are installed by default.
    </p><section class="sect2" id="sec-nfs-import-yast2" data-id-title="Importing file systems with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.1 </span><span class="title-name">Importing file systems with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-import-yast2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Authorized users can mount NFS directories from an NFS server into the
        local file tree using the YaST NFS client module. Proceed as follows:
      </p><div class="procedure" id="pro-nfs-import-yast2" data-id-title="Importing NFS directories"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.2: </span><span class="title-name">Importing NFS directories </span></span><a title="Permalink" class="permalink" href="#pro-nfs-import-yast2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Start the YaST NFS client module.
          </p></li><li class="step"><p>
            Click <span class="guimenu">Add</span> in the <span class="guimenu">NFS Shares</span>
            tab. Enter the host name of the NFS server, the directory to
            import, and the mount point at which to mount this directory
            locally.
          </p></li><li class="step"><p>
            When using NFSv4, select <span class="guimenu">Enable NFSv4</span> in the
            <span class="guimenu">NFS Settings</span> tab. Additionally, the
            <span class="guimenu">NFSv4 Domain Name</span> must contain the same value as
            used by the NFSv4 server. The default domain is
            <code class="literal">localdomain</code>.
          </p></li><li class="step"><p>
            To use Kerberos authentication for NFS, GSS security must be enabled.
            Select <span class="guimenu">Enable GSS Security</span>.
          </p></li><li class="step"><p>
            If <code class="systemitem">firewalld</code> is active on your system, configure it separately
            for NFS (see <a class="xref" href="#sec-nfs-firewall" title="19.5. Operating an NFS server and clients behind a firewall">Section 19.5, “Operating an NFS server and clients behind a firewall”</a>).
            YaST does not yet have complete support for <code class="systemitem">firewalld</code>, so
            ignore the <span class="quote">“<span class="quote">Firewall not configurable</span>”</span> message and continue.
          </p></li><li class="step"><p>
            Click <span class="guimenu">OK</span> to save your changes.
          </p></li></ol></div></div><p>
        The configuration is written to <code class="filename">/etc/fstab</code> and the
        specified file systems are mounted. When you start the YaST
        configuration client at a later time, it also reads the existing
        configuration from this file.
      </p><div id="id-1.11.6.7.6.3.5" data-id-title="NFS as a root file system" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: NFS as a root file system</div><p>
          On (diskless) systems where the root partition is mounted via network
          as an NFS share, you need to be careful when configuring the network
          device with which the NFS share is accessible.
        </p><p>
          When shutting down or rebooting the system, the default processing
          order is to turn off network connections then unmount the root
          partition. With NFS root, this order causes problems as the root
          partition cannot be cleanly unmounted as the network connection to
          the NFS share is already deactivated. To prevent the system from
          deactivating the relevant network device, open the network device
          configuration tab as described in
          <span class="intraxref">Book “Administration Guide”, Chapter 23 “Basic networking”, Section 23.4.1.2.5 “Activating the network device”</span> and choose
          <span class="guimenu">On NFSroot</span> in the <span class="guimenu">Device
          Activation</span> pane.
        </p></div></section><section class="sect2" id="sec-nfs-import" data-id-title="Importing file systems manually"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.2 </span><span class="title-name">Importing file systems manually</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-import">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The prerequisite for importing file systems manually from an NFS server
        is a running RPC port mapper. The <code class="option">nfs</code> service takes
        care to start it properly; thus, start it by entering
        <code class="command">systemctl start nfs</code> as
        <code class="systemitem">root</code>. Then remote file
        systems can be mounted in the file system just like local partitions,
        using the <code class="command">mount</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount <em class="replaceable">HOST</em>:<em class="replaceable">REMOTE-PATH</em> <em class="replaceable">LOCAL-PATH</em></pre></div><p>
        To import user directories from the <code class="systemitem">nfs.example.com</code>
        machine, for example, use:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount nfs.example.com:/home /home</pre></div><p>
        To define a count of TCP connections that the clients make to the NFS
        server, you can use the <code class="literal">nconnect</code> option of the
        <code class="command">mount</code> command. You can specify any number between 1
        and 16, where 1 is the default value if the mount option has not been
        specified.
      </p><p>
        The <code class="literal">nconnect</code> setting is applied only during the
        first mount process to the particular NFS server. If the same client
        executes the mount command to the same NFS server, all already
        established connections will be shared—no new connection will be
        established. To change the <code class="literal">nconnect</code> setting, you
        have to unmount <span class="bold"><strong>all</strong></span> client connections
        to the particular NFS server. Then you can define a new value for the
        <code class="literal">nconnect</code> option.
      </p><p>
        You can find the value of <code class="literal">nconnect</code> that is in
        currently in effect in the output of the <code class="command">mount</code>, or
        in the file <code class="filename">/proc/mounts</code>. If there is no value for
        the mount option, then the option has not been used during mounting and
        the default value of <span class="emphasis"><em>1</em></span> is in use.
      </p><div id="id-1.11.6.7.6.4.9" data-id-title="Different number of connections than defined by nconnect" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Different number of connections than defined by <code class="literal">nconnect</code></div><p>
          As you can close and open connections after the first mount, the
          actual count of connections does not necessarily have to be the same
          as the value of <code class="literal">nconnect</code>.
        </p></div><section class="sect3" id="sec-nfs-automount" data-id-title="Using the automount service"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">19.4.2.1 </span><span class="title-name">Using the automount service</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-automount">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The autofs daemon can be used to mount remote file systems
          automatically. Add the following entry to the
          <code class="filename">/etc/auto.master</code> file:
        </p><div class="verbatim-wrap"><pre class="screen">/nfsmounts /etc/auto.nfs</pre></div><p>
          Now the <code class="filename">/nfsmounts</code> directory acts as the root
          for all the NFS mounts on the client if the
          <code class="filename">auto.nfs</code> file is filled appropriately. The name
          <code class="filename">auto.nfs</code> is chosen for the sake of
          convenience—you can choose any name. In
          <code class="filename">auto.nfs</code> add entries for all the NFS mounts as
          follows:
        </p><div class="verbatim-wrap"><pre class="screen">localdata -fstype=nfs server1:/data
nfs4mount -fstype=nfs4 server2:/</pre></div><p>
          Activate the settings with <code class="command">systemctl start autofs</code>
          as <code class="systemitem">root</code>. In this example,
          <code class="filename">/nfsmounts/localdata</code>, the
          <code class="filename">/data</code> directory of
          <code class="systemitem">server1</code>, is mounted with NFS and
          <code class="filename">/nfsmounts/nfs4mount</code> from
          <code class="systemitem">server2</code> is mounted with NFSv4.
        </p><p>
          If the <code class="filename">/etc/auto.master</code> file is edited while the
          service autofs is running, the automounter must be restarted for the
          changes to take effect with <code class="command">systemctl restart
          autofs</code>.
        </p></section><section class="sect3" id="sec-nfs-fstab" data-id-title="Manually editing /etc/fstab"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">19.4.2.2 </span><span class="title-name">Manually editing <code class="filename">/etc/fstab</code></span></span> <a title="Permalink" class="permalink" href="#sec-nfs-fstab">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          A typical NFSv3 mount entry in <code class="filename">/etc/fstab</code> looks
          like this:
        </p><div class="verbatim-wrap"><pre class="screen">nfs.example.com:/data /local/path nfs rw,noauto 0 0</pre></div><p>
          For NFSv4 mounts, use <code class="literal">nfs4</code> instead of
          <code class="literal">nfs</code> in the third column:
        </p><div class="verbatim-wrap"><pre class="screen">nfs.example.com:/data /local/pathv4 nfs4 rw,noauto 0 0</pre></div><p>
          The <code class="literal">noauto</code> option prevents the file system from
          being mounted automatically at start-up. If you want to mount the
          respective file system manually, it is possible to shorten the mount
          command specifying the mount point only:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount /local/path</pre></div><div id="id-1.11.6.7.6.4.11.8" data-id-title="Mounting at start-up" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Mounting at start-up</div><p>
            If you do not enter the <code class="literal">noauto</code> option, the init
            scripts of the system will handle the mount of those file systems
            at start-up. In that case, you may consider adding the option
            <code class="option">_netdev</code>, which prevents scripts from trying to
            mount the share before the network is available.
         </p></div></section></section><section class="sect2" id="sec-nfs-pnfs" data-id-title="Parallel NFS (pNFS)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.3 </span><span class="title-name">Parallel NFS (pNFS)</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-pnfs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        NFS is one of the oldest protocols, developed in the 1980s. As such,
        NFS is usually sufficient if you want to share small files. However,
        when you want to transfer big files or many clients want to access
        data, an NFS server becomes a bottleneck and has a significant impact
        on the system performance. This is because files are quickly getting
        bigger, whereas the relative speed of Ethernet has not fully kept pace.
      </p><p>
        When you request a file from a regular NFS server, the server looks up
        the file metadata, collects all the data, and transfers it over the
        network to your client. However, the performance bottleneck becomes
        apparent no matter how small or big the files are:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            With small files, most of the time is spent collecting the
            metadata.
          </p></li><li class="listitem"><p>
            With big files, most of the time is spent on transferring the data
            from server to client.
          </p></li></ul></div><p>
        pNFS, or parallel NFS, overcomes this limitation as it separates the
        file system metadata from the location of the data. As such, pNFS
        requires two types of servers:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            A <span class="emphasis"><em>metadata</em></span> or <span class="emphasis"><em>control
            server</em></span> that handles all the non-data traffic
          </p></li><li class="listitem"><p>
            One or more <span class="emphasis"><em>storage server(s)</em></span> that hold(s) the
            data
          </p></li></ul></div><p>
        The metadata and the storage servers form a single, logical NFS server.
        When a client wants to read or write, the metadata server tells the
        NFSv4 client which storage server to use to access the file chunks. The
        client can access the data directly on the server.
      </p><p>
        <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> supports pNFS on the client side only.
      </p><section class="sect3" id="sec-nfs-pnfs-yast" data-id-title="Configuring pNFS client with YaST"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">19.4.3.1 </span><span class="title-name">Configuring pNFS client with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-pnfs-yast">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Proceed as described in <a class="xref" href="#pro-nfs-import-yast2" title="Importing NFS directories">Procedure 19.2, “Importing NFS directories”</a>, but
          click the <span class="guimenu">pNFS (v4.2)</span> check box and optionally
          <span class="guimenu">NFSv4 share</span>. YaST will do all the necessary
          steps and will write all the required options in the file
          <code class="filename">/etc/exports</code>.
        </p></section><section class="sect3" id="sec-nfs-pnfs-manual" data-id-title="Configuring pNFS client manually"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">19.4.3.2 </span><span class="title-name">Configuring pNFS client manually</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-pnfs-manual">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Refer to <a class="xref" href="#sec-nfs-import" title="19.4.2. Importing file systems manually">Section 19.4.2, “Importing file systems manually”</a> to start. Most of the
          configuration is done by the NFSv4 server. For pNFS, the only
          difference is to add the <code class="option">nfsvers</code> option and the
          metadata server <em class="replaceable">MDS_SERVER</em> to your
          <code class="command">mount</code> command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> mount -t nfs4 -o nfsvers=4.2 <em class="replaceable">MDS_SERVER</em> <em class="replaceable">MOUNTPOINT</em></pre></div><p>
          To help with debugging, change the value in the
          <code class="filename">/proc</code> file system:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> echo 32767 &gt; /proc/sys/sunrpc/nfsd_debug
<code class="prompt user">&gt; </code><code class="command">sudo</code> echo 32767 &gt; /proc/sys/sunrpc/nfs_debug</pre></div></section></section></section><section class="sect1" id="sec-nfs-firewall" data-id-title="Operating an NFS server and clients behind a firewall"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.5 </span><span class="title-name">Operating an NFS server and clients behind a firewall</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-firewall">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Communication between an NFS server and its clients happens via Remote
      Procedure Calls (RPC). Several RPC services, such as the mount daemon or the
      file locking service, are part of the Linux NFS implementation. If
      the server and the clients run behind a firewall, these services and the
      firewall(s) need to be configured to not block the client-server
      communication.
    </p><p>
      An NFS 4 server is backwards-compatible with NFS version 3, and firewall
      configurations vary for both versions. If any of your clients use
      NFS 3 to mount shares, configure your firewall to allow
      both NFS 4 and NFS 3.
    </p><section class="sect2" id="nfs-firewall-nfsv4" data-id-title="NFS 4.x"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.5.1 </span><span class="title-name">NFS 4.<em class="replaceable">x</em></span></span> <a title="Permalink" class="permalink" href="#nfs-firewall-nfsv4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        NFS 4 requires TCP port 2049 to be open on the server side only. To
        open this port on the firewall, enable the <code class="literal">nfs</code>
        service in firewalld <span class="emphasis"><em>on the NFS server</em></span>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> firewall-cmd --permanent --add-service=nfs --zone=<em class="replaceable">ACTIVE_ZONE</em>
firewall-cmd --reload</pre></div><p>
        Replace <em class="replaceable">ACTIVE_ZONE</em> with the firewall zone
        used on the NFS server.
      </p><p>
        No additional firewall configuration on the client side is needed when
        using NFSv4. By default mount defaults to the highest supported
        NFS version, so if your client supports NFSv4, shares will
        automatically be mounted as version 4.2.
      </p></section><section class="sect2" id="nfs-firewall-nfsv3" data-id-title="NFS 3"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.5.2 </span><span class="title-name">NFS 3</span></span> <a title="Permalink" class="permalink" href="#nfs-firewall-nfsv3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        NFS 3 requires the following services:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="systemitem">portmapper</code></p></li><li class="listitem"><p><code class="systemitem">nfsd</code></p></li><li class="listitem"><p><code class="systemitem">mountd</code></p></li><li class="listitem"><p><code class="systemitem">lockd</code></p></li><li class="listitem"><p><code class="systemitem">statd</code></p></li></ul></div><p>
        These services are operated by <code class="systemitem">rpcbind</code>, which,
        by default, dynamically assigns ports. To allow access to these
        services behind a firewall, they need to be configured to run on a
        static port first. These ports need to be opened in the firewall(s) afterwards.
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.7.7.5.5.1"><span class="term"><code class="systemitem">portmapper</code></span></dt><dd><p>
              On <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, <code class="systemitem">portmapper</code> is already configured to
              run on a static port.
            </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Port</p></td><td style="border-bottom: 1px solid ; "><p>111</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Protocol(s)</p></td><td style="border-bottom: 1px solid ; "><p>TCP, UDP</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Runs on</p></td><td style="border-bottom: 1px solid ; "><p>Client, Server</p></td></tr><tr><td colspan="2">
                       <div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> firewall-cmd --add-service=rpc-bind --permanent --zone=<em class="replaceable">ACTIVE_ZONE</em></pre></div>
                     </td></tr></tbody></table></div></dd><dt id="id-1.11.6.7.7.5.5.2"><span class="term"><code class="systemitem">nfsd</code></span></dt><dd><p>
              On <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, <code class="systemitem">nfsd</code> is already configured to
              run on a static port.
            </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Port</p></td><td style="border-bottom: 1px solid ; "><p>2049</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Protocol(s)</p></td><td style="border-bottom: 1px solid ; "><p>TCP, UDP</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Runs on</p></td><td style="border-bottom: 1px solid ; "><p>Server</p></td></tr><tr><td colspan="2">
                       <div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> firewall-cmd --add-service=nfs3 --permanent --zone=<em class="replaceable">ACTIVE_ZONE</em></pre></div>
                     </td></tr></tbody></table></div></dd><dt id="id-1.11.6.7.7.5.5.3"><span class="term"><code class="systemitem">mountd</code></span></dt><dd><p>
               On <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, <code class="systemitem">mountd</code> is already configured to
              run on a static port.
            </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Port</p></td><td style="border-bottom: 1px solid ; "><p><em class="replaceable">20048</em></p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Protocol(s)</p></td><td style="border-bottom: 1px solid ; "><p>TCP, UDP</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Runs on</p></td><td style="border-bottom: 1px solid ; "><p>Server</p></td></tr><tr><td colspan="2">
                       <div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> firewall-cmd --add-service=mountd --permanent --zone=<em class="replaceable">ACTIVE_ZONE</em></pre></div>
                     </td></tr></tbody></table></div></dd><dt id="id-1.11.6.7.7.5.5.4"><span class="term"><code class="systemitem">lockd</code></span></dt><dd><p>
               To set a static port for <code class="systemitem">lockd</code>:
            </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
                  Edit<code class="filename">/etc/sysconfig/nfs</code> on the server and
                  find and set
                </p><div class="verbatim-wrap"><pre class="screen">LOCKD_TCPPORT=<em class="replaceable">NNNNN</em>
LOCKD_UDPPORT=<em class="replaceable">NNNN</em></pre></div><p>
                  Replace <em class="replaceable">NNNNN</em> with an unused port of
                  your choice. Use the same port for both protocols.
                </p></li><li class="step"><p>
                  Restart the NFS server:
                </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart nfs-server</pre></div></li></ol></div></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Port</p></td><td style="border-bottom: 1px solid ; "><p><em class="replaceable">NNNNN</em></p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Protocol(s)</p></td><td style="border-bottom: 1px solid ; "><p>TCP, UDP</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Runs on</p></td><td style="border-bottom: 1px solid ; "><p>Client, Server</p></td></tr><tr><td colspan="2">
                       <div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> firewall-cmd --add-port=<em class="replaceable">NNNNN</em>/{tcp,udp} --permanent --zone=<em class="replaceable">ACTIVE_ZONE</em></pre></div>
                     </td></tr></tbody></table></div></dd><dt id="id-1.11.6.7.7.5.5.5"><span class="term"><code class="systemitem">statd</code></span></dt><dd><p>
               To set a static port for <code class="systemitem">statd</code>:
            </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
                  Edit<code class="filename">/etc/sysconfig/nfs</code> on the server and
                  find and set
                </p><div class="verbatim-wrap"><pre class="screen">STATD_PORT=<em class="replaceable">NNNNN</em></pre></div><p>
                  Replace <em class="replaceable">NNNNN</em> with an unused port of
                  your choice.
                </p></li><li class="step"><p>
                  Restart the NFS server:
                </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart nfs-server</pre></div></li></ol></div></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Port</p></td><td style="border-bottom: 1px solid ; "><p><em class="replaceable">NNNNN</em></p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Protocol(s)</p></td><td style="border-bottom: 1px solid ; "><p>TCP, UDP</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Runs on</p></td><td style="border-bottom: 1px solid ; "><p>Client, Server</p></td></tr><tr><td colspan="2">
                       <div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> firewall-cmd --add-port=<em class="replaceable">NNNNN</em>/{tcp,udp} --permanent --zone=<em class="replaceable">ACTIVE_ZONE</em></pre></div>
                     </td></tr></tbody></table></div></dd></dl></div><div id="id-1.11.6.7.7.5.6" data-id-title="Loading a changed firewalld configuration" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Loading a changed <code class="systemitem">firewalld</code> configuration</div><p>
          Whenever you change the <code class="systemitem">firewalld</code> configuration, you need to reload
          the daemon to activate the changes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> firewall-cmd --reload</pre></div></div><div id="id-1.11.6.7.7.5.7" data-id-title="Firewall zone" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Firewall zone</div><p>
          Make sure to replace <em class="replaceable">ACTIVE_ZONE</em> with the firewall zone
          used on the respective machine. Note that, depending on the firewall
          configuration, the active zone can differ from machine to machine.
        </p></div></section></section><section class="sect1" id="nfs4-acls" data-id-title="Managing Access Control Lists over NFSv4"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.6 </span><span class="title-name">Managing Access Control Lists over NFSv4</span></span> <a title="Permalink" class="permalink" href="#nfs4-acls">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      There is no single standard for Access Control Lists (ACLs) in Linux
      beyond the simple read, write, and execute (<code class="literal">rwx</code>) flags
      for user, group, and others (<code class="literal">ugo</code>). One option for
      finer control is the <em class="citetitle">Draft POSIX ACLs</em>, which were
      never formally standardized by POSIX. Another is the NFSv4 ACLs, which
      were designed to be part of the NFSv4 network file system with the goal
      of making something that provided reasonable compatibility between POSIX
      systems on Linux and WIN32 systems on Microsoft Windows.
    </p><p>
      NFSv4 ACLs are not sufficient to correctly implement Draft POSIX ACLs so
      no attempt has been made to map ACL accesses on an NFSv4 client (such as
      using <code class="command">setfacl</code>).
    </p><p>
      When using NFSv4, Draft POSIX ACLs cannot be used even in emulation and
      NFSv4 ACLs need to be used directly; that means while
      <code class="command">setfacl</code> can work on NFSv3, it cannot work on NFSv4. To
      allow NFSv4 ACLs to be used on an NFSv4 file system, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>
      provides the <code class="filename">nfs4-acl-tools</code> package, which contains
      the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="command">nfs4-getfacl</code>
        </p></li><li class="listitem"><p>
          <code class="command">nfs4-setfacl</code>
        </p></li><li class="listitem"><p>
          <code class="command">nfs4-editacl</code>
        </p></li></ul></div><p>
      These operate in a generally similar way to <code class="command">getfacl</code>
      and <code class="command">setfacl</code> for examining and modifying NFSv4 ACLs.
      These commands are effective only if the file system on the NFS server
      provides full support for NFSv4 ACLs. Any limitation imposed by the
      server will affect programs running on the client in that some particular
      combinations of Access Control Entries (ACEs) might not be possible.
    </p><p>
      It is not supported to mount NFS volumes locally on the exporting NFS
      server.
    </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.11.6.7.8.8"><span class="name">Additional Information</span><a title="Permalink" class="permalink" href="#id-1.11.6.7.8.8">#</a></h2></div><p>
      For information, see <em class="citetitle">Introduction to NFSv4 ACLs</em> at
      <a class="link" href="https://wiki.linux-nfs.org/wiki/index.php/ACLs#Introduction_to_NFSv4_ACLs" target="_blank">https://wiki.linux-nfs.org/wiki/index.php/ACLs#Introduction_to_NFSv4_ACLs</a>.
    </p></section><section class="sect1" id="sec-nfs-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.7 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      In addition to the man pages of <code class="command">exports</code>,
      <code class="command">nfs</code>, and <code class="command">mount</code>, information about
      configuring an NFS server and client is available in
      <code class="filename">/usr/share/doc/packages/nfsidmap/README</code>. For further
      documentation online, refer to the following Web sites:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          For general information about network security, refer to
          <span class="intraxref">Book “Security and Hardening Guide”, Chapter 23 “Masquerading and firewalls”</span>.
        </p></li><li class="listitem"><p>
          Refer to <a class="xref" href="#sec-autofs-nfs" title="21.4. Auto-mounting an NFS share">Section 21.4, “Auto-mounting an NFS share”</a> if you need to
          automatically mount NFS exports.
        </p></li><li class="listitem"><p>
          For more details about configuring NFS by using AutoYaST, refer to
          <span class="intraxref">Book “AutoYaST Guide”, Chapter 4 “Configuration and installation options”, Section 4.21 “NFS client and server”</span>.
        </p></li><li class="listitem"><p>
          For instructions about securing NFS exports with Kerberos, refer to
          <span class="intraxref">Book “Security and Hardening Guide”, Chapter 6 “Network authentication with Kerberos”, Section 6.6 “Kerberos and NFS”</span>.
        </p></li><li class="listitem"><p>
          Find the detailed technical documentation online at
          <a class="link" href="https://nfs.sourceforge.net/" target="_blank">SourceForge</a>.
        </p></li></ul></div></section><section class="sect1" id="sec-nfs-troubleshooting" data-id-title="Gathering information for NFS troubleshooting"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.8 </span><span class="title-name">Gathering information for NFS troubleshooting</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-troubleshooting">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-nfs-common-troubleshooting" data-id-title="Common troubleshooting"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.8.1 </span><span class="title-name">Common troubleshooting</span></span> <a title="Permalink" class="permalink" href="#sec-nfs-common-troubleshooting">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        In some cases, you can understand the problem in your NFS by reading
        the error messages produced and looking into the
        <code class="filename">/var/log/messages</code> file. However, in many cases,
        the information provided by the error messages and in
        <code class="filename">/var/log/messages</code> is not detailed enough. In these
        cases, most NFS problems can be best understood through capturing
        network packets while reproducing the problem.
      </p><p>
        Clearly define the problem. Examine the problem by testing the system
        in a variety of ways and determining when the problem occurs. Isolate
        the simplest steps that lead to the problem. Then try to reproduce the
        problem as described in the procedure below.
      </p><div class="procedure" id="id-1.11.6.7.10.2.4" data-id-title="Reproducing the problem"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.3: </span><span class="title-name">Reproducing the problem </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.7.10.2.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Capture network packets. On Linux, you can use the
            <code class="command">tcpdump</code> command, which is supplied by the
            <span class="package">tcpdump</span> package.
          </p><p>
            An example of <code class="command">tcpdump</code> syntax follows:
          </p><div class="verbatim-wrap"><pre class="screen">tcpdump -s0 -i <em class="replaceable">eth0</em> -w <em class="replaceable">/tmp/nfs-demo.cap</em> host <em class="replaceable">x.x.x.x</em></pre></div><p>
            Where:
          </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.7.10.2.4.2.5.1"><span class="term">s0</span></dt><dd><p>
                  Prevents packet truncation
                </p></dd><dt id="id-1.11.6.7.10.2.4.2.5.2"><span class="term">eth0</span></dt><dd><p>
                  Should be replaced with the name of the local interface which
                  the packets will pass through. You can use the
                  <code class="literal">any</code> value to capture all interfaces at the
                  same time, but usage of this attribute often results in
                  inferior data as well as confusion in analysis.
                </p></dd><dt id="id-1.11.6.7.10.2.4.2.5.3"><span class="term">w</span></dt><dd><p>
                  Designates the name of the capture file to write.
                </p></dd><dt id="id-1.11.6.7.10.2.4.2.5.4"><span class="term">x.x.x.x</span></dt><dd><p>
                  Should be replaced with the IP address of the other end of
                  the NFS connection. For example, when taking a
                  <code class="command">tcpdump</code> at the NFS client side, specify
                  the IP address of the NFS Server, and vice versa.
                </p></dd></dl></div><div id="id-1.11.6.7.10.2.4.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
              In some cases, capturing the data at either the NFS client or NFS
              server is sufficient. However, in cases where end-to-end network
              integrity is in doubt, it is often necessary to capture data at
              both ends.
            </p></div><p>
            Do not shut down the <code class="command">tcpdump</code> process and proceed
            to the next step.
          </p></li><li class="step"><p>
            (Optional) If the problem occurs during execution of the
            <code class="command">nfs mount</code> command itself, you can try to use the
            high-verbosity option (<code class="literal">-vvv</code>) of the <code class="command">nfs
            mount</code> command to get more output.
          </p></li><li class="step"><p>
            (Optional) Get an <code class="command">strace</code> of the reproduction
            method. An <code class="command">strace</code> of reproduction steps records
            exactly what system calls were made at exactly what time. This
            information can be used to further determine on which events in the
            <code class="literal">tcpdump</code> you should focus.
          </p><p>
            For example, if you found out that executing the command
            <span class="emphasis"><em>mycommand --param</em></span> was failing on an NFS mount,
            then you could <code class="command">strace</code> the command with:
          </p><div class="verbatim-wrap"><pre class="screen">strace -ttf -s128 -o/tmp/nfs-strace.out mycommand --param</pre></div><p>
            In case you do not get any <code class="command">strace</code> of the
            reproduction step, note the time when the problem was reproduced.
            Check the <code class="filename">/var/log/messages</code> log file to
            isolate the problem.
          </p></li><li class="step"><p>
            Once the problem has been reproduced, stop
            <code class="command">tcpdump</code> running in your terminal by pressing
            <span class="keycap">CTRL</span><span class="key-connector">–</span><span class="keycap">c</span>. If
            the <code class="command">strace</code> command resulted in a hang, also
            terminate the <code class="command">strace</code> command.
          </p></li><li class="step"><p>
            An administrator with experience in analyzing packet traces and
            <code class="command">strace</code> data can now inspect data in
            <code class="filename">/tmp/nfs-demo.cap</code> and
            <code class="filename">/tmp/nfs-strace.out</code>.
          </p></li></ol></div></div></section><section class="sect2" id="sec-advance-debugging" data-id-title="Advanced NFS debugging"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.8.2 </span><span class="title-name">Advanced NFS debugging</span></span> <a title="Permalink" class="permalink" href="#sec-advance-debugging">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.11.6.7.10.3.2" data-id-title="Advanced debugging is for experts" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Advanced debugging is for experts</div><p>
          Please bear in mind that the following section is intended only for
          skilled NFS administrators who understand the NFS code. Therefore,
          perform the first steps described in
          <a class="xref" href="#sec-nfs-common-troubleshooting" title="19.8.1. Common troubleshooting">Section 19.8.1, “Common troubleshooting”</a> to help narrow down
          the problem and to inform an expert about which areas of debug code
          (if any) might be needed to learn deeper details.
        </p></div><p>
        There are various areas of debug code that can be enabled to gather
        additional NFS-related information. However, the debug messages are
        quite cryptic and the volume of them can be so large that the use of
        debug code can affect system performance. It may even impact the system
        enough to prevent the problem from occurring. In the majority of cases,
        the debug code output is not needed, nor is it typically useful to
        anyone who is not highly familiar with the NFS code.
      </p><section class="sect3" id="sec-rpcdebug" data-id-title="Activating debugging with rpcdebug"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">19.8.2.1 </span><span class="title-name">Activating debugging with <code class="command">rpcdebug</code></span></span> <a title="Permalink" class="permalink" href="#sec-rpcdebug">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The <code class="command">rpcdebug</code> tool allows you to set and clear NFS client and server
          debug flags. In case the <code class="command">rpcdebug</code> tool is not available in your
          <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> installation, you can install it from the package
          <span class="package">nfs-client</span> or <span class="package">nfs-kernel-server</span> for the NFS server.
        </p><p>
          To set debug flags, run:
        </p><div class="verbatim-wrap"><pre class="screen">rpcdebug -m <em class="replaceable">module</em> -s flags</pre></div><p>
          To clear the debug flags, run:
        </p><div class="verbatim-wrap"><pre class="screen">rpcdebug -m <em class="replaceable">module</em> -c flags</pre></div><p>
          where <em class="replaceable">module</em> can be:
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.7.10.3.4.8.1"><span class="term">nfsd</span></dt><dd><p>
                Debug for the NFS server code
              </p></dd><dt id="id-1.11.6.7.10.3.4.8.2"><span class="term">nfs</span></dt><dd><p>
                Debug for the NFS client code
              </p></dd><dt id="id-1.11.6.7.10.3.4.8.3"><span class="term">nlm</span></dt><dd><p>
                Debug for the NFS Lock Manager, at either the NFS client or NFS
                server. This only applies to NFS v2/v3.
              </p></dd><dt id="id-1.11.6.7.10.3.4.8.4"><span class="term">rpc</span></dt><dd><p>
                Debug for the Remote Procedure Call module, at either the NFS
                client or NFS server.
              </p></dd></dl></div><p>
          For information on detailed usage of the <code class="command">rpcdebug</code>
          command, refer to the manual page:
        </p><div class="verbatim-wrap"><pre class="screen">man 8 rpcdebug</pre></div></section><section class="sect3" id="sec-other-nfs-debug" data-id-title="Activating debug for other code upon which NFS depends"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">19.8.2.2 </span><span class="title-name">Activating debug for other code upon which NFS depends</span></span> <a title="Permalink" class="permalink" href="#sec-other-nfs-debug">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/storage_nfs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          NFS activities may depend on other related services, such as the NFS
          mount daemon—<code class="command">rpc.mountd</code>. You can set options
          for related services within <code class="filename">/etc/sysconfig/nfs</code>.
        </p><p>
          For example, <code class="filename">/etc/sysconfig/nfs</code> contains the
          parameter:
        </p><div class="verbatim-wrap"><pre class="screen">MOUNTD_OPTIONS=""</pre></div><p>
          To enable the debug mode, you have to use the <code class="literal">-d</code>
          option followed by any of the values: <code class="literal">all</code>,
          <code class="literal">auth</code>, <code class="literal">call</code>,
          <code class="literal">general</code>, or <code class="literal">parse</code>.
        </p><p>
          For example, the following code enables all forms of
          <code class="command">rpc.mountd</code> logging:
        </p><div class="verbatim-wrap"><pre class="screen">MOUNTD_OPTIONS="-d all"</pre></div><p>
          For all available options refer to the manual pages:
        </p><div class="verbatim-wrap"><pre class="screen">man 8 rpc.mountd</pre></div><p>
          After changing <code class="filename">/etc/sysconfig/nfs</code>, services need
          to be restarted:
        </p><div class="verbatim-wrap"><pre class="screen">systemctl restart nfs-server  # for nfs server related changes
systemctl restart nfs  # for nfs client related changes</pre></div></section></section></section></section><section class="chapter" id="cha-samba" data-id-title="Samba"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">20 </span><span class="title-name">Samba</span></span> <a title="Permalink" class="permalink" href="#cha-samba">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
        Using Samba, a Unix machine can be configured as a file and print
        server for macOS, Windows, and OS/2 machines. Samba has developed into
        a fully fledged and rather complex product. Configure Samba with
        YaST, or by editing the configuration file manually.
      </p></div></div></div></div><div id="id-1.11.6.8.3" data-id-title="SMB1 is unsupported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: SMB1 is unsupported</div><p>
      Starting with Samba version 4.17, the SMB1 protocol has been disabled in
      <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span><span class="phrase"> and is no longer
      supported</span>.
    </p></div><section class="sect1" id="sec-samba-term" data-id-title="Terminology"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.1 </span><span class="title-name">Terminology</span></span> <a title="Permalink" class="permalink" href="#sec-samba-term">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      The following are some terms used in Samba documentation and in the
      YaST module.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.8.4.3.1"><span class="term">SMB protocol</span></dt><dd><p>
            Samba uses the SMB (server message block) protocol, which is based
            on <span class="productname">NetBIOS</span> services. Microsoft
            released the protocol so that software from other manufacturers
            could establish connections to servers running Microsoft
            operating systems. Samba implements the SMB protocol on top of the
            TCP/IP protocol, which means that TCP/IP must be installed and
            enabled on all clients.
          </p><div id="id-1.11.6.8.4.3.1.2.2" data-id-title="IBM Z: NetBIOS support" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: IBM Z: NetBIOS support</div><p>
              IBM Z merely supports SMB over TCP/IP. NetBIOS support is not
              available on these systems.
            </p></div></dd><dt id="id-1.11.6.8.4.3.2"><span class="term">CIFS protocol</span></dt><dd><p>
            The CIFS (Common Internet File System) protocol is an early version
            of the SMB protocol, also known as SMB1. CIFS defines a standard
            remote file system access protocol for use over TCP/IP, enabling
            groups of users to work together and share documents across the
            Internet.
          </p><p>
            SMB1 was superseded by SMB2, first released as part of Microsoft
            Windows Vista™. This was in turn superseded by SMB3 in
            Microsoft Windows 8™ and Microsoft Windows Server 2012. In
            recent versions of Samba, SMB1 is disabled by default for security
            reasons.
          </p></dd><dt id="id-1.11.6.8.4.3.3"><span class="term">NetBIOS</span></dt><dd><p>
            <span class="productname">NetBIOS</span> is a software interface
            (API) designed for name resolution and communication between
            computers on a network. It enables machines connected to the
            network to reserve names for themselves. After reservation, these
            machines can be addressed by name. There is no central process that
            checks names. Any machine on the network can reserve as many names
            as it wants, as long as the names are not already in use. NetBIOS
            can be implemented on top of different network protocols. One
            relatively simple, non-routable implementation is called
            <span class="productname">NetBEUI</span>. (This is often
            confused with the NetBIOS API.) NetBIOS is also supported on top of
            the Novell IPX/SPX protocol. Since version 3.2, Samba supports
            NetBIOS over both IPv4 and IPv6.
          </p><p>
            The NetBIOS names sent via TCP/IP have nothing in common with the
            names used in <code class="filename">/etc/hosts</code> or those defined by
            DNS. NetBIOS uses its own, completely independent naming
            convention. However, we recommend using names that correspond
            to DNS host names, to make administration easier, or to use DNS
            natively. This is the default used by Samba.
          </p></dd><dt id="id-1.11.6.8.4.3.4"><span class="term">Samba server</span></dt><dd><p>
            Samba server provides SMB/CIFS services and NetBIOS over IP naming
            services to clients. For Linux, there are three daemons for the
            Samba server: <code class="literal">smbd</code> for SMB/CIFS services,
            <code class="literal">nmbd</code> for naming services, and
            <code class="literal">winbind</code> for authentication.
          </p></dd><dt id="id-1.11.6.8.4.3.5"><span class="term">Samba client</span></dt><dd><p>
            The Samba client is a system that uses Samba services from a Samba
            server over the SMB protocol. Common operating systems, such as
            Windows and macOS, support the SMB protocol. The TCP/IP protocol
            must be installed on all computers. Samba provides a client for the
            different Unix flavors. For Linux, there is a kernel module for SMB
            that allows the integration of SMB resources on the Linux system
            level. You do not need to run any daemon for the Samba client.
          </p></dd><dt id="id-1.11.6.8.4.3.6"><span class="term">Shares</span></dt><dd><p>
            SMB servers provide resources to the clients by
            <span class="emphasis"><em>shares</em></span>. Shares are directories (including
            their subdirectories) and printers on the server. A share is
            exported by means of a <span class="emphasis"><em>share name</em></span>, and can be
            accessed by this name. The share name can be set to any
            name—it does not need to be the name of the export directory.
            Shared printers are also assigned names. Clients can access shared
            directories and printers by their names.
          </p><p>
            By convention, share names ending with a dollar symbol
            (<code class="literal">$</code>) are hidden. When using a Windows
            computer to browse available shares, they will not be displayed.
          </p></dd><dt id="id-1.11.6.8.4.3.7"><span class="term">DC</span></dt><dd><p>
            A domain controller (DC) is a server that handles accounts in a
            domain. For data replication, it is possible to have multiple
            domain controllers in a single domain.
          </p></dd></dl></div></section><section class="sect1" id="sec-samba-install" data-id-title="Installing a Samba server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.2 </span><span class="title-name">Installing a Samba server</span></span> <a title="Permalink" class="permalink" href="#sec-samba-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      To install a Samba server, start YaST and select
      <span class="guimenu">Software</span> › <span class="guimenu">Software
      Management</span>. Choose
      <span class="guimenu">View</span> › <span class="guimenu">Patterns</span> and select <span class="guimenu">File
      Server</span>. Confirm the installation of the required packages to
      finish the installation process.
    </p></section><section class="sect1" id="sec-samba-serv-start" data-id-title="Starting and stopping Samba"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.3 </span><span class="title-name">Starting and stopping Samba</span></span> <a title="Permalink" class="permalink" href="#sec-samba-serv-start">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      You can start or stop the Samba server automatically (during boot) or
      manually. The starting and stopping policy is a part of the YaST Samba
      server configuration described in <a class="xref" href="#sec-samba-yast2-conf" title="20.4.1. Configuring a Samba server with YaST">Section 20.4.1, “Configuring a Samba server with YaST”</a>.
    </p><p>
      From a command line, stop services required for Samba with
      <code class="command">systemctl stop smb nmb</code> and start them with
      <code class="command">systemctl start nmb smb</code>. The <code class="literal">smb</code>
      service cares about <code class="literal">winbind</code> if needed.
    </p><div id="id-1.11.6.8.6.4" data-id-title="winbind" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: <code class="systemitem">winbind</code></div><p>
        <code class="systemitem">winbind</code> is an independent service and
        is also offered as an individual <code class="systemitem">samba-winbind</code>
        package.
      </p></div></section><section class="sect1" id="sec-samba-serv-inst" data-id-title="Configuring a Samba server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.4 </span><span class="title-name">Configuring a Samba server</span></span> <a title="Permalink" class="permalink" href="#sec-samba-serv-inst">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      A Samba server in <span class="productname"><span class="phrase">SUSE® Linux Enterprise Server</span></span> can be configured in two different
      ways: with YaST or manually. Manual configuration offers a higher level
      of detail, but lacks the convenience of the YaST GUI.
    </p><section class="sect2" id="sec-samba-yast2-conf" data-id-title="Configuring a Samba server with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.4.1 </span><span class="title-name">Configuring a Samba server with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To configure a Samba server, start YaST and select
        <span class="guimenu">Network Services</span> › <span class="guimenu">Samba
        Server</span>.
      </p><section class="sect3" id="sec-samba-yast2-conf-inst" data-id-title="Initial Samba configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.4.1.1 </span><span class="title-name">Initial Samba configuration</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf-inst">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          When starting the module for the first time, the <span class="guimenu">Samba
          Installation</span> dialog starts, prompting you to make a few
          basic decisions concerning the administration of the server. At the end
          of the configuration, it prompts for the Samba administrator password
          (<span class="guimenu">Samba Root Password</span>). For later starts, the
          <span class="guimenu">Samba Configuration</span> dialog appears.
        </p><p>
          The <span class="guimenu">Samba Installation</span> dialog consists of two
          steps and optional detailed settings:
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.8.7.3.3.4.1"><span class="term">Workgroup or domain name</span></dt><dd><p>
                Select an existing name from <span class="guimenu">Workgroup or Domain
                Name</span> or enter a new one and click
                <span class="guimenu">Next</span>.
              </p></dd><dt id="id-1.11.6.8.7.3.3.4.2"><span class="term">Samba server type</span></dt><dd><p>
                In the next step, specify whether your server should act as a
                primary domain controller (PDC), backup domain controller
                (BDC), or not act as a domain controller. Continue with
                <span class="guimenu">Next</span>.
              </p></dd></dl></div><p>
          If you do not want to proceed with a detailed server configuration,
          confirm with <span class="guimenu">OK</span>. Then in the final pop-up box, set
          the <span class="guimenu">Samba root Password</span>.
        </p><p>
          You can change all settings later in the <span class="guimenu">Samba
          Configuration</span> dialog with the <span class="guimenu">Start-Up</span>,
          <span class="guimenu">Shares</span>, <span class="guimenu">Identity</span>,
          <span class="guimenu">Trusted Domains</span>, and <span class="guimenu">LDAP
          Settings</span> tabs.
        </p></section><section class="sect3" id="sec-samba-server-new-protocol" data-id-title="Enabling current versions of the SMB protocol on the server"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.4.1.2 </span><span class="title-name">Enabling current versions of the SMB protocol on the server</span></span> <a title="Permalink" class="permalink" href="#sec-samba-server-new-protocol">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          On clients running current versions of <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> or other recent
          Linux versions, the insecure SMB1/CIFS protocol is disabled by
          default. However, existing instances of Samba may be configured to
          only serve shares using the SMB1/CIFS version of the protocol. To
          interact with such clients, you need to configure Samba to serve
          shares using at least the SMB 2.1 protocol.
        </p><p>
          There are setups in which only SMB1 can be used—for example,
          because they rely on SMB1's/CIFS's Unix extensions. These extensions
          have not been ported to newer protocol versions. If you are in this
          situation, consider changing your setup or see
          <a class="xref" href="#sec-samba-client-old-server" title="20.5.2. Mounting SMB1/CIFS shares on clients">Section 20.5.2, “Mounting SMB1/CIFS shares on clients”</a>.
        </p><p>
          To do so, in the configuration file
          <code class="filename">/etc/samba/smb.conf</code>, set the global parameter
          <code class="literal">server max protocol = SMB2_10</code>. For a list of all
          possible values, see <code class="command">man smb.conf</code>.
        </p></section><section class="sect3" id="sec-samba-yast2-conf-adv" data-id-title="Advanced Samba configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.4.1.3 </span><span class="title-name">Advanced Samba configuration</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf-adv">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          During the first start of the Samba server module, the <span class="guimenu">Samba
          Configuration</span> dialog appears directly after the two initial
          steps described in <a class="xref" href="#sec-samba-yast2-conf-inst" title="20.4.1.1. Initial Samba configuration">Section 20.4.1.1, “Initial Samba configuration”</a>. Use
          it to adjust your Samba server configuration.
        </p><p>
          After editing your configuration, click <span class="guimenu">OK</span> to save
          your settings.
        </p><section class="sect4" id="sec-samba-yast2-conf-adv-start" data-id-title="Starting the server"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">20.4.1.3.1 </span><span class="title-name">Starting the server</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf-adv-start">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            In the <span class="guimenu">Start Up</span> tab, configure the start of the
            Samba server. To start the service every time your system boots,
            select <span class="guimenu">During Boot</span>. To activate manual start,
            choose <span class="guimenu">Manually</span>. More information about starting
            a Samba server is provided in
            <a class="xref" href="#sec-samba-serv-start" title="20.3. Starting and stopping Samba">Section 20.3, “Starting and stopping Samba”</a>.
          </p><p>
            In this tab, you can also open ports in your firewall. To do so,
            select <span class="guimenu">Open Port in Firewall</span>. If you have
            multiple network interfaces, select the network interface for Samba
            services by clicking <span class="guimenu">Firewall Details</span>, selecting
            the interfaces, and clicking <span class="guimenu">OK</span>.
          </p></section><section class="sect4" id="sec-samba-yast2-conf-adv-shares" data-id-title="Shares"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">20.4.1.3.2 </span><span class="title-name">Shares</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf-adv-shares">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            In the <span class="guimenu">Shares</span> tab, determine the Samba shares to
            activate. There are some predefined shares, like homes and
            printers. Use <span class="guimenu">Toggle Status</span> to switch between
            <span class="guimenu">Active</span> and <span class="guimenu">Inactive</span>. Click
            <span class="guimenu">Add</span> to add new shares and
            <span class="guimenu">Delete</span> to delete the selected share.
          </p><p>
            <span class="guimenu">Allow Users to Share Their Directories</span> enables
            members of the group in <span class="guimenu">Permitted Group</span> to share
            directories they own with other users. For example,
            <code class="systemitem">users</code> for a local scope or
            <code class="systemitem">DOMAIN\Users</code> for a domain scope. The user
            also must make sure that the file system permissions allow access.
            With <span class="guimenu">Maximum Number of Shares</span>, limit the total
            amount of shares that may be created. To permit access to user
            shares without authentication, enable <span class="guimenu">Allow Guest
            Access</span>.
          </p></section><section class="sect4" id="sec-samba-yast2-conf-adv-identity" data-id-title="Identity"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">20.4.1.3.3 </span><span class="title-name">Identity</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf-adv-identity">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            In the <span class="guimenu">Identity</span> tab, you can determine the
            domain with which the host is associated (<span class="guimenu">Base
            Settings</span>) and whether to use an alternative host name in
            the network (<span class="guimenu">NetBIOS Hostname</span>).
            
            It is also possible to use Microsoft Windows Internet Name Service
            (WINS) for name resolution. In this case, activate <span class="guimenu">Use
            WINS for Hostname Resolution</span> and decide whether to
            <span class="guimenu">Retrieve WINS server via DHCP</span>. To set expert
            global settings or set a user authentication source,
            <span class="phrase">for example LDAP instead of TDB database,
            </span>click <span class="guimenu">Advanced Settings</span>.
          </p></section><section class="sect4" id="sec-samba-yast2-conf-adv-trusted" data-id-title="Trusted domains"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">20.4.1.3.4 </span><span class="title-name">Trusted domains</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf-adv-trusted">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            To enable users from other domains to access your domain, make the
            appropriate settings in the <span class="guimenu">Trusted Domains</span> tab.
            Add a new domain by clicking <span class="guimenu">Add</span>. To remove the
            selected domain, click <span class="guimenu">Delete</span>.
          </p></section><section class="sect4" id="sec-samba-yast2-conf-adv-ldap" data-id-title="LDAP settings"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">20.4.1.3.5 </span><span class="title-name">LDAP settings</span></span> <a title="Permalink" class="permalink" href="#sec-samba-yast2-conf-adv-ldap">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
            In the tab <span class="guimenu">LDAP Settings</span>, you can determine the
            LDAP server to use for authentication. To test the connection to
            your LDAP server, click <span class="guimenu">Test Connection</span>. To set
            expert LDAP settings or use default values, click <span class="guimenu">Advanced
            Settings</span>.
          </p><p>
            For more information about LDAP configuration, see
            <span class="intraxref">Book “Security and Hardening Guide”, Chapter 5 “LDAP with 389 Directory Server”</span>.
          </p></section></section></section><section class="sect2" id="sec-samba-serv-inst-manual" data-id-title="Configuring the server manually"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.4.2 </span><span class="title-name">Configuring the server manually</span></span> <a title="Permalink" class="permalink" href="#sec-samba-serv-inst-manual">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        If you intend to use Samba as a server, install
        <code class="systemitem">samba</code>. The main configuration
        file for Samba is <code class="filename">/etc/samba/smb.conf</code>. This file
        can be divided into two logical parts. The <code class="literal">[global]</code>
        section contains the central and global settings. The following default
        sections contain the individual file and printer shares:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            [homes]
          </p></li><li class="listitem"><p>
            [profiles]
          </p></li><li class="listitem"><p>
            [users]
          </p></li><li class="listitem"><p>
            [groups]
          </p></li><li class="listitem"><p>
            [printers]
          </p></li><li class="listitem"><p>
            [print$]
          </p></li></ul></div><p>
        Using this approach, options of the shares can be set differently or
        globally in the <code class="literal">[global]</code> section, which makes the
        configuration file easier to understand.
      </p><section class="sect3" id="sec-samba-smb-conf-Erlaeuterung" data-id-title="The global section"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.4.2.1 </span><span class="title-name">The global section</span></span> <a title="Permalink" class="permalink" href="#sec-samba-smb-conf-Erlaeuterung">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The following parameters of the <code class="literal">[global]</code> section
          should be modified to match the requirements of your network setup,
          so other machines can access your Samba server via SMB in a Windows
          environment.
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.8.7.4.5.4.1"><span class="term"><code class="literal">workgroup = WORKGROUP</code></span></dt><dd><p>
                This line assigns the Samba server to a workgroup. Replace
                <code class="literal">WORKGROUP</code> with an appropriate workgroup in
                your networking environment. Your Samba server appears under
                its DNS name unless this name has been assigned to some other
                machine in the network. If the DNS name is not available, set
                the server name using
                <code class="literal">netbiosname=<em class="replaceable">MYNAME</em></code>.
                For more details about this parameter, see the
                <code class="systemitem">smb.conf</code> man page.
              </p></dd><dt id="id-1.11.6.8.7.4.5.4.2"><span class="term"><code class="literal">os level = 20</code></span></dt><dd><p>
                This parameter triggers whether your Samba server tries to
                become an LMB (local master browser) for its workgroup. Choose a
                very low value, such as <code class="literal">2</code>, to spare the
                existing Windows network from any interruptions caused by a
                misconfigured Samba server. More information about this topic
                can be found in the Network Browsing chapter of the Samba 3
                Howto. For more information about the Samba 3 Howto, see
                <a class="xref" href="#sec-samba-info" title="20.9. More information">Section 20.9, “More information”</a>.
              </p><p>
                If no other SMB server is in your network (such as Windows
                2000 server) and you want the Samba server to keep a list of
                all systems present in the local environment, set the
                <code class="literal">os level</code> to a higher value (for example,
                <code class="literal">65</code>). Your Samba server is then chosen as LMB
                for your local network.
              </p><p>
                When changing this setting, consider carefully how this could
                affect an existing Windows network environment. First, test the
                changes in an isolated network or at a noncritical time of day.
              </p></dd><dt id="id-1.11.6.8.7.4.5.4.3"><span class="term"><code class="literal">wins support</code> and <code class="literal">wins server</code></span></dt><dd><p>
                To integrate your Samba server into an existing Windows network
                with an active WINS server, enable the <code class="option">wins
                server</code> option and set its value to the IP address of
                that WINS server.
              </p><p>
                If your Windows machines are connected to separate subnets and
                need to still be aware of each other, you need to set up a WINS
                server. To turn a Samba server into such a WINS server, set the
                option <code class="literal">wins support = Yes</code>. Make sure that
                only one Samba server of the network has this setting enabled.
                The options <code class="literal">wins server</code> and <code class="literal">wins
                support</code> must never be enabled at the same time in
                your <code class="filename">smb.conf</code> file.
              </p></dd></dl></div></section><section class="sect3" id="sec-samba-smb-conf-shares" data-id-title="Shares"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.4.2.2 </span><span class="title-name">Shares</span></span> <a title="Permalink" class="permalink" href="#sec-samba-smb-conf-shares">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The following examples illustrate how a CD-ROM drive and the user
          directories (<code class="literal">homes</code>) are made available to the SMB
          clients.
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.8.7.4.6.3.1"><span class="term">[cdrom]</span></dt><dd><p>
                To avoid having the CD-ROM drive accidentally made available,
                these lines are deactivated with comment marks (semicolons in
                this case). Remove the semicolons in the first column to share
                the CD-ROM drive with Samba.
              </p><div class="example" id="dat-cd-rom" data-id-title="A CD-ROM share"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 20.1: </span><span class="title-name">A CD-ROM share </span></span><a title="Permalink" class="permalink" href="#dat-cd-rom">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">[cdrom]
       comment = Linux CD-ROM
       path = /media/cdrom
       locking = No</pre></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.8.7.4.6.3.1.2.3.1"><span class="term"><code class="option">[cdrom]</code> and <code class="option">comment</code></span></dt><dd><p>
                      The <code class="literal">[cdrom]</code> section entry is the name
                      of the share that can be seen by all SMB clients on the
                      network. An additional <code class="literal">comment</code> can be
                      added to further describe the share.
                    </p></dd><dt id="id-1.11.6.8.7.4.6.3.1.2.3.2"><span class="term"><code class="option">path = /media/cdrom</code></span></dt><dd><p>
                      <code class="option">path</code> exports the directory
                      <code class="filename">/media/cdrom</code>.
                    </p></dd></dl></div><p>
                By means of a very restrictive default configuration, this kind
                of share is only made available to the users present on this
                system. If this share should be made available to everybody,
                add a line <code class="literal">guest ok = yes</code> to the
                configuration. This setting gives read permissions to anyone on
                the network. We recommend handling this parameter with
                great care. This applies even more to the use of this parameter
                in the <code class="literal">[global]</code> section.
              </p></dd><dt id="id-1.11.6.8.7.4.6.3.2"><span class="term"><code class="option">[homes]</code></span></dt><dd><p>
                The <code class="option">[homes]</code> share is of special importance
                here. If the user has a valid account and password for the
                Linux file server and their own home directory, they can be
                connected to it.
              </p><div class="example" id="dat-homes-frei" data-id-title="[homes] share"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 20.2: </span><span class="title-name">[homes] share </span></span><a title="Permalink" class="permalink" href="#dat-homes-frei">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">[homes]
        comment = Home Directories
        valid users = %S
        browseable = No
        read only = No
        inherit acls = Yes</pre></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.8.7.4.6.3.2.2.3.1"><span class="term">[homes]</span></dt><dd><p>
                      As long as there is no other share using the share name
                      of the user connecting to the SMB server, a share is
                      dynamically generated using the
                      <code class="literal">[homes]</code> share directives. The
                      resulting name of the share is the user name.
                    </p></dd><dt id="id-1.11.6.8.7.4.6.3.2.2.3.2"><span class="term"><code class="option">valid users = %S</code></span></dt><dd><p>
                      <code class="literal">%S</code> is replaced with the concrete name
                      of the share when a connection has been successfully
                      established. For a <code class="option">[homes]</code> share, this
                      is always the user name. As a consequence, access rights
                      to a user's share are restricted exclusively to that
                      user.
                    </p></dd><dt id="id-1.11.6.8.7.4.6.3.2.2.3.3"><span class="term"><code class="option">browseable = No</code></span></dt><dd><p>
                      This setting makes the share invisible in the network
                      environment.
                    </p></dd><dt id="id-1.11.6.8.7.4.6.3.2.2.3.4"><span class="term"><code class="option">read only = No</code></span></dt><dd><p>
                      By default, Samba prohibits write access to any exported
                      share by means of the <code class="literal">read only = Yes</code>
                      parameter. To make a share writable, set the value
                      <code class="literal">read only = No</code>, which is synonymous
                      with <code class="literal">writable = Yes</code>.
                    </p></dd><dt id="id-1.11.6.8.7.4.6.3.2.2.3.5"><span class="term"><code class="option">create mask = 0640</code></span></dt><dd><p>
                      Systems that are not based on MS Windows NT do not
                      understand the concept of Unix permissions, so they
                      cannot assign permissions when creating a file. The
                      parameter <code class="literal">create mask</code> defines the
                      access permissions assigned to newly created files. This
                      only applies to writable shares. In effect, this setting
                      means the owner has read and write permissions and the
                      members of the owner's primary group have read
                      permissions. <code class="option">valid users = %S</code> prevents
                      read access even if the group has read permissions. For
                      the group to have read or write access, deactivate the
                      line <code class="option">valid users = %S</code>.
                    </p></dd></dl></div></dd></dl></div><div id="id-1.11.6.8.7.4.6.4" data-id-title="Do not share NFS mounts with Samba" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Do not share <code class="literal">NFS</code> mounts with Samba</div><p>
            Sharing <code class="literal">NFS</code> mounts with Samba may result in data
            loss and is not supported. Install Samba directly on the file
            server or consider using alternatives, such as
            <code class="literal">iSCSI</code>.
          </p></div></section><section class="sect3" id="sec-samba-rechte" data-id-title="Security levels"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.4.2.3 </span><span class="title-name">Security levels</span></span> <a title="Permalink" class="permalink" href="#sec-samba-rechte">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          To improve security, each share access can be protected with a
          password. SMB offers the following ways of checking permissions:
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.8.7.4.7.3.1"><span class="term">User level security (<code class="literal">security = user</code>)</span></dt><dd><p>
                This variant introduces the concept of the user to SMB. Each
                user must register with the server with their own password.
                After registration, the server can grant access to individual
                exported shares dependent on user names.
              </p></dd><dt id="id-1.11.6.8.7.4.7.3.2"><span class="term">ADS level security (<code class="literal">security = ADS</code>)</span></dt><dd><p>
                In this mode, Samba will act as a domain member in an Active
                Directory environment. To operate in this mode, the machine
                running Samba needs Kerberos installed and configured. You must
                join the machine using Samba to the ADS realm. This can be done
                using the YaST <span class="guimenu">Windows Domain Membership</span>
                module.
              </p></dd><dt id="id-1.11.6.8.7.4.7.3.3"><span class="term">Domain level security (<code class="literal">security = domain</code>)</span></dt><dd><p>
                This mode will only work correctly if the machine has been
                joined to a Windows NT domain. Samba will try to validate the
                user name and password by passing it to a Windows Primary or
                Backup Domain Controller, the same way as a Windows Server
                would do. It expects the encrypted passwords parameter to be
                set to <code class="literal">yes</code>.
              </p></dd></dl></div><p>
          The selection of share, user, server, or domain level security
          applies to the entire server. It is not possible to offer individual
          shares of a server configuration with share level security and others
          with user level security. However, you can run a separate Samba
          server for each configured IP address on a system.
        </p><p>
          More information about this subject can be found in the Samba 3
          HOWTO. For multiple servers on one system, pay attention to the
          options <code class="option">interfaces</code> and <code class="option">bind interfaces
          only</code>.
        </p></section></section></section><section class="sect1" id="sec-samba-client-inst" data-id-title="Configuring clients"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.5 </span><span class="title-name">Configuring clients</span></span> <a title="Permalink" class="permalink" href="#sec-samba-client-inst">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Clients can only access the Samba server via TCP/IP. NetBEUI and NetBIOS
      via IPX cannot be used with Samba.
    </p><section class="sect2" id="sec-samba-client-inst-yast" data-id-title="Configuring a Samba client with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.5.1 </span><span class="title-name">Configuring a Samba client with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-samba-client-inst-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Configure a Samba client to access resources (files or printers) on the
        Samba or Windows server. Enter the Windows or Active Directory domain
        or workgroup in the dialog <span class="guimenu">Network
        Services</span> › <span class="guimenu">Windows Domain
        Membership</span>. If you activate <span class="guimenu">Also Use
        SMB Information for Linux Authentication</span>, the user
        authentication runs over the Samba, Windows, or Kerberos server.
      </p><p>
        Click <span class="guimenu">Expert Settings</span> for advanced configuration
        options. For example, use the <span class="guimenu">Mount Server
        Directories</span> table to enable mounting the server home directory
        automatically with authentication. This way users can access their home
        directories when hosted on CIFS. For details, see the
        <code class="systemitem">pam_mount</code> man page.
      </p><p>
        After completing all settings, confirm the dialog to finish the
        configuration.
      </p></section><section class="sect2" id="sec-samba-client-old-server" data-id-title="Mounting SMB1/CIFS shares on clients"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.5.2 </span><span class="title-name">Mounting SMB1/CIFS shares on clients</span></span> <a title="Permalink" class="permalink" href="#sec-samba-client-old-server">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The first version of the SMB network protocol, SMB1 or CIFS, is an old
        and insecure protocol, which has been deprecated by its originator,
        Microsoft. For security reasons, the <code class="command">mount</code> command
        on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> will only mount SMB shares using newer protocol
        versions by default, namely SMB 2.1, SMB 3.0, or SMB 3.02.
      </p><p>
        However, this change only affects <code class="command">mount</code> and mounting
        via <code class="filename">/etc/fstab</code>. SMB1 is still available by
        explicitly requiring it. Use the following:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            The <code class="command">smbclient</code> tool.
          </p></li><li class="listitem"><p>
            The Samba server software shipped with
            
            <span class="phrase">SUSE Linux Enterprise Server</span>.
          </p></li></ul></div><p>
        There are setups in which this default setting will lead to connection
        failures, because only SMB1 can be used:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            Setups using an SMB server that does not support newer SMB protocol
            versions. Windows has offered SMB 2.1 support since Windows 7 and
            Windows Server 2008.
          </p></li><li class="listitem"><p>
            Setups that rely on SMB1's/CIFS's Unix extensions. These extensions
            have not been ported to newer protocol versions.
          </p></li></ul></div><div id="id-1.11.6.8.8.4.7" data-id-title="Decreased system security" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Decreased system security</div><p>
          Following the instruction below makes it possible to exploit security
          issues. For more information about the issues, see
          <a class="link" href="https://blogs.technet.microsoft.com/filecab/2016/09/16/stop-using-smb1/" target="_blank">https://blogs.technet.microsoft.com/filecab/2016/09/16/stop-using-smb1/</a>.
        </p><p>
          As soon as possible, upgrade your server to allow for a more secure
          SMB version.
        </p><p>
          For information about enabling suitable protocol versions on
          <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, see <a class="xref" href="#sec-samba-server-new-protocol" title="20.4.1.2. Enabling current versions of the SMB protocol on the server">Section 20.4.1.2, “Enabling current versions of the SMB protocol on the server”</a>.
        </p></div><p>
        If you need to enable SMB1 shares on the current <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> kernel,
        add the option <code class="option">vers=1.0</code> to the
        <code class="command">mount</code> command line you use:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t cifs //<em class="replaceable">HOST</em>/<em class="replaceable">SHARE</em> /<em class="replaceable">MOUNT_POINT</em> –o username=<em class="replaceable">USER_ID</em>,<span class="bold"><strong>vers=1.0</strong></span></pre></div><p>
        Alternatively, you can enable SMB1 shares globally within your
        <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> installation. To do so, add the following to
        <code class="filename">/etc/samba/smb.conf</code> under the section
        <code class="literal">[global]</code>:
      </p><div class="verbatim-wrap"><pre class="screen">        client min protocol = CORE</pre></div></section></section><section class="sect1" id="sec-samba-anmeld-serv" data-id-title="Samba as login server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.6 </span><span class="title-name">Samba as login server</span></span> <a title="Permalink" class="permalink" href="#sec-samba-anmeld-serv">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      In business settings, it is often desirable to allow access only to users
      registered on a central instance. In a Windows-based network, this task
      is handled by a primary domain controller (PDC). You can use a Windows
      Server configured as PDC, but this task can also be done with a Samba
      server. The entries that must be made in the <code class="literal">[global]</code>
      section of <code class="filename">smb.conf</code> are shown in
      <a class="xref" href="#dat-samba-smb-conf-dom" title="Global section in smb.conf">Example 20.3, “Global section in smb.conf”</a>.
    </p><div class="example" id="dat-samba-smb-conf-dom" data-id-title="Global section in smb.conf"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 20.3: </span><span class="title-name">Global section in smb.conf </span></span><a title="Permalink" class="permalink" href="#dat-samba-smb-conf-dom">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">[global]
    workgroup = WORKGROUP
    domain logons = Yes
    domain master = Yes</pre></div></div></div><p>
      It is necessary to prepare user accounts and passwords in an encryption
      format that conforms to Windows. Do this with the command
      <code class="command">smbpasswd</code> <code class="option">-a name</code>. Create the domain
      account for the computers required by the Windows domain concept with
      the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">useradd hostname
smbpasswd -a -m hostname</pre></div><p>
      With the <code class="command">useradd</code> command, a dollar sign is added. The
      command <code class="command">smbpasswd</code> inserts this automatically when the
      parameter <code class="option">-m</code> is used. The commented configuration
      example
      (<code class="filename">/usr/share/doc/packages/samba/examples/smb.conf.SUSE</code>)
      contains settings that automate this task.
    </p><div class="verbatim-wrap"><pre class="screen">add machine script = /usr/sbin/useradd -g nogroup -c "NT Machine Account" \
-s /bin/false %m</pre></div><p>
      To make sure that Samba can execute this script correctly, choose a Samba
      user with the required administrator permissions and add it to the
      <code class="systemitem">ntadmin</code> group. Then all users
      belonging to this Linux group can be assigned <code class="literal">Domain
      Admin</code> status with the command:
    </p><div class="verbatim-wrap"><pre class="screen">net groupmap add ntgroup="Domain Admins" unixgroup=ntadmin</pre></div></section><section class="sect1" id="sec-samba-adnet" data-id-title="Samba server in the network with Active Directory"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.7 </span><span class="title-name">Samba server in the network with Active Directory</span></span> <a title="Permalink" class="permalink" href="#sec-samba-adnet">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      If you run Linux servers and Windows servers together, you can build two
      independent authentication systems and networks or connect servers to one
      network with one central authentication system. Because Samba can
      cooperate with an Active Directory domain, you can join your
      <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> server with an Active Directory (AD) domain.
    </p><p>
      To join an AD domain, proceed as follows:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          Log in as <code class="systemitem">root</code> and start YaST.
        </p></li><li class="step"><p>
          Start <span class="guimenu">Network Services</span> › <span class="guimenu">Windows Domain Membership</span>.
        </p></li><li class="step"><p>
          Enter the domain to join in the <span class="guimenu">Domain or
          Workgroup</span> field in the <span class="guimenu">Windows Domain
          Membership</span> screen.
        </p><div class="figure" id="id-1.11.6.8.10.4.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/ad_sambaclient.png"><img src="images/ad_sambaclient.png" alt="Joining workgroup or domain." title="Joining workgroup or domain."/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 20.1: </span><span class="title-name">Determining Windows domain membership </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.8.10.4.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
          Check <span class="guimenu">Also Use SMB Information for Linux
          Authentication</span> to use the SMB source for Linux
          authentication on your server.
        </p></li><li class="step"><p>
          Click <span class="guimenu">OK</span> and confirm the domain join when prompted
          for it.
        </p></li><li class="step"><p>
          Provide the password for the Windows Administrator on the AD server
          and click <span class="guimenu">OK</span>.
        </p><p>
          Your server is now set up to pull in all authentication data from the
          Active Directory domain controller.
        </p></li></ol></div></div><p>
      Alternatively, you can use the <code class="command">realmd</code> tool to connect
      to Active Directory. For details, refer to
      <a class="xref" href="#sec-realmd-ad" title="20.7.1. Using realmd to manage Active Directory">Section 20.7.1, “Using <code class="command">realmd</code> to manage Active Directory”</a>.
    </p><div id="id-1.11.6.8.10.6" data-id-title="Identity mapping" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Identity mapping</div><p>
        In an environment with more than one Samba server, UIDs and GIDs will
        not be created consistently. The UIDs that get assigned to users will
        be dependent on the order in which they first log in, which results in
        UID conflicts across servers. To fix this, you need to use identity
        mapping. See
        <a class="link" href="https://www.samba.org/samba/docs/man/Samba-HOWTO-Collection/idmapper.html" target="_blank">https://www.samba.org/samba/docs/man/Samba-HOWTO-Collection/idmapper.html</a>
        for more details.
      </p></div><section class="sect2" id="sec-realmd-ad" data-id-title="Using realmd to manage Active Directory"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.7.1 </span><span class="title-name">Using <code class="command">realmd</code> to manage Active Directory</span></span> <a title="Permalink" class="permalink" href="#sec-realmd-ad">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        <code class="command">realmd</code> is a DBus service that allows you to
        configure network authentication and domain membership.
      </p><section class="sect3" id="sec-realmd-discovery" data-id-title="Discovering Active Directory domains"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.7.1.1 </span><span class="title-name">Discovering Active Directory domains</span></span> <a title="Permalink" class="permalink" href="#sec-realmd-discovery">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          <code class="command">realmd</code> discovers which domains or realms can be
          used or configured by checking the DNS SRV records. Ensure that a
          DNS SRV record is available for the Active Directory domain to be
          discovered; <span class="emphasis"><em>domain.example.com</em></span> in the following
          example:
        </p><div class="verbatim-wrap"><pre class="screen">_ldap._tcp.dc._msdcs.domain.example.com.</pre></div><p>
          The DNS records should be created automatically by the DNS server
          that comes with the Active Directory.
        </p><p>
          To discover a particular domain name, run the command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">realm discover --verbose domain.example.com</code>

* Resolving: _ldap._tcp.dc._msdcs.domain.example.com
 * Sending MS-CLDAP ping to: 192.168.20.10
 * Sending MS-CLDAP ping to: 192.168.12.12
 * Successfully discovered: domain.example.com
...</pre></div><p>
          To join a particular Active Directory domain, run the command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">realm join --verbose domain.example.com</code></pre></div><p>
          Once you joined the Active Directory domain, you can configure the
          machine to enable logging in using the domain accounts. To do so,
          run:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">realm permit --realm domain.example.com --all</code></pre></div><p>
          To permit only specific accounts by specifying them in the command as
          follows:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">realm permit --realm domain.example.com DOMAIN\\<em class="replaceable">USERNAME</em> DOMAIN\\<em class="replaceable">USERNAME</em></code></pre></div><p>
          To deny logins from any domain account, use the following command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">realm deny --realm domain.example.com --all</code></pre></div></section></section></section><section class="sect1" id="sec-samba-advanced" data-id-title="Advanced topics"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.8 </span><span class="title-name">Advanced topics</span></span> <a title="Permalink" class="permalink" href="#sec-samba-advanced">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      This section introduces more advanced techniques to manage both the
      client and server parts of the Samba suite.
    </p><section class="sect2" id="sec-automount-systemd" data-id-title="Automounting CIFS file system using systemd"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.8.1 </span><span class="title-name">Automounting CIFS file system using <code class="systemitem">systemd</code></span></span> <a title="Permalink" class="permalink" href="#sec-automount-systemd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        You can use <code class="systemitem">systemd</code> to mount CIFS shares on startup. To do so,
        proceed as described further:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Create the mount points:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>mkdir -p <em class="replaceable">PATH_SERVER_SHARED_FOLDER</em></pre></div><p>
            where <em class="replaceable">PATH_SERVER_SHARED_FOLDER</em> is
            <code class="literal">/cifs/shared</code> in further steps.
          </p></li><li class="step"><p>
            Create the <code class="systemitem">systemd</code> unit file and generate a file name from the
            path specified in the previous step where "/" are replaced with
            "-", for example:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> touch /etc/systemd/system/cifs-shared.mount</pre></div><p>
            with the following content:
          </p><div class="verbatim-wrap"><pre class="screen">[Unit]
Description=CIFS share from The-Server

[Mount]
What=//The-Server/Shared-Folder
Where=/cifs/shared
Type=cifs
Options=rw,username=vagrant,password=admin

[Install]
WantedBy=multi-user.target</pre></div></li><li class="step"><p>
            Enable the service:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl enable cifs-shared.mount</pre></div></li><li class="step"><p>
            Start the service:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl start cifs-shared.mount</pre></div><p>
            To verify that the service is running, run the command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl status cifs-shared.mount</pre></div></li><li class="step"><p>
            To confirm that the CIFS shared path is available, try the
            following command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code> cd /cifs/shared
<code class="prompt user">&gt; </code>ls -l

total 0
-rwxrwxrwx. 1 root    root    0 Oct 24 22:31 hello-world-cifs.txt
drwxrwxrwx. 2 root    root    0 Oct 24 22:31 subfolder
-rw-r--r--. 1 vagrant vagrant 0 Oct 28 21:51 testfile.txt</pre></div></li></ol></div></div></section><section class="sect2" id="sec-samba-advanced-compress" data-id-title="Transparent file compression on Btrfs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.8.2 </span><span class="title-name">Transparent file compression on Btrfs</span></span> <a title="Permalink" class="permalink" href="#sec-samba-advanced-compress">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Samba allows clients to remotely manipulate file and directory
        compression flags for shares placed on the Btrfs file system. Windows
        Explorer provides the ability to flag files/directories for transparent
        compression via the
        <span class="guimenu">File</span> › <span class="guimenu">Properties</span> › <span class="guimenu">Advanced</span>
        dialog:
      </p><div class="figure" id="id-1.11.6.8.11.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/samba_win_expl_advattr.png"><img src="images/samba_win_expl_advattr.png" alt="File properties" title="File properties"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 20.2: </span><span class="title-name">Windows Explorer <span class="guimenu">Advanced Attributes</span> dialog </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.8.11.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div><p>
        Files flagged for compression are transparently compressed and
        decompressed by the underlying file system when accessed or modified.
        This normally results in storage capacity savings at the expense of
        extra CPU overhead when accessing the file. New files and directories
        inherit the compression flag from the parent directory, unless created
        with the FILE_NO_COMPRESSION option.
      </p><p>
        Windows Explorer presents compressed files and directories visually
        differently to those that are not compressed:
      </p><div class="figure" id="id-1.11.6.8.11.4.6"><div class="figure-contents"><div class="mediaobject"><a href="images/samba_win_expl_view_compressed.png"><img src="images/samba_win_expl_view_compressed.png" alt="Windows Explorer directory listing" title="Windows Explorer directory listing"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 20.3: </span><span class="title-name">Windows Explorer directory listing with compressed files </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.8.11.4.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div><p>
        You can enable Samba share compression either manually by adding
      </p><div class="verbatim-wrap"><pre class="screen">vfs objects = btrfs</pre></div><p>
        to the share configuration in <code class="filename">/etc/samba/smb.conf</code>,
        or using YaST: <span class="guimenu">Network
        Services</span> › <span class="guimenu">Samba
        Server</span> › <span class="guimenu">Add</span>, and checking
        <span class="guimenu">Utilize Btrfs Features</span>.
      </p><p>
        A general overview of compression on Btrfs can be found in
        <a class="xref" href="#sec-filesystems-major-btrfs-compress" title="1.2.2.1. Mounting compressed Btrfs file systems">Section 1.2.2.1, “Mounting compressed Btrfs file systems”</a>.
      </p></section><section class="sect2" id="sec-samba-advanced-snapshots" data-id-title="Snapshots"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.8.3 </span><span class="title-name">Snapshots</span></span> <a title="Permalink" class="permalink" href="#sec-samba-advanced-snapshots">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Snapshots, also called Shadow Copies, are copies of the state of a file
        system subvolume at a certain point in time. Snapper is the tool to
        manage these snapshots in Linux. Snapshots are supported on the Btrfs
        file system or thinly provisioned LVM volumes. The Samba suite supports
        managing remote snapshots through the FSRVP protocol on both the server
        and client side.
      </p><section class="sect3" id="sec-samba-advanced-snapshots-client" data-id-title="Previous versions"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.8.3.1 </span><span class="title-name">Previous versions</span></span> <a title="Permalink" class="permalink" href="#sec-samba-advanced-snapshots-client">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Snapshots on a Samba server can be exposed to remote Windows clients
          as previous versions of files or directories.
        </p><p>
          To enable snapshots on a Samba server, the following conditions must
          be fulfilled:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              The SMB network share resides on a Btrfs subvolume.
            </p></li><li class="listitem"><p>
              The SMB network share path has a related Snapper configuration
              file. You can create the snapper file with
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> snapper -c &lt;cfg_name&gt; create-config <code class="option">/path/to/share</code></pre></div><p>
              For more information on Snapper, see
              <span class="intraxref">Book “Administration Guide”, Chapter 10 “System recovery and snapshot management with Snapper”</span>.
            </p></li><li class="listitem"><p>
              The snapshot directory tree must allow access for relevant users.
              For more information, see the PERMISSIONS section of the
              vfs_snapper manual page (<code class="command">man 8 vfs_snapper</code>).
            </p></li></ul></div><p>
          To support remote snapshots, you need to modify the
          <code class="filename">/etc/samba/smb.conf</code> file. You can do this either
          with <span class="guimenu">YaST</span> › <span class="guimenu">Network
          Services</span> › <span class="guimenu">Samba Server</span>, or
          manually by enhancing the relevant share section with
        </p><div class="verbatim-wrap"><pre class="screen">vfs objects = snapper</pre></div><p>
          Note that you need to restart the Samba service for manual changes to
          <code class="filename">smb.conf</code> to take effect:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart nmb smb</pre></div><div class="figure" id="fig-yast2-samba-snapshot"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_samba_snapshot.png"><img src="images/yast2_samba_snapshot.png" alt="Adding a new Samba share" title="Adding a new Samba share"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 20.4: </span><span class="title-name">Adding a new Samba share with snapshots enabled </span></span><a title="Permalink" class="permalink" href="#fig-yast2-samba-snapshot">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div><p>
          After being configured, snapshots created by Snapper for the Samba
          share path can be accessed from Windows Explorer from a file or
          directory's <span class="guimenu">Previous Versions</span> tab.
        </p><div class="figure" id="fig-samba-win-explorer"><div class="figure-contents"><div class="mediaobject"><a href="images/samba_winexpl_previous_versions.png"><img src="images/samba_winexpl_previous_versions.png" alt="Previous versions tab" title="Previous versions tab"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 20.5: </span><span class="title-name">The <span class="guimenu">Previous versions</span> tab in Windows explorer </span></span><a title="Permalink" class="permalink" href="#fig-samba-win-explorer">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></section><section class="sect3" id="sec-samba-advanced-snapshots-server" data-id-title="Remote share snapshots"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.8.3.2 </span><span class="title-name">Remote share snapshots</span></span> <a title="Permalink" class="permalink" href="#sec-samba-advanced-snapshots-server">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          By default, snapshots can only be created and deleted on the Samba
          server locally, via the Snapper command line utility, or using
          Snapper's timeline feature.
        </p><p>
          Samba can be configured to process share snapshot creation and
          deletion requests from remote hosts using the File Server Remote VSS
          Protocol (FSRVP).
        </p><p>
          In addition to the configuration and prerequisites documented in
          <a class="xref" href="#sec-samba-advanced-snapshots-client" title="20.8.3.1. Previous versions">Section 20.8.3.1, “Previous versions”</a>, the following
          global configuration is required in
          <code class="filename">/etc/samba/smb.conf</code>:
        </p><div class="verbatim-wrap"><pre class="screen">[global]
rpc_daemon:fssd = fork
registry shares = yes
include = registry</pre></div><p>
          FSRVP clients, including Samba's <code class="command">rpcclient</code> and
          Windows Server 2012 <code class="command">DiskShadow.exe</code>, can then
          instruct Samba to create or delete a snapshot for a given share, and
          expose the snapshot as a new share.
        </p></section><section class="sect3" id="sec-samba-advanced-snapshots-client-linux" data-id-title="Managing snapshots remotely from Linux with rpcclient"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.8.3.3 </span><span class="title-name">Managing snapshots remotely from Linux with <code class="command">rpcclient</code></span></span> <a title="Permalink" class="permalink" href="#sec-samba-advanced-snapshots-client-linux">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          The <code class="systemitem">samba-client</code> package contains an FSRVP
          client that can remotely request a Windows/Samba server to create and
          expose a snapshot of a given share. You can then use existing tools
          in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> to mount the exposed share and back up its files.
          Requests to the server are sent using the
          <code class="command">rpcclient</code> binary.
        </p><div class="complex-example"><div class="example" id="id-1.11.6.8.11.5.5.3" data-id-title="Using rpcclient to request a Windows server 2012 share snapshot"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 20.4: </span><span class="title-name">Using <code class="command">rpcclient</code> to request a Windows server 2012 share snapshot </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.8.11.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
            Connect to <code class="literal">win-server.example.com</code> server as an
            administrator in an <code class="literal">EXAMPLE</code> domain:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>rpcclient -U '<em class="replaceable">EXAMPLE</em>\Administrator' ncacn_np:<em class="replaceable">win-server.example.com</em>[ndr64,sign]
Enter EXAMPLE/Administrator's password:</pre></div><p>
            Check that the SMB share is visible for
            <code class="command">rpcclient</code>:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>rpcclient $&gt; netshareenum
netname: windows_server_2012_share
remark:
path:   C:\Shares\windows_server_2012_share
password:       (null)</pre></div><p>
            Check that the SMB share supports snapshot creation:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>rpcclient $&gt; fss_is_path_sup windows_server_2012_share \
UNC \\WIN-SERVER\windows_server_2012_share\ supports shadow copy requests</pre></div><p>
            Request the creation of a share snapshot:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>rpcclient $&gt; fss_create_expose backup ro windows_server_2012_share
13fe880e-e232-493d-87e9-402f21019fb6: shadow-copy set created
13fe880e-e232-493d-87e9-402f21019fb6(1c26544e-8251-445f-be89-d1e0a3938777): \
\\WIN-SERVER\windows_server_2012_share\ shadow-copy added to set
13fe880e-e232-493d-87e9-402f21019fb6: prepare completed in 0 secs
13fe880e-e232-493d-87e9-402f21019fb6: commit completed in 1 secs
13fe880e-e232-493d-87e9-402f21019fb6(1c26544e-8251-445f-be89-d1e0a3938777): \
share windows_server_2012_share@{1C26544E-8251-445F-BE89-D1E0A3938777} \
exposed as a snapshot of \\WIN-SERVER\windows_server_2012_share\</pre></div><p>
            Confirm that the snapshot share is exposed by the server:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>rpcclient $&gt; netshareenum
netname: windows_server_2012_share
remark:
path:   C:\Shares\windows_server_2012_share
password:       (null)

netname: windows_server_2012_share@{1C26544E-8251-445F-BE89-D1E0A3938777}
remark: (null)
path:   \\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy{F6E6507E-F537-11E3-9404-B8AC6F927453}\Shares\windows_server_2012_share\
password:       (null)</pre></div><p>
            Attempt to delete the snapshot share:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>rpcclient $&gt; fss_delete windows_server_2012_share \
13fe880e-e232-493d-87e9-402f21019fb6 1c26544e-8251-445f-be89-d1e0a3938777
13fe880e-e232-493d-87e9-402f21019fb6(1c26544e-8251-445f-be89-d1e0a3938777): \
\\WIN-SERVER\windows_server_2012_share\ shadow-copy deleted</pre></div><p>
            Confirm that the snapshot share has been removed by the server:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>rpcclient $&gt; netshareenum
netname: windows_server_2012_share
remark:
path:   C:\Shares\windows_server_2012_share
password:       (null)</pre></div></div></div></div></section><section class="sect3" id="sec-samba-advanced-snapshots-client-winserver" data-id-title="Managing snapshots remotely from Windows with DiskShadow.exe"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">20.8.3.4 </span><span class="title-name">Managing snapshots remotely from Windows with <code class="command">DiskShadow.exe</code></span></span> <a title="Permalink" class="permalink" href="#sec-samba-advanced-snapshots-client-winserver">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          You can manage snapshots of SMB shares on the Linux Samba server from
          Windows clients as well. Windows Server 2012 includes the
          <code class="command">DiskShadow.exe</code> utility which can manage remote
          shares similarly to the <code class="command">rpcclient</code> command
          described in
          <a class="xref" href="#sec-samba-advanced-snapshots-client-linux" title="20.8.3.3. Managing snapshots remotely from Linux with rpcclient">Section 20.8.3.3, “Managing snapshots remotely from Linux with <code class="command">rpcclient</code>”</a>. Note
          that you need to carefully set up the Samba server first.
        </p><p>
          The following is an example procedure to set up the Samba server so
          that the Windows client can manage its shares' snapshots. Note that
          <em class="replaceable">EXAMPLE</em> is the Active Directory domain
          used in the testing environment, <code class="uri">fsrvp-server.example.com</code>
          is the host name of the Samba server, and
          <code class="filename">/srv/smb</code> is the path to the SMB share.
        </p><div class="procedure" id="id-1.11.6.8.11.5.6.4" data-id-title="Detailed Samba server configuration"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.1: </span><span class="title-name">Detailed Samba server configuration </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.8.11.5.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Join an Active Directory domain via YaST.<span class="phrase">
              For more information, see
              <a class="xref" href="#sec-samba-adnet" title="20.7. Samba server in the network with Active Directory">Section 20.7, “Samba server in the network with Active Directory”</a>.</span>
            </p></li><li class="step"><p>
              Ensure that the Active Directory domain's DNS entry is correct:
            </p><div class="verbatim-wrap"><pre class="screen">fsrvp-server:~ # net -U 'Administrator' ads dns register \
fsrvp-server.example.com &lt;IP address&gt;
Successfully registered hostname with DNS</pre></div></li><li class="step"><p>
              Create Btrfs subvolume at <code class="filename">/srv/smb</code>:
            </p><div class="verbatim-wrap"><pre class="screen">fsrvp-server:~ # btrfs subvolume create /srv/smb</pre></div></li><li class="step"><p>
              Create a Snapper configuration file for the path
              <code class="filename">/srv/smb</code>:
            </p><div class="verbatim-wrap"><pre class="screen">fsrvp-server:~ # snapper -c &lt;snapper_config&gt; create-config /srv/smb</pre></div></li><li class="step"><p>
              Create a new share with path <code class="filename">/srv/smb</code>, and
              the YaST <span class="guimenu">Expose Snapshots</span> check box enabled.
              Make sure to add the following snippets to the global section of
              <code class="filename">/etc/samba/smb.conf</code>, as mentioned in
              <a class="xref" href="#sec-samba-advanced-snapshots-server" title="20.8.3.2. Remote share snapshots">Section 20.8.3.2, “Remote share snapshots”</a>:
            </p><div class="verbatim-wrap"><pre class="screen">[global]
 rpc_daemon:fssd = fork
 registry shares = yes
 include = registry</pre></div></li><li class="step"><p>
              Restart Samba with <code class="command">systemctl restart nmb smb</code>.
            </p></li><li class="step"><p>
              Configure Snapper permissions:
            </p><div class="verbatim-wrap"><pre class="screen">fsrvp-server:~ # snapper -c &lt;snapper_config&gt; set-config \
ALLOW_USERS="EXAMPLE\\\\Administrator EXAMPLE\\\\win-client$"</pre></div><p>
              Ensure that any instances of <code class="literal">ALLOW_USERS</code> are
              also permitted access to the <code class="filename">.snapshots</code>
              subdirectory.
            </p><div class="verbatim-wrap"><pre class="screen">fsrvp-server:~ # snapper -c &lt;snapper_config&gt; set-config SYNC_ACL=yes</pre></div><div id="id-1.11.6.8.11.5.6.4.8.5" data-id-title="Path escaping" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Path escaping</div><p>
                Be careful about the '\' escapes! Escape twice to ensure that
                the value stored in
                <code class="filename">/etc/snapper/configs/&lt;snapper_config&gt;</code>
                is escaped once.
              </p></div><p>
              "EXAMPLE\win-client$" corresponds to the Windows client computer
              account. Windows issues initial FSRVP requests while
              authenticated with this account.
            </p></li><li class="step"><p>
              Grant the Windows client account necessary privileges:
            </p><div class="verbatim-wrap"><pre class="screen">fsrvp-server:~ # net -U 'Administrator' rpc rights grant \
"EXAMPLE\\win-client$" SeBackupPrivilege
Successfully granted rights.</pre></div><p>
              The previous command is not needed for the
              "EXAMPLE\Administrator" user, which has privileges already
              granted.
            </p></li></ol></div></div><div class="procedure" id="id-1.11.6.8.11.5.6.5" data-id-title="Windows client setup and DiskShadow.exe in action"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.2: </span><span class="title-name">Windows client setup and <code class="command">DiskShadow.exe</code> in action </span></span><a title="Permalink" class="permalink" href="#id-1.11.6.8.11.5.6.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
              Boot Windows Server 2012 (example host name WIN-CLIENT).
            </p></li><li class="step"><p>
              Join the same Active Directory domain EXAMPLE as with the
              <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>.
            </p></li><li class="step"><p>
              Reboot.
            </p></li><li class="step"><p>
              Open PowerShell.
            </p></li><li class="step"><p>
              Start <code class="command">DiskShadow.exe</code> and begin the backup
              procedure:
            </p><div class="verbatim-wrap"><pre class="screen">PS C:\Users\Administrator.EXAMPLE&gt; diskshadow.exe
Microsoft DiskShadow version 1.0
Copyright (C) 2012 Microsoft Corporation
On computer:  WIN-CLIENT,  6/17/2014 3:53:54 PM

DISKSHADOW&gt; begin backup</pre></div></li><li class="step"><p>
              Specify that shadow copies persist across program exits, resets,
              and reboots:
            </p><div class="verbatim-wrap"><pre class="screen">DISKSHADOW&gt; set context PERSISTENT</pre></div></li><li class="step"><p>
              Check whether the specified share supports snapshots, and create
              one:
            </p><div class="verbatim-wrap"><pre class="screen">DISKSHADOW&gt; add volume \\fsrvp-server\sles_snapper

DISKSHADOW&gt; create
Alias VSS_SHADOW_1 for shadow ID {de4ddca4-4978-4805-8776-cdf82d190a4a} set as \
 environment variable.
Alias VSS_SHADOW_SET for shadow set ID {c58e1452-c554-400e-a266-d11d5c837cb1} \
 set as environment variable.

Querying all shadow copies with the shadow copy set ID \
 {c58e1452-c554-400e-a266-d11d5c837cb1}

 * Shadow copy ID = {de4ddca4-4978-4805-8776-cdf82d190a4a}     %VSS_SHADOW_1%
    - Shadow copy set: {c58e1452-c554-400e-a266-d11d5c837cb1}  %VSS_SHADOW_SET%
    - Original count of shadow copies = 1
    - Original volume name: \\FSRVP-SERVER\SLES_SNAPPER\ \
      [volume not on this machine]
    - Creation time: 6/17/2014 3:54:43 PM
    - Shadow copy device name:
      \\FSRVP-SERVER\SLES_SNAPPER@{31afd84a-44a7-41be-b9b0-751898756faa}
    - Originating machine: FSRVP-SERVER
    - Service machine: win-client.example.com
    - Not exposed
    - Provider ID: {89300202-3cec-4981-9171-19f59559e0f2}
    - Attributes:  No_Auto_Release Persistent FileShare

Number of shadow copies listed: 1</pre></div></li><li class="step"><p>
              Finish the backup procedure:
            </p><div class="verbatim-wrap"><pre class="screen">DISKSHADOW&gt; end backup</pre></div></li><li class="step"><p>
              After the snapshot was created, try to delete it and verify the
              deletion:
            </p><div class="verbatim-wrap"><pre class="screen">DISKSHADOW&gt; delete shadows volume \\FSRVP-SERVER\SLES_SNAPPER\
Deleting shadow copy {de4ddca4-4978-4805-8776-cdf82d190a4a} on volume \
 \\FSRVP-SERVER\SLES_SNAPPER\ from provider \
{89300202-3cec-4981-9171-19f59559e0f2} [Attributes: 0x04000009]...

Number of shadow copies deleted: 1

DISKSHADOW&gt; list shadows all

Querying all shadow copies on the computer ...
No shadow copies found in system.</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="sec-samba-info" data-id-title="More information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.9 </span><span class="title-name">More information</span></span> <a title="Permalink" class="permalink" href="#sec-samba-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/net_samba.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Man pages:</span>
            To see a list of all <code class="command">man</code> pages installed with
            the package <span class="package">samba</span>, run <code class="command">apropos
            samba</code>. Open any of the man pages with <code class="command">man
            <em class="replaceable">NAME_OF_MAN_PAGE</em></code>.
          </p></li><li class="listitem"><p><span class="formalpara-title">SUSE-specific README file:</span>
            The package <span class="package">samba-client</span> contains the file
            <code class="filename">/usr/share/doc/packages/samba/README.SUSE</code>.
          </p></li><li class="listitem"><p><span class="formalpara-title">Additional package documentation:</span>
            Install the package <code class="systemitem">samba-doc</code> with
            <code class="command">zypper install samba-doc</code>.
          </p><p>
          This documentation installs into
          <code class="filename">/usr/share/doc/packages/samba</code>. It contains an
          HTML version of the man pages and a library of example configurations
          (such as <code class="filename">smb.conf.SUSE</code>).
        </p></li><li class="listitem"><p><span class="formalpara-title">Online documentation:</span>
            The Samba wiki contains extensive <em class="citetitle">User
            Documentation</em> at
            <a class="link" href="https://wiki.samba.org/index.php/User_Documentation" target="_blank">https://wiki.samba.org/index.php/User_Documentation</a>.
          </p></li></ul></div></section></section><section class="chapter" id="cha-autofs" data-id-title="On-demand mounting with autofs"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">21 </span><span class="title-name">On-demand mounting with autofs</span></span> <a title="Permalink" class="permalink" href="#cha-autofs">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    <code class="systemitem">autofs</code> is a program that automatically mounts
    specified directories on an on-demand basis. It is based on a kernel module
    for high efficiency, and can manage both local directories and network
    shares. These automatic mount points are mounted only when they are
    accessed, and unmounted after a certain period of inactivity. This
    on-demand behavior saves bandwidth and results in better performance than
    static mounts managed by <code class="filename">/etc/fstab</code>. While
    <code class="systemitem">autofs</code> is a control script,
    <code class="command">automount</code> is the command (daemon) that does the actual
    auto-mounting.
   </p></div></div></div></div><section class="sect1" id="sec-autofs-installation" data-id-title="Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.1 </span><span class="title-name">Installation</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="systemitem">autofs</code> is not installed on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> by
   default. To use its auto-mounting capabilities, first install it with
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper install autofs</pre></div></section><section class="sect1" id="sec-autofs-configuration" data-id-title="Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.2 </span><span class="title-name">Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You need to configure <code class="systemitem">autofs</code> manually by editing
   its configuration files with a text editor, such as <code class="command">vim</code>.
   There are two basic steps to configure
   <code class="systemitem">autofs</code>—the <span class="emphasis"><em>master</em></span> map
   file, and specific map files.
  </p><section class="sect2" id="sec-autofs-configuration-master-map" data-id-title="The master map file"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.2.1 </span><span class="title-name">The master map file</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-configuration-master-map">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The default master configuration file for <code class="systemitem">autofs</code>
    is <code class="filename">/etc/auto.master</code>. You can change its location by
    changing the value of the <code class="option">DEFAULT_MASTER_MAP_NAME</code> option
    in <code class="filename">/etc/sysconfig/autofs</code>. Here is the content of the
    default one for <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>:
   </p><div class="verbatim-wrap"><pre class="screen">#
# Sample auto.master file
# This is an automounter map and it has the following format
# key [ -mount-options-separated-by-comma ] location
# For details of the format look at autofs(5).<span class="callout" id="co-autofs-manpage">1</span>
#
#/misc  /etc/auto.misc<span class="callout" id="co-autofs-map">2</span>
#/net -hosts
#
# Include /etc/auto.master.d/*.autofs<span class="callout" id="co-autofs-include">3</span>
#
#+dir:/etc/auto.master.d
#
# Include central master map if it can be found using
# nsswitch sources.
#
# Note that if there are entries for /net or /misc (as
# above) in the included master map any keys that are the
# same will not be seen as the first read key seen takes
# precedence.
#
+auto.master<span class="callout" id="co-autofs-plus">4</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-autofs-manpage"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The <code class="systemitem">autofs</code> manual page (<code class="command">man 5
      autofs</code>) offers a lot of valuable information on the format of
      the automounter maps.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-autofs-map"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Although commented out (#) by default, this is an example of a simple
      automounter mapping syntax.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-autofs-include"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      In case you need to split the master map into several files, uncomment
      the line, and put the mappings (suffixed with <code class="literal">.autofs</code>)
      in the <code class="filename">/etc/auto.master.d/</code> directory.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-autofs-plus"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      <code class="literal">+auto.master</code> ensures that those using NIS
      <span class="phrase">(see <span class="intraxref">Book “Security and Hardening Guide”, Chapter 3 “Using NIS”, Section 3.1 “Configuring NIS servers”</span> for more
      information on NIS)</span> will still find their master map.
     </p></td></tr></table></div><p>
    Entries in <code class="filename">auto.master</code> have three fields with the
    following syntax:
   </p><div class="verbatim-wrap"><pre class="screen">mount point      map name      options</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.9.4.3.7.1"><span class="term">mount point</span></dt><dd><p>
       The base location where to mount the <code class="systemitem">autofs</code>
       file system, such as <code class="literal">/home</code>.
      </p></dd><dt id="id-1.11.6.9.4.3.7.2"><span class="term">map name</span></dt><dd><p>
       The name of a map source to use for mounting. For the syntax of the map
       files, see <a class="xref" href="#sec-autofs-mapfiles" title="21.2.2. Map files">Section 21.2.2, “Map files”</a>.
      </p></dd><dt id="id-1.11.6.9.4.3.7.3"><span class="term">options</span></dt><dd><p>
       These options (if specified) will apply as defaults to all entries in
       the given map.
      </p></dd></dl></div><div id="id-1.11.6.9.4.3.8" data-id-title="More information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: More information</div><p>
     For more detailed information on the specific values of the optional
     <code class="literal">map-type</code>, <code class="literal">format</code>, and
     <code class="literal">options</code>, see the <span class="guimenu">auto.master</span> manual
     page (<code class="command">man 5 auto.master</code>).
    </p></div><p>
    The following entry in <code class="filename">auto.master</code> tells
    <code class="systemitem">autofs</code> to look in
    <code class="filename">/etc/auto.smb</code>, and create mount points in the
    <code class="filename">/smb</code> directory:
   </p><div class="verbatim-wrap"><pre class="screen">/smb   /etc/auto.smb</pre></div><section class="sect3" id="sec-autofs-directmount" data-id-title="Direct mounts"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">21.2.1.1 </span><span class="title-name">Direct mounts</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-directmount">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Direct mounts create a mount point at the path specified inside the
     relevant map file. Instead of specifying the mount point in
     <code class="filename">auto.master</code>, replace the mount point field with
     <code class="literal">/-</code>. For example, the following line tells
     <code class="systemitem">autofs</code> to create a mount point in the place
     specified in <code class="filename">auto.smb</code>:
    </p><div class="verbatim-wrap"><pre class="screen">/-        /etc/auto.smb</pre></div><div id="id-1.11.6.9.4.3.11.4" data-id-title="Maps without full path" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Maps without full path</div><p>
      If the map file is not specified with its full local or network path, it
      is located using the Name Service Switch (NSS) configuration:
     </p><div class="verbatim-wrap"><pre class="screen">/-        auto.smb</pre></div></div></section></section><section class="sect2" id="sec-autofs-mapfiles" data-id-title="Map files"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.2.2 </span><span class="title-name">Map files</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-mapfiles">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.11.6.9.4.4.2" data-id-title="Other types of maps" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Other types of maps</div><p>
     Although <span class="emphasis"><em>files</em></span> are the most common types of maps for
     auto-mounting with <code class="systemitem">autofs</code>, there are other types
     as well. A map specification can be the output of a command, or a result
     of a query in LDAP or a database. For more detailed information on map
     types, see the manual page <code class="command">man 5 auto.master</code>.
    </p></div><p>
    Map files specify the (local or network) source location, and the mount
    point where to mount the source locally. The general format of maps is
    similar to the master map. The difference is that the
    <span class="emphasis"><em>options</em></span> appear between the mount point and the
    location instead of at the end of the entry:
   </p><div class="verbatim-wrap"><pre class="screen">mount point      options      location</pre></div><p>
    Make sure that map files are not marked as executable. You can remove
    the executable bits by executing <code class="command">chmod -x <em class="replaceable">MAP_FILE</em></code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.9.4.4.6.1"><span class="term">mount point</span></dt><dd><p>
       Specifies where to mount the source location. This can be either a
       single directory name (so-called <span class="emphasis"><em>indirect</em></span> mount) to
       be added to the base mount point specified in
       <code class="filename">auto.master</code>, or the full path of the mount point
       (direct mount, see <a class="xref" href="#sec-autofs-directmount" title="21.2.1.1. Direct mounts">Section 21.2.1.1, “Direct mounts”</a>).
      </p></dd><dt id="id-1.11.6.9.4.4.6.2"><span class="term">options</span></dt><dd><p>
       Specifies an optional comma-separated list of mount options for the
       relevant entries. If <code class="filename">auto.master</code> contains options
       for this map file as well, these are appended.
      </p></dd><dt id="id-1.11.6.9.4.4.6.3"><span class="term">location</span></dt><dd><p>
       Specifies from where the file system is to be mounted. It is usually an
       NFS or SMB volume in the usual notation
       <code class="literal">host_name:path_name</code>. If the file system to be mounted
       begins with a '/' (such as local <code class="filename">/dev</code> entries or
       smbfs shares), a colon symbol ':' needs to be prefixed, such as
       <code class="literal">:/dev/sda1</code>.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-autofs-debugging" data-id-title="Operation and debugging"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.3 </span><span class="title-name">Operation and debugging</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-debugging">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section introduces information on how to control the
   <code class="systemitem">autofs</code> service operation, and how to view more
   debugging information when tuning the automounter operation.
  </p><section class="sect2" id="sec-autofs-debugging-service" data-id-title="Controlling the autofs service"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.3.1 </span><span class="title-name">Controlling the <code class="systemitem">autofs</code> service</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-debugging-service">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The operation of the <code class="systemitem">autofs</code> service is controlled
    by <code class="systemitem">systemd</code>. The general syntax of the <code class="command">systemctl</code>
    command for <code class="systemitem">autofs</code> is
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl <em class="replaceable">SUB_COMMAND</em> autofs</pre></div><p>
    where <em class="replaceable">SUB_COMMAND</em> is one of:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.11.6.9.5.3.5.1"><span class="term">enable</span></dt><dd><p>
       Starts the automounter daemon at boot.
      </p></dd><dt id="id-1.11.6.9.5.3.5.2"><span class="term">start</span></dt><dd><p>
       Starts the automounter daemon.
      </p></dd><dt id="id-1.11.6.9.5.3.5.3"><span class="term">stop</span></dt><dd><p>
       Stops the automounter daemon. Automatic mount points are not accessible.
      </p></dd><dt id="id-1.11.6.9.5.3.5.4"><span class="term">status</span></dt><dd><p>
       Prints the current status of the <code class="systemitem">autofs</code> service
       together with a part of a relevant log file.
      </p></dd><dt id="id-1.11.6.9.5.3.5.5"><span class="term">restart</span></dt><dd><p>
       Stops and starts the automounter, terminating all running daemons and
       starting new ones.
      </p></dd><dt id="id-1.11.6.9.5.3.5.6"><span class="term">reload</span></dt><dd><p>
       Checks the current <code class="filename">auto.master</code> map, restarts those
       daemons whose entries have changed, and starts new ones for new entries.
      </p></dd></dl></div></section><section class="sect2" id="sec-autofs-debugging-problems" data-id-title="Debugging automounter problems"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.3.2 </span><span class="title-name">Debugging automounter problems</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-debugging-problems">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you experience problems when mounting directories with
    <code class="systemitem">autofs</code>, it is useful to run the
    <code class="command">automount</code> daemon manually and watch its output messages:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop <code class="systemitem">autofs</code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl stop autofs</pre></div></li><li class="step"><p>
      From one terminal, run <code class="command">automount</code> manually in the
      foreground, producing verbose output.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> automount -f -v</pre></div></li><li class="step"><p>
      From another terminal, try to mount the auto-mounting file systems by
      accessing the mount points (for example by <code class="command">cd</code> or
      <code class="command">ls</code>).
     </p></li><li class="step"><p>
      Check the output of <code class="command">automount</code> from the first terminal
      for more information on why the mount failed, or why it was not even
      attempted.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-autofs-nfs" data-id-title="Auto-mounting an NFS share"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.4 </span><span class="title-name">Auto-mounting an NFS share</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-nfs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following procedure illustrates how to configure
   <code class="systemitem">autofs</code> to auto-mount an NFS share available on your
   network. It uses the information mentioned above, and assumes you
   are familiar with NFS exports. For more information on NFS, see
   <a class="xref" href="#cha-nfs" title="Chapter 19. Sharing file systems with NFS">Chapter 19, <em>Sharing file systems with NFS</em></a>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit the master map file <code class="filename">/etc/auto.master</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vim /etc/auto.master</pre></div><p>
     Add a new entry for the new NFS mount at the end of
     <code class="filename">/etc/auto.master</code>:
    </p><div class="verbatim-wrap"><pre class="screen">/nfs      /etc/auto.nfs      --timeout=10</pre></div><p>
     This tells <code class="systemitem">autofs</code> that the base mount point is
     <code class="filename">/nfs</code>, the NFS shares are specified in the
     <code class="filename">/etc/auto.nfs</code> map, and that all shares in this map
     will be automatically unmounted after 10 seconds of inactivity.
    </p></li><li class="step"><p>
     Create a new map file for NFS shares:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> vim /etc/auto.nfs</pre></div><p>
     <code class="filename">/etc/auto.nfs</code> normally contains a separate line for
     each NFS share. Its format is described in
     <a class="xref" href="#sec-autofs-mapfiles" title="21.2.2. Map files">Section 21.2.2, “Map files”</a>. Add the line describing the mount point
     and the NFS share network address:
    </p><div class="verbatim-wrap"><pre class="screen">export      jupiter.com:/home/geeko/doc/export</pre></div><p>
     The above line means that the <code class="filename">/home/geeko/doc/export</code>
     directory on the <code class="literal">jupiter.com</code> host will be auto-mounted
     to the <code class="filename">/nfs/export</code> directory on the local host
     (<code class="filename">/nfs</code> is taken from the
     <code class="filename">auto.master</code> map) when requested. The
     <code class="filename">/nfs/export</code> directory will be created automatically
     by <code class="systemitem">autofs</code>.
    </p></li><li class="step"><p>
     Optionally comment out the related line in <code class="filename">/etc/fstab</code>
     if you previously mounted the same NFS share statically. The line should
     look similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">#jupiter.com:/home/geeko/doc/export /nfs/export nfs defaults 0 0</pre></div></li><li class="step"><p>
     Reload <code class="systemitem">autofs</code> and check if it works:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart autofs</pre></div><div class="verbatim-wrap"><pre class="screen"># ls -l /nfs/export
total 20
drwxr-xr-x  5 1001 users 4096 Jan 14  2017 .images/
drwxr-xr-x 10 1001 users 4096 Aug 16  2017 .profiled/
drwxr-xr-x  3 1001 users 4096 Aug 30  2017 .tmp/
drwxr-xr-x  4 1001 users 4096 Apr 25 08:56 manual/</pre></div><p>
     If you can see the list of files on the remote share, then
     <code class="systemitem">autofs</code> is functioning.
    </p></li></ol></div></div></section><section class="sect1" id="sec-autofs-advanced" data-id-title="Advanced topics"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.5 </span><span class="title-name">Advanced topics</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-advanced">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section describes topics that are beyond the basic introduction to
   <code class="systemitem">autofs</code>—auto-mounting of NFS shares that are
   available on your network, using wild cards in map files, and information
   specific to the CIFS file system.
  </p><section class="sect2" id="sec-autofs-advanced-net" data-id-title="/net mount point"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.1 </span><span class="title-name"><code class="filename">/net</code> mount point</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-advanced-net">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This helper mount point is useful if you use a lot of NFS shares.
    <code class="filename">/net</code> auto-mounts all NFS shares on your local network
    on demand. The entry is already present in the
    <code class="filename">auto.master</code> file, so all you need to do is uncomment
    it and restart <code class="systemitem">autofs</code>:
   </p><div class="verbatim-wrap"><pre class="screen">/net      -hosts</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl restart autofs</pre></div><p>
    For example, if you have a server named <code class="literal">jupiter</code> with an
    NFS share called <code class="filename">/export</code>, you can mount it by typing
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> cd /net/jupiter/export</pre></div><p>
    on the command line.
   </p></section><section class="sect2" id="sec-autofs-advanced-wildcards" data-id-title="Using wild cards to auto-mount subdirectories"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.2 </span><span class="title-name">Using wild cards to auto-mount subdirectories</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-advanced-wildcards">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you have a directory with subdirectories that you need to auto-mount
    individually—the typical case is the <code class="filename">/home</code>
    directory with individual users' home directories
    inside—<code class="systemitem">autofs</code> offers a clever solution.
   </p><p>
    In case of home directories, add the following line in
    <code class="filename">auto.master</code>:
   </p><div class="verbatim-wrap"><pre class="screen">/home      /etc/auto.home</pre></div><p>
    Now you need to add the correct mapping to the
    <code class="filename">/etc/auto.home</code> file, so that the users' home
    directories are mounted automatically. One solution is to create separate
    entries for each directory:
   </p><div class="verbatim-wrap"><pre class="screen">wilber      jupiter.com:/home/wilber
penguin      jupiter.com:/home/penguin
tux      jupiter.com:/home/tux
[...]</pre></div><p>
    This is very awkward as you need to manage the list of users inside
    <code class="filename">auto.home</code>. You can use the asterisk '*' instead of the
    mount point, and the ampersand '&amp;' instead of the directory to be
    mounted:
   </p><div class="verbatim-wrap"><pre class="screen">*      jupiter:/home/&amp;</pre></div></section><section class="sect2" id="sec-autofs-advanced-cifs" data-id-title="Auto-mounting CIFS file system"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">21.5.3 </span><span class="title-name">Auto-mounting CIFS file system</span></span> <a title="Permalink" class="permalink" href="#sec-autofs-advanced-cifs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/autofs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you want to auto-mount an SMB/CIFS share (see
    <a class="xref" href="#cha-samba" title="Chapter 20. Samba">Chapter 20, <em>Samba</em></a> for more information on the SMB/CIFS protocol),
    you need to modify the syntax of the map file. Add
    <code class="option">-fstype=cifs</code> in the option field, and prefix the share
    location with a colon ':'.
   </p><div class="verbatim-wrap"><pre class="screen">mount point      -fstype=cifs      ://jupiter.com/export</pre></div></section></section></section></div><div class="legal-section"><section class="appendix" id="id-1.11.7" data-id-title="GNU licenses"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">GNU licenses</span></span> <a title="Permalink" class="permalink" href="#id-1.11.7">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_legal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This appendix contains the GNU Free Documentation License version 1.2.
 </p><section class="sect1" id="id-1.11.7.4" data-id-title="GNU Free Documentation License"><div class="titlepage"><div><div><div class="title-container"><h2 class="title legal"><span class="title-number-name"><span class="title-name">GNU Free Documentation License</span></span> <a title="Permalink" class="permalink" href="#id-1.11.7.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sle/edit/main/xml/common_license_gfdl1.2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.4"><span class="name">
    0. PREAMBLE
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.4">#</a></h5></div><p>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </p><p>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </p><p>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.8"><span class="name">
    1. APPLICABILITY AND DEFINITIONS
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.8">#</a></h5></div><p>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </p><p>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </p><p>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </p><p>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </p><p>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </p><p>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </p><p>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </p><p>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </p><p>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </p><p>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.19"><span class="name">
    2. VERBATIM COPYING
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.19">#</a></h5></div><p>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </p><p>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.22"><span class="name">
    3. COPYING IN QUANTITY
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.22">#</a></h5></div><p>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </p><p>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </p><p>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </p><p>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.27"><span class="name">
    4. MODIFICATIONS
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.27">#</a></h5></div><p>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </p></li><li class="listitem"><p>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </p></li><li class="listitem"><p>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </p></li><li class="listitem"><p>
    Preserve all the copyright notices of the Document.
   </p></li><li class="listitem"><p>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </p></li><li class="listitem"><p>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </p></li><li class="listitem"><p>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </p></li><li class="listitem"><p>
    Include an unaltered copy of this License.
   </p></li><li class="listitem"><p>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </p></li><li class="listitem"><p>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </p></li><li class="listitem"><p>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </p></li><li class="listitem"><p>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </p></li><li class="listitem"><p>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </p></li><li class="listitem"><p>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </p></li><li class="listitem"><p>
    Preserve any Warranty Disclaimers.
   </p></li></ol></div><p>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </p><p>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </p><p>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </p><p>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.34"><span class="name">
    5. COMBINING DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.34">#</a></h5></div><p>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </p><p>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </p><p>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.38"><span class="name">
    6. COLLECTIONS OF DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.38">#</a></h5></div><p>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </p><p>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.41"><span class="name">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.41">#</a></h5></div><p>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </p><p>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.44"><span class="name">
    8. TRANSLATION
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.44">#</a></h5></div><p>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </p><p>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.47"><span class="name">
    9. TERMINATION
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.47">#</a></h5></div><p>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.49"><span class="name">
    10. FUTURE REVISIONS OF THIS LICENSE
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.49">#</a></h5></div><p>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <a class="link" href="https://www.gnu.org/copyleft/" target="_blank">https://www.gnu.org/copyleft/</a>.
 </p><p>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.11.7.4.52"><span class="name">
    ADDENDUM: How to use this License for your documents
  </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.52">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</pre></div><p>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the “with...Texts.” line with this:
 </p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </p><p>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
 </p></section></section></div></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>