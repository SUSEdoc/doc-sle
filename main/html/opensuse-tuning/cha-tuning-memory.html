<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Tuning the memory management subsystem | System Analysis and Tuning Guide | openSUSE Leap 15.7</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" />
<meta name="title" content="Tuning the memory management subsystem | openSUSE Leap…" />
<meta name="description" content="To understand and tune the memory management behavior of the kernel, it is important to first have an overview of how it works and cooperates with other subsys…" />
<meta name="product-name" content="openSUSE Leap" />
<meta name="product-number" content="15.7" />
<meta name="book-title" content="System Analysis and Tuning Guide" />
<meta name="chapter-title" content="Chapter 15. Tuning the memory management subsystem" />
<meta name="tracker-url" content="https://bugzilla.opensuse.org/enter_bug.cgi" />
<meta name="tracker-type" content="bsc" />
<meta name="tracker-bsc-assignee" content="fs@suse.com" />
<meta name="tracker-bsc-component" content="Documentation" />
<meta name="tracker-bsc-product" content="openSUSE Distribution" />
<meta name="tracker-bsc-version" content="Leap 15.4" />
<meta property="og:title" content="Tuning the memory management subsystem | openSUSE Leap…" />
<meta property="og:description" content="To understand and tune the memory management behavior of the kernel, it is important to first have an overview of how it works and cooperates with other subsys…" />
<meta property="og:type" content="article" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Tuning the memory management subsystem | openSUSE Leap…" />
<meta name="twitter:description" content="To understand and tune the memory management behavior of the kernel, it is important to first have an overview of how it works and cooperates with other subsys…" />
<link rel="home" href="index.html" title="openSUSE Leap Documentation" /><link rel="up" href="part-tuning-kernel.html" title="Part V. Kernel tuning" /><link rel="prev" href="cha-tuning-taskscheduler.html" title="Chapter 14. Tuning the task scheduler" /><link rel="next" href="cha-tuning-network.html" title="Chapter 16. Tuning the network" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css" /></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="System Analysis and Tuning Guide"><span class="book-icon">System Analysis and Tuning Guide</span></a><span> › </span><a class="crumb" href="part-tuning-kernel.html">Kernel tuning</a><span> › </span><a class="crumb" href="cha-tuning-memory.html">Tuning the memory management subsystem</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>System Analysis and Tuning Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="preface-tuning.html"><span class="number"> </span><span class="name">Preface</span></a></li><li class="inactive"><a href="part-tuning-basics.html"><span class="number">I </span><span class="name">Basics</span></a><ol><li class="inactive"><a href="cha-tuning-basics.html"><span class="number">1 </span><span class="name">General notes on system tuning</span></a></li></ol></li><li class="inactive"><a href="part-tuning-monitoring.html"><span class="number">II </span><span class="name">System monitoring</span></a><ol><li class="inactive"><a href="cha-util.html"><span class="number">2 </span><span class="name">System monitoring utilities</span></a></li><li class="inactive"><a href="cha-tuning-syslog.html"><span class="number">3 </span><span class="name">System log files</span></a></li></ol></li><li class="inactive"><a href="part-tuning-kerneltrace.html"><span class="number">III </span><span class="name">Kernel monitoring</span></a><ol><li class="inactive"><a href="cha-tuning-systemtap.html"><span class="number">4 </span><span class="name">SystemTap—filtering and analyzing system data</span></a></li><li class="inactive"><a href="cha-tuning-kprobes.html"><span class="number">5 </span><span class="name">Kernel probes</span></a></li><li class="inactive"><a href="cha-perf.html"><span class="number">6 </span><span class="name">Hardware-based performance monitoring with Perf</span></a></li><li class="inactive"><a href="cha-tuning-oprofile.html"><span class="number">7 </span><span class="name">OProfile—system-wide profiler</span></a></li><li class="inactive"><a href="cha-tuning-dynamic-debug.html"><span class="number">8 </span><span class="name">Dynamic debug—kernel debugging messages</span></a></li></ol></li><li class="inactive"><a href="part-tuning-resources.html"><span class="number">IV </span><span class="name">Resource management</span></a><ol><li class="inactive"><a href="cha-tuning-resources.html"><span class="number">9 </span><span class="name">General system resource management</span></a></li><li class="inactive"><a href="cha-tuning-cgroups.html"><span class="number">10 </span><span class="name">Kernel control groups</span></a></li><li class="inactive"><a href="cha-tuning-numactl.html"><span class="number">11 </span><span class="name">Automatic Non-Uniform Memory Access (NUMA) balancing</span></a></li><li class="inactive"><a href="cha-tuning-power.html"><span class="number">12 </span><span class="name">Power management</span></a></li></ol></li><li class="inactive"><a href="part-tuning-kernel.html"><span class="number">V </span><span class="name">Kernel tuning</span></a><ol><li class="inactive"><a href="cha-tuning-io.html"><span class="number">13 </span><span class="name">Tuning I/O performance</span></a></li><li class="inactive"><a href="cha-tuning-taskscheduler.html"><span class="number">14 </span><span class="name">Tuning the task scheduler</span></a></li><li class="inactive"><a href="cha-tuning-memory.html"><span class="number">15 </span><span class="name">Tuning the memory management subsystem</span></a></li><li class="inactive"><a href="cha-tuning-network.html"><span class="number">16 </span><span class="name">Tuning the network</span></a></li></ol></li><li class="inactive"><a href="part-tuning-dumps.html"><span class="number">VI </span><span class="name">Handling system dumps</span></a><ol><li class="inactive"><a href="cha-tuning-tracing.html"><span class="number">17 </span><span class="name">Tracing tools</span></a></li><li class="inactive"><a href="cha-tuning-kexec.html"><span class="number">18 </span><span class="name">Kexec and Kdump</span></a></li><li class="inactive"><a href="cha-tuning-systemd-coredump.html"><span class="number">19 </span><span class="name">Using <code class="systemitem">systemd-coredump</code> to debug application crashes</span></a></li></ol></li><li class="inactive"><a href="part-tuning-ptp.html"><span class="number">VII </span><span class="name">Synchronized clocks with Precision Time Protocol</span></a><ol><li class="inactive"><a href="cha-tuning-ptp.html"><span class="number">20 </span><span class="name">Precision Time Protocol</span></a></li></ol></li><li class="inactive"><a href="bk05apa.html"><span class="number">A </span><span class="name">GNU licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 14. Tuning the task scheduler" href="cha-tuning-taskscheduler.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 16. Tuning the network" href="cha-tuning-network.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="System Analysis and Tuning Guide"><span class="book-icon">System Analysis and Tuning Guide</span></a><span> › </span><a class="crumb" href="part-tuning-kernel.html">Kernel tuning</a><span> › </span><a class="crumb" href="cha-tuning-memory.html">Tuning the memory management subsystem</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 14. Tuning the task scheduler" href="cha-tuning-taskscheduler.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 16. Tuning the network" href="cha-tuning-network.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="cha-tuning-memory"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="productname"><span class="phrase">openSUSE Leap</span></span></span> <span class="productnumber "><span class="productnumber"><span class="phrase">15.7</span></span></span></div><div><h2 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning the memory management subsystem</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="cha-tuning-memory.html#cha-tuning-memory-usage"><span class="number">15.1 </span><span class="name">Memory usage</span></a></span></dt><dt><span class="sect1"><a href="cha-tuning-memory.html#cha-tuning-memory-optimize"><span class="number">15.2 </span><span class="name">Reducing memory usage</span></a></span></dt><dt><span class="sect1"><a href="cha-tuning-memory.html#cha-tuning-memory-vm"><span class="number">15.3 </span><span class="name">Virtual memory manager (VM) tunable parameters</span></a></span></dt><dt><span class="sect1"><a href="cha-tuning-memory.html#cha-tuning-memory-monitoring"><span class="number">15.4 </span><span class="name">Monitoring VM behavior</span></a></span></dt></dl></div></div><p>
  To understand and tune the memory management behavior of the
  kernel, it is important to first have an overview of how it works and
  cooperates with other subsystems.
 </p><p>
  
  The memory management subsystem, also called the virtual memory manager,
  will subsequently be called <span class="quote">“<span class="quote ">VM</span>”</span>. The role of the VM
  is to manage the allocation of physical memory (RAM) for the entire kernel
  and user programs. It is also responsible for providing a virtual memory
  environment for user processes (managed via POSIX APIs with Linux
  extensions). Finally, the VM frees up RAM when there
  is a shortage, either by trimming caches or swapping out
  <span class="quote">“<span class="quote ">anonymous</span>”</span> memory.
 </p><p>
  The most important thing to understand when examining and tuning VM is how
  its caches are managed. The basic goal of the VM's caches is to minimize
  the cost of I/O as generated by swapping and file system operations
  (including network file systems). This is achieved by avoiding I/O or by submitting I/O in better patterns.
 </p><p>
  Free memory is used and filled up by these caches as required. The
  more memory is available for caches and anonymous memory, the more
  effectively the caches and swapping operate. However, if a memory
  shortage is encountered, the caches are trimmed or the memory is swapped
  out.
 </p><p>
  For a particular workload, the first thing that can be done to improve
  performance is to increase memory and reduce the frequency that memory
  must be trimmed or swapped. The second thing is to change the way caches
  are managed by changing kernel parameters.
 </p><p>
  Finally, the workload itself should be examined and tuned as well. If an
  application is allowed to run more processes or threads, effectiveness of
  VM caches can be reduced, if each process is operating in its own area of
  the file system. Memory overheads are also increased. If applications
  allocate their own buffers or caches, larger caches mean that less
  memory is available for VM caches. However, more processes and threads can
  mean more opportunity to overlap and pipeline I/O, and may take better
  advantage of multiple cores. Experimentation is required for the best
  results.
 </p><div class="sect1 " id="cha-tuning-memory-usage"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Memory usage</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage</li></ul></div></div></div></div><p>
   Memory allocations can be characterized as
   <span class="quote">“<span class="quote ">pinned</span>”</span> (also known as <span class="quote">“<span class="quote ">unreclaimable</span>”</span>),
   <span class="quote">“<span class="quote ">reclaimable</span>”</span> or <span class="quote">“<span class="quote ">swappable</span>”</span>.
  </p><div class="sect2 " id="cha-tuning-memory-usage-anonymous"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Anonymous memory</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-anonymous">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-anonymous</li></ul></div></div></div></div><p>
    Anonymous memory tends to be program heap and stack memory (for example,
    <code class="literal">&gt;malloc()</code>). It is reclaimable, except in special
    cases such as <code class="literal">mlock</code> or if there is no available swap
    space. Anonymous memory must be written to swap before it can be
    reclaimed. Swap I/O (both swapping in and swapping out pages) tends to
    be less efficient than pagecache I/O, because of allocation and access
    patterns.
   </p></div><div class="sect2 " id="cha-tuning-memory-usage-pagecache"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pagecache</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-pagecache">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-pagecache</li></ul></div></div></div></div><p>
    A cache of file data. When a file is read from disk or network, the
    contents are stored in pagecache. No disk or network access is required,
    if the contents are up-to-date in pagecache. tmpfs and shared memory
    segments count toward pagecache.
   </p><p>
    When a file is written to, the new data is stored in pagecache before
    being written back to a disk or the network (making it a write-back
    cache). When a page has new data not written back yet, it is called
    <span class="quote">“<span class="quote ">dirty</span>”</span>. Pages not classified as dirty are
    <span class="quote">“<span class="quote ">clean</span>”</span>. Clean pagecache pages can be reclaimed if there is
    a memory shortage by simply freeing them. Dirty pages must first be made
    clean before being reclaimed.
   </p></div><div class="sect2 " id="cha-tuning-memory-usage-buffercache"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Buffercache</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-buffercache">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-buffercache</li></ul></div></div></div></div><p>
    This is a type of pagecache for block devices (for example, /dev/sda). A
    file system typically uses the buffercache when accessing its on-disk
    metadata structures such as inode tables, allocation bitmaps, and so
    forth. Buffercache can be reclaimed similarly to pagecache.
   </p></div><div class="sect2 " id="cha-tuning-memory-usage-bufferheads"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Buffer heads</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-bufferheads">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-bufferheads</li></ul></div></div></div></div><p>
    Buffer heads are small auxiliary structures that tend to be allocated
    upon pagecache access. They can generally be reclaimed easily when the
    pagecache or buffercache pages are clean.
   </p></div><div class="sect2 " id="cha-tuning-memory-usage-writeback"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Writeback</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-writeback">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-writeback</li></ul></div></div></div></div><p>
    As applications write to files, the pagecache becomes dirty
    and the buffercache may become dirty. When the amount of
    dirty memory reaches a specified number of pages in bytes
    (<span class="emphasis"><em>vm.dirty_background_bytes</em></span>), or when the
    amount of dirty memory reaches a specific ratio to total memory
    (<span class="emphasis"><em>vm.dirty_background_ratio</em></span>), or when the pages
    have been dirty for longer than a specified amount of time
    (<span class="emphasis"><em>vm.dirty_expire_centisecs</em></span>), the kernel begins
    writeback of pages starting with files that had the pages dirtied first.
    The background bytes and ratios are mutually exclusive and setting one
    will overwrite the other. Flusher threads perform writeback in the
    background and allow applications to continue running. If the I/O
    cannot keep up with applications dirtying pagecache, and dirty data
    reaches a critical setting (<span class="emphasis"><em>vm.dirty_bytes</em></span> or
    <span class="emphasis"><em>vm.dirty_ratio</em></span>), then applications begin to be
    throttled to prevent dirty data exceeding this threshold.
   </p></div><div class="sect2 " id="cha-tuning-memory-usage-readahead"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Readahead</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-readahead">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-readahead</li></ul></div></div></div></div><p>
    The VM monitors file access patterns and may attempt to perform
    readahead. Readahead reads pages into the pagecache from the file system
    that have not been requested yet. It is done to allow fewer,
    larger I/O requests to be submitted (more efficient). And for I/O to be
    pipelined (I/O performed at the same time as the application is
    running).
   </p></div><div class="sect2 " id="cha-tuning-memory-usage-vfs"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VFS caches</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-vfs">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-vfs</li></ul></div></div></div></div><div class="sect3 " id="cha-tuning-memory-usage-vfs-inode"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Inode cache</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-vfs-inode">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-vfs-inode</li></ul></div></div></div></div><p>
     This is an in-memory cache of the inode structures for each file
     system. These contain attributes such as the file size, permissions and
     ownership, and pointers to the file data.
    </p></div><div class="sect3 " id="cha-tuning-memory-usage-vfs-dir-entry"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Directory entry cache</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-usage-vfs-dir-entry">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-usage-vfs-dir-entry</li></ul></div></div></div></div><p>
     This is an in-memory cache of the directory entries in the system.
     These contain a name (the name of a file), the inode which it refers
     to, and children entries. This cache is used when traversing the
     directory structure and accessing a file by name.
    </p></div></div></div><div class="sect1 " id="cha-tuning-memory-optimize"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reducing memory usage</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-optimize">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-optimize</li></ul></div></div></div></div><div class="sect2 " id="cha-tuning-memory-optimize-malloc"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reducing malloc (anonymous) usage</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-optimize-malloc">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-optimize-malloc</li></ul></div></div></div></div><p>
    Applications running on <span class="productname"><span class="phrase">openSUSE Leap</span></span> <span class="productnumber"><span class="phrase">15.7</span></span> can allocate
    more memory compared to older releases. This is because of
    <code class="systemitem">glibc</code> changing its default
    behavior while allocating user space memory. See
    <a class="link" href="https://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html" target="_blank">https://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html</a>
    for explanation of these parameters.
   </p><p>
    To restore behavior similar to older releases, M_MMAP_THRESHOLD should
    be set to 128*1024. This can be done with mallopt() call from the
    application, or via setting <code class="varname">MALLOC_MMAP_THRESHOLD_</code>
    environment variable before running the application.
   </p></div><div class="sect2 " id="cha-tuning-memory-optimize-overhead"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reducing kernel memory overheads</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-optimize-overhead">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-optimize-overhead</li></ul></div></div></div></div><p>
    Kernel memory that is reclaimable (caches, described above) is
    trimmed automatically during memory shortages. Most other kernel memory
    cannot be easily reduced but is a property of the workload given to the
    kernel.
   </p><p>
    Reducing the requirements of the user space workload reduces the
    kernel memory usage (fewer processes, fewer open files and sockets,
    etc.).
   </p></div><div class="sect2 " id="cha-tuning-memory-optimize-cgoups"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Memory controller (memory cgroups)</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-optimize-cgoups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-optimize-cgoups</li></ul></div></div></div></div><p>
    If the memory cgroups feature is not needed, it can be switched off by
    passing cgroup_disable=memory on the kernel command line, reducing
    memory consumption of the kernel a bit. There is also a slight
    performance benefit as there is a small amount of accounting overhead
    when memory cgroups are available even if none are configured.
   </p></div></div><div class="sect1 " id="cha-tuning-memory-vm"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Virtual memory manager (VM) tunable parameters</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-vm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-vm</li></ul></div></div></div></div><p>
   When tuning the VM, it should be understood that certain changes take time to affect the workload and take full effect. If the workload
   changes throughout the day, it may behave differently at different
   times. A change that increases throughput under certain conditions may
   decrease it under other conditions.
  </p><div class="sect2 " id="cha-tuning-memory-vm-reclaim"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reclaim ratios</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-vm-reclaim">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-vm-reclaim</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.7.7.4.11.3.2.1"><span class="term "><code class="filename">/proc/sys/vm/swappiness</code>
     </span></dt><dd><p>
       This control is used to define how aggressively the kernel swaps out
       anonymous memory relative to pagecache and other caches. Increasing
       the value increases the amount of swapping. The default value is
       <code class="literal">60</code>.
      </p><p>
       Swap I/O tends to be much less efficient than other I/O. However,
       certain pagecache pages are accessed much more frequently than less
       used anonymous memory. The right balance should be found here.
      </p><p>
       If swap activity is observed during slowdowns, it may be worth
       reducing this parameter. If there is a lot of I/O activity and the
       amount of pagecache in the system is rather small, or if there are
       large dormant applications running, increasing this value can
       improve performance.
      </p><p>
       The more data is swapped out, the longer the system
       takes to swap data back in when it is needed.
      </p></dd><dt id="id-1.7.7.4.11.3.2.2"><span class="term "><code class="filename">/proc/sys/vm/vfs_cache_pressure</code>
     </span></dt><dd><p>
       This variable controls the tendency of the kernel to reclaim the
       memory which is used for caching of VFS caches, versus pagecache and
       swap. Increasing this value increases the rate at which VFS caches
       are reclaimed.
      </p><p>
       It is difficult to know when this should be changed, other than by
       experimentation. The <code class="command">slabtop</code> command (part of the
       package <code class="systemitem">procps</code>) shows top
       memory objects used by the kernel. The vfs caches are the "dentry"
       and the "*_inode_cache" objects. If these are consuming a large
       amount of memory in relation to pagecache, it may be worth trying to
       increase pressure. Could also help to reduce swapping. The default
       value is <code class="literal">100</code>.
      </p></dd><dt id="id-1.7.7.4.11.3.2.3"><span class="term "><code class="filename">/proc/sys/vm/min_free_kbytes</code>
     </span></dt><dd><p>
       This controls the amount of memory that is kept free for use by
       special reserves including <span class="quote">“<span class="quote ">atomic</span>”</span> allocations (those
       which cannot wait for reclaim). This should not normally be lowered
       unless the system is being carefully tuned for memory usage
       (normally useful for embedded rather than server applications). If
       <span class="quote">“<span class="quote ">page allocation failure</span>”</span> messages and stack traces are
       frequently seen in logs, min_free_kbytes could be increased until the
       errors disappear. There is no need for concern if these messages are infrequent. The default value depends on the amount of RAM.
      </p></dd><dt id="id-1.7.7.4.11.3.2.4"><span class="term "><code class="filename">/proc/sys/vm/watermark_scale_factor</code></span></dt><dd><p>
      Broadly speaking, free memory has high, low and min watermarks. When
      the low watermark is reached then <code class="command">kswapd</code> wakes to
      reclaim memory in the background. It stays awake until free memory
      reaches the high watermark. Applications will stall and reclaim
      memory when the min watermark is reached.
      </p><p>
      The <code class="literal">watermark_scale_factor</code> defines the amount
      of memory left in a node/system before kswapd is woken up and how
      much memory needs to be free before kswapd goes back to sleep.
      The unit is in fractions of 10,000. The default value of 10 means
      the distances between watermarks are 0.1% of the available memory
      in the node/system. The maximum value is 1000, or 10% of memory.
      </p><p>
      Workloads that frequently stall in direct reclaim, accounted by
      <code class="literal">allocstall</code> in <code class="filename">/proc/vmstat</code>,
      may benefit from altering this parameter. Similarly, if
      <code class="command">kswapd</code> is sleeping prematurely, as accounted for by
      <code class="literal">kswapd_low_wmark_hit_quickly</code>, then it may indicate
      that the number of pages kept free to avoid stalls is too low.
      </p></dd></dl></div></div><div class="sect2 " id="cha-tuning-memory-vm-writeback"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Writeback parameters</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-vm-writeback">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-vm-writeback</li></ul></div></div></div></div><p>
    One important change in writeback behavior since <span class="productname"><span class="phrase">openSUSE Leap</span></span> 10 is
    that modification to file-backed mmap() memory is accounted immediately
    as dirty memory (and subject to writeback). Whereas previously it would
    only be subject to writeback after it was unmapped, upon an msync()
    system call, or under heavy memory pressure.
   </p><p>
    Some applications do not expect mmap modifications to be subject to such
    writeback behavior, and performance can be reduced. Increasing writeback
    ratios and times can improve this type of slowdown.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.7.7.4.11.4.4.1"><span class="term "><code class="filename">/proc/sys/vm/dirty_background_ratio</code>
     </span></dt><dd><p>
       This is the percentage of the total amount of free and reclaimable
       memory. When the amount of dirty pagecache exceeds this percentage,
       writeback threads start writing back dirty memory. The default value
       is <code class="literal">10</code> (%).
      </p></dd><dt id="id-1.7.7.4.11.4.4.2"><span class="term "><code class="filename">/proc/sys/vm/dirty_background_bytes</code>
     </span></dt><dd><p>
       This contains the amount of dirty memory at which
       the background kernel flusher threads start writeback.
       <code class="filename">dirty_background_bytes</code> is the counterpart of
       <code class="filename">dirty_background_ratio</code>. If one of them is set,
       the other one will automatically be read as <code class="literal">0</code>.
      </p></dd><dt id="id-1.7.7.4.11.4.4.3"><span class="term "><code class="filename">/proc/sys/vm/dirty_ratio</code>
     </span></dt><dd><p>
       Similar percentage value as for
       <code class="filename">dirty_background_ratio</code>. When this is exceeded,
       applications that want to write to the pagecache are blocked and
       wait for kernel background flusher threads to reduce the amount of dirty
       memory. The default value is <code class="literal">20</code> (%).
      </p></dd><dt id="id-1.7.7.4.11.4.4.4"><span class="term "><code class="filename">/proc/sys/vm/dirty_bytes</code>
     </span></dt><dd><p>
       This file controls the same tunable as <code class="filename">dirty_ratio</code>
       however the amount of dirty memory is in bytes as opposed to a
       percentage of reclaimable memory. Since both
       <code class="filename">dirty_ratio</code> and <code class="filename">dirty_bytes</code>
       control the same tunable, if one of them is set, the other one is
       automatically read as <code class="literal">0</code>. The minimum value allowed
       for <code class="filename">dirty_bytes</code> is two pages (in bytes); any value
       lower than this limit is ignored and the old configuration will be
       retained.
      </p></dd><dt id="id-1.7.7.4.11.4.4.5"><span class="term "><code class="filename">/proc/sys/vm/dirty_expire_centisecs</code>
     </span></dt><dd><p>
       The data which has been dirty in-memory for longer than this interval
       is written out next time a flusher thread wakes up. Expiration
       is measured based on the modification time of a file's inode.
       Therefore, multiple dirtied pages from the same file are all written when the interval is exceeded.
      </p></dd></dl></div><p>
    <code class="filename">dirty_background_ratio</code> and
    <code class="filename">dirty_ratio</code> together determine the pagecache
    writeback behavior. If these values are increased, more dirty memory is
    kept in the system for a longer time. With more dirty memory allowed in
    the system, the chance to improve throughput by avoiding writeback I/O
    and to submitting more optimal I/O patterns increases. However, more
    dirty memory can either harm latency when memory needs to be reclaimed
    or at points of data integrity (<span class="quote">“<span class="quote ">synchronization points</span>”</span>) when it
    needs to be written back to disk.
   </p></div><div class="sect2 " id="cha-tuning-memory-vm-readahead"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Readahead parameters</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-vm-readahead">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-vm-readahead</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.7.7.4.11.5.2.1"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">&lt;bdev&gt;</em>/queue/read_ahead_kb</code>
     </span></dt><dd><p>
       If one or more processes are sequentially reading a file, the kernel
       reads certain data in advance (ahead) to reduce the amount of
       time that processes need to wait for data to be available. The actual
       amount of data being read in advance is computed dynamically, based
       on the extent of <span class="emphasis"><em>sequentiality</em></span> of the I/O. This parameter sets the
       maximum amount of data that the kernel reads ahead for a single file.
       If you observe that large sequential reads from a file are not fast
       enough, you can try increasing this value. Increasing it too far may
       result in readahead thrashing where pagecache used for readahead is
       reclaimed before it can be used, or slowdowns because of a large
       amount of useless I/O. The default value is <code class="literal">512</code>
       (KB).
      </p></dd></dl></div></div><div class="sect2 " id="cha-tuning-memory-vm-thp"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Transparent HugePage parameters</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-vm-thp">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-vm-thp</li></ul></div></div></div></div><p>
     Transparent HugePages (THP) provide a way to dynamically allocate huge
     pages either on‑demand by the process or deferring the allocation
     until later via the <code class="command">khugepaged</code> kernel thread. This
     method is distinct from the use of <code class="literal">hugetlbfs</code> to
     manually manage their allocation and use. Workloads with contiguous memory
     access patterns can benefit greatly from THP. A 1000-fold decrease in page
     faults can be observed when running synthetic workloads with contiguous
     memory access patterns.
   </p><p>
     There are cases when THP may be undesirable. Workloads with sparse memory
     access patterns can perform poorly with THP due to excessive memory
     usage. For example, 2 MB of memory may be used at fault time instead of 4
     KB for each fault and ultimately lead to premature page reclaim. 
   </p><p>
     The behavior of THP may be configured via the
     <code class="option">transparent_hugepage=</code> kernel parameter or via
     sysfs. For example, it may be disabled by adding the kernel parameter
     <code class="option">transparent_hugepage=never</code>, rebuilding your grub2
     configuration, and rebooting. Verify if THP is disabled with:

    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</pre></div><p>
      If disabled, the value <code class="literal">never</code> is shown
      in square brackets like in the example above. A value of
      <code class="literal">always</code> mandatorily tries and uses THP at fault
      time but defers to <code class="command">khugepaged</code> if the allocation
      fails. A value of <code class="literal">madvise</code> will only allocate THP
      for address spaces explicitly specified by an application.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.7.7.4.11.6.7.1"><span class="term "><code class="filename">/sys/kernel/mm/transparent_hugepage/defrag</code>
       </span></dt><dd><p>
         This parameter controls how much effort an application commits when
         allocating a THP. A value of <code class="literal">always</code> is the default
         for <span class="phrase">openSUSE 42.1 and earlier</span> releases
         that supported THP. If a THP is not available, the application tries to defragment memory.
         It potentially incurs large stalls in an application if the memory is fragmented and a THP
         is not available.
        </p><p>
         A value of <code class="literal">madvise</code> means that THP allocation
         requests will only defragment if the application explicitly requests
         it. This is the default for <span class="phrase">openSUSE 42.2</span> and later
         releases.
        </p><p>
         <code class="literal">defer</code> is only available on <span class="phrase">openSUSE
         42.2</span> and later releases. If a THP is not available, the
         application falls back to using small pages if a THP is not
         available. It wakes the <code class="command">kswapd</code> and
         <code class="command">kcompactd</code> kernel threads to defragment memory in
         the background and a THP will be allocated later by
         <code class="command">khugepaged</code>.
       </p><p>
         The final option <code class="literal">never</code> uses small pages if
         a THP is unavailable but no other action will take place.
        </p></dd></dl></div></div><div class="sect2 " id="cha-tuning-memory-vm-khugepaged"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">khugepaged parameters</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-vm-khugepaged">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-vm-khugepaged</li></ul></div></div></div></div><p>
    khugepaged is automatically started when
    <code class="literal">transparent_hugepage</code> is set to
    <code class="literal">always</code> or <code class="literal">madvise</code>, and it will be
    automatically shut down if it is set to <code class="literal">never</code>. Normally
    this runs at low frequency but the behavior can be tuned.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.7.7.4.11.7.3.1"><span class="term "><code class="filename">/sys/kernel/mm/transparent_hugepage/khugepaged/defrag</code> </span></dt><dd><p>
        A value of 0 will disable <code class="command">khugepaged</code> even though
        THP may still be used at fault time. This may be important for
        latency-sensitive applications that benefit from THP but cannot
        tolerate a stall if <code class="command">khugepaged</code> tries to update an
        application memory usage.
       </p></dd><dt id="id-1.7.7.4.11.7.3.2"><span class="term "><code class="filename">/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan</code></span></dt><dd><p>
        This parameter controls how many pages are scanned by
        <code class="command">khugepaged</code> in a single pass. A scan identifies
        small pages that can be reallocated as THP. Increasing this value
         will allocate THP in the background faster at the cost of CPU
         usage.
       </p></dd><dt id="id-1.7.7.4.11.7.3.3"><span class="term "><code class="filename">/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs</code></span></dt><dd><p>
       <code class="command">khugepaged</code> sleeps for a short interval specified
       by this parameter after each pass to limit how much CPU usage is
       used. Reducing this value allocates THP in the background faster
       at the cost of CPU usage. A value of 0 will force continual scanning.
       </p></dd><dt id="id-1.7.7.4.11.7.3.4"><span class="term "><code class="filename">/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs</code>
      </span></dt><dd><p>
         This parameter controls how long <code class="command">khugepaged</code> will
         sleep in the event it fails to allocate a THP in the background waiting
         for <code class="command">kswapd</code> and <code class="command">kcompactd</code> to
         take action.
       </p></dd></dl></div><p>
     The remaining parameters for <code class="command">khugepaged</code> are rarely
     useful for performance tuning but are fully documented in
     <code class="filename">/usr/src/linux/Documentation/vm/transhuge.txt</code>
   </p></div><div class="sect2 " id="cha-tuning-memory-vm-more"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Further VM parameters</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-vm-more">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-vm-more</li></ul></div></div></div></div><p>
    For the complete list of the VM tunable parameters, see
    <code class="filename">/usr/src/linux/Documentation/sysctl/vm.txt</code>
    (available after having installed the
    <code class="systemitem">kernel-source</code> package).
   </p></div></div><div class="sect1 " id="cha-tuning-memory-monitoring"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring VM behavior</span> <a title="Permalink" class="permalink" href="cha-tuning-memory.html#cha-tuning-memory-monitoring">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE/doc-sle/edit/main/xml/tuning_memory.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha-tuning-memory-monitoring</li></ul></div></div></div></div><p>
   Some simple tools that can help monitor VM behavior:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     vmstat: This tool gives a good overview of what the VM is doing. See
     <a class="xref" href="cha-util.html#sec-util-multi-vmstat" title="2.1.1. vmstat">Section 2.1.1, “<code class="command">vmstat</code>”</a> for details.
    </p></li><li class="listitem "><p>
     <code class="filename">/proc/meminfo</code>: This file gives a detailed
     breakdown of where memory is being used. See
     <a class="xref" href="cha-util.html#sec-util-memory-meminfo" title="2.4.2. Detailed memory usage: /proc/meminfo">Section 2.4.2, “Detailed memory usage: <code class="filename">/proc/meminfo</code>”</a> for details.
    </p></li><li class="listitem "><p>
     <code class="command">slabtop</code>: This tool provides detailed information
     about kernel slab memory usage. buffer_head, dentry, inode_cache,
     ext3_inode_cache, etc. are the major caches. This command is available
     with the package <code class="systemitem">procps</code>.
    </p></li><li class="listitem "><p>
    <code class="filename">/proc/vmstat</code>: This file gives a detailed breakdown of
    internal VM behavior. The information contained within is implementation
    specific and may not always be available. Some information is duplicated in
    <code class="filename">/proc/meminfo</code> and other information can be presented
    in a friendly fashion by utilities. For maximum utility, this file needs to
    be monitored over time to observe rates of change. The most important
    pieces of information that are hard to derive from other sources are as
    follows:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.7.7.4.12.3.4.2.1"><span class="term "><code class="literal">pgscan_kswapd_*, pgsteal_kswapd_*</code></span></dt><dd><p>
        These report respectively the number of pages scanned and reclaimed
        by <code class="command">kswapd</code> since the system started. The ratio
        between these values can be interpreted as the reclaim efficiency
        with a low efficiency implying that the system is struggling to
        reclaim memory and may be thrashing. Light activity here is
        generally not something to be concerned with.
       </p></dd><dt id="id-1.7.7.4.12.3.4.2.2"><span class="term "><code class="literal">pgscan_direct_*, pgsteal_direct_*</code></span></dt><dd><p>
         These report respectively the number of pages scanned and
         reclaimed by an application directly. This is correlated with
         increases in the <code class="literal">allocstall</code> counter. This is
         more serious than <code class="command">kswapd</code> activity as these
         events indicate that processes are stalling. Heavy activity
         here combined with <code class="command">kswapd</code> and high rates of
         <code class="literal">pgpgin</code>, <code class="literal">pgpout</code> and/or high
         rates of <code class="literal">pswapin</code> or <code class="literal">pswpout</code>
         are signs that a system is thrashing heavily.
        </p><p>
         More detailed information can be obtained using tracepoints.
        </p></dd><dt id="id-1.7.7.4.12.3.4.2.3"><span class="term "><code class="literal">thp_fault_alloc, thp_fault_fallback</code></span></dt><dd><p>
        These counters correspond to how many THPs were allocated directly
        by an application and how many times a THP was not available and
        small pages were used. Generally a high fallback rate is harmless
        unless the application is sensitive to TLB pressure.
       </p></dd><dt id="id-1.7.7.4.12.3.4.2.4"><span class="term "><code class="literal">thp_collapse_alloc, thp_collapse_alloc_failed</code></span></dt><dd><p>
        These counters correspond to how many THPs were allocated by
        <code class="command">khugepaged</code> and how many times a THP was not
        available and small pages were used. A high fallback rate implies
        that the system is fragmented and THPs are not being used even
        when the memory usage by applications would allow them. It is
        only a problem for applications that are sensitive to TLB pressure.
       </p></dd><dt id="id-1.7.7.4.12.3.4.2.5"><span class="term "><code class="literal">compact_*_scanned, compact_stall, compact_fail,
      compact_success</code></span></dt><dd><p>
        These counters may increase when THP is enabled and the system is
        fragmented. <code class="literal">compact_stall</code> is incremented when
        an application stalls allocating THP. The remaining counters
        account for pages scanned, the number of defragmentation events
        that succeeded or failed.
       </p></dd></dl></div></li></ol></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="cha-tuning-network.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 16 </span>Tuning the network</span></a><a class="nav-link" href="cha-tuning-taskscheduler.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 14 </span>Tuning the task scheduler</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2024 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>